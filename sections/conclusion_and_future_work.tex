\section{Conclusions}
\label{sec:conclusion}
This paper investigates whether sparse feedforward layers in small transformers result in better language understanding and faster inference than the standard feedforward network. We found that inference speed does not increase due to software and hardware limitations associated with our small language model setting. Some sparse feedforward layers, such as the controller feedforward layer, in the right configurations, achieves better language understanding than the standard feedforward network while activating just a fraction of their parameters. We hypothesize that this is because the sparse feedforward layers more efficiently utilize their parameters by approximating larger networks. We recommend future research exploring more configurations of different sparse feedforward architectures.


% Briefly summarize the (main) research question(s).
% Provide your conclusions, the answers to the research question(s).
% Make statements.
% Highlight interesting elements, contributions.

% Discuss open issues, possible improvements, and new questions that arise from this work; formulate recommendations for further research.

% Ideally, this section can stand on its own: it should be readable without having read the earlier sections and accessible to anyone with a bachelor degree in Computer Science.
