
@inproceedings{radford_language_2019,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	url = {https://api.semanticscholar.org/CorpusID:160025533},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	urldate = {2024-06-23},
	author = {Radford, Alec and Wu, Jeff and Child, R. and Luan, D. and Amodei, Dario and Sutskever, I.},
	year = {2019},
}

@misc{noauthor_papers_nodate,
	title = {Papers with {Code} - {Language} {Models} are {Unsupervised} {Multitask} {Learners}},
	url = {https://paperswithcode.com/paper/language-models-are-unsupervised-multitask},
	abstract = {üèÜ SOTA for Language Modelling on enwik8 (Bit per Character (BPC) metric)},
	language = {en},
	urldate = {2024-06-23},
}

@article{radford_language_nodate,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciÔ¨Åc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underÔ¨Åts WebText. Samples from the model reÔ¨Çect these improvements and contain coherent paragraphs of text. These Ô¨Åndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	language = {en},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
}

@misc{fedus_switch_2022,
	title = {Switch {Transformers}: {Scaling} to {Trillion} {Parameter} {Models} with {Simple} and {Efficient} {Sparsity}},
	shorttitle = {Switch {Transformers}},
	url = {http://arxiv.org/abs/2101.03961},
	doi = {10.48550/arXiv.2101.03961},
	abstract = {In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the "Colossal Clean Crawled Corpus" and achieve a 4x speedup over the T5-XXL model.},
	urldate = {2024-06-23},
	publisher = {arXiv},
	author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
	month = jun,
	year = {2022},
	note = {arXiv:2101.03961 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{rajanand_erfrelu_2023,
	title = {{ErfReLU}: {Adaptive} {Activation} {Function} for {Deep} {Neural} {Network}},
	shorttitle = {{ErfReLU}},
	url = {http://arxiv.org/abs/2306.01822},
	doi = {10.48550/arXiv.2306.01822},
	abstract = {Recent research has found that the activation function (AF) selected for adding non-linearity into the output can have a big impact on how effectively deep learning networks perform. Developing activation functions that can adapt simultaneously with learning is a need of time. Researchers recently started developing activation functions that can be trained throughout the learning process, known as trainable, or adaptive activation functions (AAF). Research on AAF that enhance the outcomes is still in its early stages. In this paper, a novel activation function 'ErfReLU' has been developed based on the erf function and ReLU. This function exploits the ReLU and the error function (erf) to its advantage. State of art activation functions like Sigmoid, ReLU, Tanh, and their properties have been briefly explained. Adaptive activation functions like Tanhsoft1, Tanhsoft2, Tanhsoft3, TanhLU, SAAF, ErfAct, Pserf, Smish, and Serf have also been described. Lastly, performance analysis of 9 trainable activation functions along with the proposed one namely Tanhsoft1, Tanhsoft2, Tanhsoft3, TanhLU, SAAF, ErfAct, Pserf, Smish, and Serf has been shown by applying these activation functions in MobileNet, VGG16, and ResNet models on CIFAR-10, MNIST, and FMNIST benchmark datasets.},
	urldate = {2024-06-19},
	publisher = {arXiv},
	author = {Rajanand, Ashish and Singh, Pradeep},
	month = jun,
	year = {2023},
	note = {arXiv:2306.01822 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{wang_superglue_2020,
	title = {{SuperGLUE}: {A} {Stickier} {Benchmark} for {General}-{Purpose} {Language} {Understanding} {Systems}},
	shorttitle = {{SuperGLUE}},
	url = {http://arxiv.org/abs/1905.00537},
	abstract = {In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at super.gluebenchmark.com.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
	month = feb,
	year = {2020},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{warstadt_learning_2020,
	title = {Learning {Which} {Features} {Matter}: {RoBERTa} {Acquires} a {Preference} for {Linguistic} {Generalizations} ({Eventually})},
	shorttitle = {Learning {Which} {Features} {Matter}},
	url = {http://arxiv.org/abs/2010.05358},
	abstract = {One reason pretraining on self-supervised linguistic tasks is effective is that it teaches models features that are helpful for language understanding. However, we want pretrained models to learn not only to represent linguistic features, but also to use those features preferentially during fine-turning. With this goal in mind, we introduce a new English-language diagnostic set called MSGS (the Mixed Signals Generalization Set), which consists of 20 ambiguous binary classification tasks that we use to test whether a pretrained model prefers linguistic or surface generalizations during fine-tuning. We pretrain RoBERTa models from scratch on quantities of data ranging from 1M to 1B words and compare their performance on MSGS to the publicly available RoBERTa-base. We find that models can learn to represent linguistic features with little pretraining data, but require far more data to learn to prefer linguistic generalizations over surface ones. Eventually, with about 30B words of pretraining data, RoBERTa-base does demonstrate a linguistic bias with some regularity. We conclude that while self-supervised pretraining is an effective way to learn helpful inductive biases, there is likely room to improve the rate at which models learn which features matter.},
	urldate = {2024-05-07},
	publisher = {arXiv},
	author = {Warstadt, Alex and Zhang, Yian and Li, Haau-Sing and Liu, Haokun and Bowman, Samuel R.},
	month = oct,
	year = {2020},
	keywords = {Computer Science - Computation and Language},
}

@misc{warstadt_blimp_2023,
	title = {{BLiMP}: {The} {Benchmark} of {Linguistic} {Minimal} {Pairs} for {English}},
	shorttitle = {{BLiMP}},
	url = {http://arxiv.org/abs/1912.00582},
	abstract = {We introduce The Benchmark of Linguistic Minimal Pairs (shortened to BLiMP), a challenge set for evaluating what language models (LMs) know about major grammatical phenomena in English. BLiMP consists of 67 sub-datasets, each containing 1000 minimal pairs isolating specific contrasts in syntax, morphology, or semantics. The data is automatically generated according to expert-crafted grammars, and aggregate human agreement with the labels is 96.4\%. We use it to evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs. We find that state-of-the-art models identify morphological contrasts reliably, but they struggle with semantic restrictions on the distribution of quantifiers and negative polarity items and subtle syntactic phenomena such as extraction islands.},
	urldate = {2024-01-22},
	publisher = {arXiv},
	author = {Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey, Anhad and Peng, Wei and Wang, Sheng-Fu and Bowman, Samuel R.},
	month = feb,
	year = {2023},
	keywords = {Computer Science - Computation and Language},
}

@misc{wang_glue_2019,
	title = {{GLUE}: {A} {Multi}-{Task} {Benchmark} and {Analysis} {Platform} for {Natural} {Language} {Understanding}},
	shorttitle = {{GLUE}},
	url = {http://arxiv.org/abs/1804.07461},
	abstract = {For natural language understanding (NLU) technology to be maximally useful, both practically and as a scientific object of study, it must be general: it must be able to process language in a way that is not exclusively tailored to any one specific task or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation benchmark (GLUE), a tool for evaluating and analyzing the performance of models across a diverse range of existing NLU tasks. GLUE is model-agnostic, but it incentivizes sharing knowledge across tasks because certain tasks have very limited training data. We further provide a hand-crafted diagnostic test suite that enables detailed linguistic analysis of NLU models. We evaluate baselines based on current methods for multi-task and transfer learning and find that they do not immediately give substantial improvements over the aggregate performance of training a separate model per task, indicating room for improvement in developing general and robust NLU systems.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
	month = feb,
	year = {2019},
	keywords = {Computer Science - Computation and Language},
}

@misc{villalobos_will_2024,
	title = {Will we run out of data? {Limits} of {LLM} scaling based on human-generated data},
	shorttitle = {Will we run out of data?},
	url = {http://arxiv.org/abs/2211.04325},
	abstract = {We investigate the potential constraints on LLM scaling posed by the availability of public humangenerated text data. We forecast the growing demand for training data based on current trends and estimate the total stock of public human text data. Our findings indicate that if current LLM development trends continue, models will be trained on datasets roughly equal in size to the available stock of public human text data between 2026 and 2032, or slightly earlier if models are overtrained. We explore how progress in language modeling can continue when human-generated text datasets cannot be scaled any further. We argue that synthetic data generation, transfer learning from datarich domains, and data efficiency improvements might support further progress.},
	language = {en},
	urldate = {2024-06-18},
	publisher = {arXiv},
	author = {Villalobos, Pablo and Ho, Anson and Sevilla, Jaime and Besiroglu, Tamay and Heim, Lennart and Hobbhahn, Marius},
	month = jun,
	year = {2024},
	note = {arXiv:2211.04325 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@misc{zhang_pegasus_2020,
	title = {{PEGASUS}: {Pre}-training with {Extracted} {Gap}-sentences for {Abstractive} {Summarization}},
	shorttitle = {{PEGASUS}},
	url = {http://arxiv.org/abs/1912.08777},
	abstract = {Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets.},
	language = {en},
	urldate = {2024-06-18},
	publisher = {arXiv},
	author = {Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter J.},
	month = jul,
	year = {2020},
	note = {arXiv:1912.08777 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{raffel_exploring_2023,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	url = {http://arxiv.org/abs/1910.10683},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	language = {en},
	urldate = {2024-06-18},
	publisher = {arXiv},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	month = sep,
	year = {2023},
	note = {arXiv:1910.10683 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{semmelrock_reproducibility_2023,
	title = {Reproducibility in {Machine} {Learning}-{Driven} {Research}},
	url = {http://arxiv.org/abs/2307.10320},
	abstract = {Research is facing a reproducibility crisis, in which the results and findings of many studies are difficult or even impossible to reproduce. This is also the case in machine learning (ML) and artificial intelligence (AI) research. Often, this is the case due to unpublished data and/or source-code, and due to sensitivity to ML training conditions. Although different solutions to address this issue are discussed in the research community such as using ML platforms, the level of reproducibility in MLdriven research is not increasing substantially. Therefore, in this mini survey, we review the literature on reproducibility in ML-driven research with three main aims: (i) reflect on the current situation of ML reproducibility in various research fields, (ii) identify reproducibility issues and barriers that exist in these research fields applying ML, and (iii) identify potential drivers such as tools, practices, and interventions that support ML reproducibility. With this, we hope to contribute to decisions on the viability of different solutions for supporting ML reproducibility.},
	language = {en},
	urldate = {2024-06-17},
	publisher = {arXiv},
	author = {Semmelrock, Harald and Kopeinik, Simone and Theiler, Dieter and Ross-Hellauer, Tony and Kowald, Dominik},
	month = jul,
	year = {2023},
	note = {arXiv:2307.10320 [cs, stat]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Methodology},
}

@misc{geva_transformer_2022,
	title = {Transformer {Feed}-{Forward} {Layers} {Build} {Predictions} by {Promoting} {Concepts} in the {Vocabulary} {Space}},
	url = {http://arxiv.org/abs/2203.14680},
	doi = {10.48550/arXiv.2203.14680},
	abstract = {Transformer-based language models (LMs) are at the core of modern NLP, but their internal prediction construction process is opaque and largely not understood. In this work, we make a substantial step towards unveiling this underlying prediction process, by reverse-engineering the operation of the feed-forward network (FFN) layers, one of the building blocks of transformer models. We view the token representation as a changing distribution over the vocabulary, and the output from each FFN layer as an additive update to that distribution. Then, we analyze the FFN updates in the vocabulary space, showing that each update can be decomposed to sub-updates corresponding to single FFN parameter vectors, each promoting concepts that are often human-interpretable. We then leverage these findings for controlling LM predictions, where we reduce the toxicity of GPT2 by almost 50\%, and for improving computation efficiency with a simple early exit rule, saving 20\% of computation on average.},
	urldate = {2024-06-11},
	publisher = {arXiv},
	author = {Geva, Mor and Caciularu, Avi and Wang, Kevin Ro and Goldberg, Yoav},
	month = oct,
	year = {2022},
	note = {arXiv:2203.14680 [cs]},
	keywords = {Computer Science - Computation and Language, Read},
}

@misc{chaudhuri_b-splines_2021,
	title = {B-{Splines}},
	url = {http://arxiv.org/abs/2108.06617},
	doi = {10.48550/arXiv.2108.06617},
	abstract = {BSplines are one of the most promising curves in computer graphics. They are blessed with some superior geometric properties which make them an ideal candidate for several applications in computer aided design industry. In this article, some basic properties of B-Spline curves are presented. Two significant B-Spline properties viz convex hull property and repeated points effects are discussed. The BSplines computation in computational devices is also illustrated. An industry application based on image processing where B-Spline curve reconstructs the 3D surfaces for CT image datasets of inner organs further highlights the strength of these curves},
	urldate = {2024-06-11},
	publisher = {arXiv},
	author = {Chaudhuri, Arindam},
	month = aug,
	year = {2021},
	note = {arXiv:2108.06617 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@book{nair_rectified_2010,
	title = {Rectified {Linear} {Units} {Improve} {Restricted} {Boltzmann} {Machines} {Vinod} {Nair}},
	volume = {27},
	abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these ‚ÄúStepped Sigmoid Units ‚Äù are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors. 1.},
	author = {Nair, Vinod and Hinton, Geoffrey},
	month = jun,
	year = {2010},
	note = {Journal Abbreviation: Proceedings of ICML
Pages: 814
Publication Title: Proceedings of ICML},
}

@article{nair_rectified_nodate,
	title = {Rectified {Linear} {Units} {Improve} {Restricted} {Boltzmann} {Machines}},
	abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an inÔ¨Ånite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these ‚ÄúStepped Sigmoid Units‚Äù are unchanged. They can be approximated eÔ¨Éciently by noisy, rectiÔ¨Åed linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face veriÔ¨Åcation on the Labeled Faces in the Wild dataset. Unlike binary units, rectiÔ¨Åed linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
	language = {en},
	author = {Nair, Vinod and Hinton, Geoffrey E},
}

@misc{warstadt_call_2023,
	title = {Call for {Papers} -- {The} {BabyLM} {Challenge}: {Sample}-efficient pretraining on a developmentally plausible corpus},
	shorttitle = {Call for {Papers} -- {The} {BabyLM} {Challenge}},
	url = {http://arxiv.org/abs/2301.11796},
	doi = {10.48550/arXiv.2301.11796},
	abstract = {We present the call for papers for the BabyLM Challenge: Sample-efficient pretraining on a developmentally plausible corpus. This shared task is intended for participants with an interest in small scale language modeling, human language acquisition, low-resource NLP, and cognitive modeling. In partnership with CoNLL and CMCL, we provide a platform for approaches to pretraining with a limited-size corpus sourced from data inspired by the input to children. The task has three tracks, two of which restrict the training data to pre-released datasets of 10M and 100M words and are dedicated to explorations of approaches such as architectural variations, self-supervised objectives, or curriculum learning. The final track only restricts the amount of text used, allowing innovation in the choice of the data, its domain, and even its modality (i.e., data from sources other than text is welcome). We will release a shared evaluation pipeline which scores models on a variety of benchmarks and tasks, including targeted syntactic evaluations and natural language understanding.},
	urldate = {2024-06-11},
	publisher = {arXiv},
	author = {Warstadt, Alex and Choshen, Leshem and Mueller, Aaron and Williams, Adina and Wilcox, Ethan and Zhuang, Chengxu},
	month = jan,
	year = {2023},
	note = {arXiv:2301.11796 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{wettig_should_2023,
	title = {Should {You} {Mask} 15\% in {Masked} {Language} {Modeling}?},
	url = {http://arxiv.org/abs/2202.08005},
	doi = {10.48550/arXiv.2202.08005},
	abstract = {Masked language models (MLMs) conventionally mask 15\% of tokens due to the belief that more masking would leave insufficient context to learn good representations; this masking rate has been widely used, regardless of model sizes or masking strategies. In this work, we revisit this important choice of MLM pre-training. We first establish that 15\% is not universally optimal, and larger models should adopt a higher masking rate. Specifically, we find that masking 40\% outperforms 15\% for BERT-large size models on GLUE and SQuAD. Interestingly, an extremely high masking rate of 80\% can still preserve 95\% fine-tuning performance and most of the accuracy in linguistic probing, challenging the conventional wisdom about the role of the masking rate. We then examine the interplay between masking rates and masking strategies and find that uniform masking requires a higher masking rate compared to sophisticated masking strategies such as span or PMI masking. Finally, we argue that increasing the masking rate has two distinct effects: it leads to more corruption, which makes the prediction task more difficult; it also enables more predictions, which benefits optimization. Using this framework, we revisit BERT's 80-10-10 corruption strategy. Together, our results contribute to a better understanding of MLM pre-training.},
	urldate = {2024-06-11},
	publisher = {arXiv},
	author = {Wettig, Alexander and Gao, Tianyu and Zhong, Zexuan and Chen, Danqi},
	month = feb,
	year = {2023},
	note = {arXiv:2202.08005 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{warstadt_findings_2023,
	address = {Singapore},
	title = {Findings of the {BabyLM} {Challenge}: {Sample}-{Efficient} {Pretraining} on {Developmentally} {Plausible} {Corpora}},
	shorttitle = {Findings of the {BabyLM} {Challenge}},
	url = {https://aclanthology.org/2023.conll-babylm.1},
	doi = {10.18653/v1/2023.conll-babylm.1},
	language = {en},
	urldate = {2024-06-11},
	booktitle = {Proceedings of the {BabyLM} {Challenge} at the 27th {Conference} on {Computational} {Natural} {Language} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Warstadt, Alex and Mueller, Aaron and Choshen, Leshem and Wilcox, Ethan and Zhuang, Chengxu and Ciro, Juan and Mosquera, Rafael and Paranjabe, Bhargavi and Williams, Adina and Linzen, Tal and Cotterell, Ryan},
	year = {2023},
	pages = {1--6},
}

@misc{eldan_tinystories_2023,
	title = {{TinyStories}: {How} {Small} {Can} {Language} {Models} {Be} and {Still} {Speak} {Coherent} {English}?},
	shorttitle = {{TinyStories}},
	url = {http://arxiv.org/abs/2305.07759},
	doi = {10.48550/arXiv.2305.07759},
	abstract = {Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention). In this work, we introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities. We also introduce a new paradigm for the evaluation of language models: We suggest a framework which uses GPT-4 to grade the content generated by these models as if those were stories written by students and graded by a (human) teacher. This new paradigm overcomes the flaws of standard benchmarks which often requires the model's output to be very structures, and moreover provides a multidimensional score for the model, providing scores for different capabilities such as grammar, creativity and consistency. We hope that TinyStories can facilitate the development, analysis and research of LMs, especially for low-resource or specialized domains, and shed light on the emergence of language capabilities in LMs.},
	urldate = {2024-06-11},
	publisher = {arXiv},
	author = {Eldan, Ronen and Li, Yuanzhi},
	month = may,
	year = {2023},
	note = {arXiv:2305.07759 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{villalobos_will_2024-1,
	title = {Will we run out of data? {Limits} of {LLM} scaling based on human-generated data},
	shorttitle = {Will we run out of data?},
	url = {http://arxiv.org/abs/2211.04325},
	doi = {10.48550/arXiv.2211.04325},
	abstract = {We investigate the potential constraints on LLM scaling posed by the availability of public human-generated text data. We forecast the growing demand for training data based on current trends and estimate the total stock of public human text data. Our findings indicate that if current LLM development trends continue, models will be trained on datasets roughly equal in size to the available stock of public human text data between 2026 and 2032, or slightly earlier if models are overtrained. We explore how progress in language modeling can continue when human-generated text datasets cannot be scaled any further. We argue that synthetic data generation, transfer learning from data-rich domains, and data efficiency improvements might support further progress.},
	urldate = {2024-06-11},
	publisher = {arXiv},
	author = {Villalobos, Pablo and Ho, Anson and Sevilla, Jaime and Besiroglu, Tamay and Heim, Lennart and Hobbhahn, Marius},
	month = jun,
	year = {2024},
	note = {arXiv:2211.04325 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@misc{wei_emergent_2022,
	title = {Emergent {Abilities} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2206.07682},
	abstract = {Scaling up language models has been shown to predictably improve performance and sample eÔ¨Éciency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence raises the question of whether additional scaling could potentially further expand the range of capabilities of language models.},
	language = {en},
	urldate = {2024-06-11},
	publisher = {arXiv},
	author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
	month = oct,
	year = {2022},
	note = {arXiv:2206.07682 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	shorttitle = {{PyTorch}},
	url = {http://arxiv.org/abs/1912.01703},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientiÔ¨Åc computing libraries, while remaining efÔ¨Åcient and supporting hardware accelerators such as GPUs.},
	language = {en},
	urldate = {2024-06-11},
	publisher = {arXiv},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K√∂pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	month = dec,
	year = {2019},
	note = {arXiv:1912.01703 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Mathematical Software, Statistics - Machine Learning},
}

@misc{shen_efficient_2018,
	title = {Efficient {Attention}: {Attention} with {Linear} {Complexities}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Efficient {Attention}},
	url = {https://arxiv.org/abs/1812.01243},
	doi = {10.48550/ARXIV.1812.01243},
	abstract = {Dot-product attention has wide applications in computer vision and natural language processing. However, its memory and computational costs grow quadratically with the input size. Such growth prohibits its application on high-resolution inputs. To remedy this drawback, this paper proposes a novel efficient attention mechanism equivalent to dot-product attention but with substantially less memory and computational costs. Its resource efficiency allows more widespread and flexible integration of attention modules into a network, which leads to better accuracies. Empirical evaluations demonstrated the effectiveness of its advantages. Efficient attention modules brought significant performance boosts to object detectors and instance segmenters on MS-COCO 2017. Further, the resource efficiency democratizes attention to complex models, where high costs prohibit the use of dot-product attention. As an exemplar, a model with efficient attention achieved state-of-the-art accuracies for stereo depth estimation on the Scene Flow dataset. Code is available at https://github.com/cmsflash/efficient-attention.},
	urldate = {2024-06-11},
	publisher = {arXiv},
	author = {Shen, Zhuoran and Zhang, Mingyuan and Zhao, Haiyu and Yi, Shuai and Li, Hongsheng},
	year = {2018},
	note = {Version Number: 10},
	keywords = {Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, I.2.6; I.2.10; I.4.8; I.4.6, Machine Learning (cs.LG)},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2005.14165},
	doi = {10.48550/ARXIV.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2024-06-10},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year = {2020},
	note = {Version Number: 4},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@misc{brown_language_2020-1,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2024-06-10},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{liu_roberta_2019,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	shorttitle = {{RoBERTa}},
	url = {http://arxiv.org/abs/1907.11692},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	language = {en},
	urldate = {2024-06-09},
	publisher = {arXiv},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2019},
	note = {arXiv:1907.11692 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{jiang_mixtral_2024,
	title = {Mixtral of {Experts}},
	url = {http://arxiv.org/abs/2401.04088},
	abstract = {We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model finetuned to follow instructions, Mixtral 8x7B ‚Äì Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B ‚Äì chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.},
	language = {en},
	urldate = {2024-06-09},
	publisher = {arXiv},
	author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and Lengyel, Gianna and Bour, Guillaume and Lample, Guillaume and Lavaud, L√©lio Renard and Saulnier, Lucile and Lachaux, Marie-Anne and Stock, Pierre and Subramanian, Sandeep and Yang, Sophia and Antoniak, Szymon and Scao, Teven Le and Gervet, Th√©ophile and Lavril, Thibaut and Wang, Thomas and Lacroix, Timoth√©e and Sayed, William El},
	month = jan,
	year = {2024},
	note = {arXiv:2401.04088 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{knaw_nederlandse_2018,
	title = {Nederlandse gedragscode wetenschappelijke integriteit},
	copyright = {info:eu-repo/semantics/openAccess, Creative Commons Zero v1.0 Universal},
	url = {https://easy.dans.knaw.nl/ui/datasets/id/easy-dataset:110600},
	doi = {10.17026/DANS-2CJ-NVWU},
	language = {en},
	urldate = {2024-06-06},
	publisher = {Data Archiving and Networked Services (DANS)},
	author = {{KNAW} and {NFU} and {NWO} and {TO2-Federatie} and {Vereniging Hogescholen} and {VSNU}},
	year = {2018},
	keywords = {Interdisciplinary sciences},
}

@misc{knaw_nederlandse_2018-1,
	title = {Nederlandse gedragscode wetenschappelijke integriteit},
	copyright = {info:eu-repo/semantics/openAccess, Creative Commons Zero v1.0 Universal},
	url = {https://easy.dans.knaw.nl/ui/datasets/id/easy-dataset:110600},
	doi = {10.17026/DANS-2CJ-NVWU},
	language = {en},
	urldate = {2024-06-06},
	publisher = {Data Archiving and Networked Services (DANS)},
	author = {{KNAW} and {NFU} and {NWO} and {TO2-Federatie} and {Vereniging Hogescholen} and {VSNU}},
	year = {2018},
	keywords = {Interdisciplinary sciences},
}

@misc{noauthor_netherlands_nodate,
	title = {Netherlands {Code} of {Conduct} for {Research} {Integrity} {\textbar} {NWO}},
	url = {https://www.nwo.nl/en/netherlands-code-conduct-research-integrity},
	abstract = {NWO adheres to the Netherlands Code of Conduct for Research Integrity as the guiding principle for its integrity policy (1 October 2018).},
	language = {en},
	urldate = {2024-06-06},
}

@misc{schaeffer_pretraining_2023,
	title = {Pretraining on the {Test} {Set} {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/2309.08632},
	doi = {10.48550/arXiv.2309.08632},
	abstract = {Inspired by recent work demonstrating the promise of smaller Transformer-based language models pretrained on carefully curated data, we supercharge such approaches by investing heavily in curating a novel, high quality, non-synthetic data mixture based solely on evaluation benchmarks. Using our novel dataset mixture consisting of less than 100 thousand tokens, we pretrain a 1 million parameter transformer-based LLM {\textbackslash}textbf\{phi-CTNL\} (pronounced ``fictional") that achieves perfect results across diverse academic benchmarks, strictly outperforming all known foundation models. {\textbackslash}textbf\{phi-CTNL\} also beats power-law scaling and exhibits a never-before-seen grokking-like ability to accurately predict downstream evaluation benchmarks' canaries.},
	urldate = {2024-06-06},
	publisher = {arXiv},
	author = {Schaeffer, Rylan},
	month = sep,
	year = {2023},
	note = {arXiv:2309.08632 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{dror_hitchhikers_2018,
	address = {Melbourne, Australia},
	title = {The {Hitchhiker}'s {Guide} to {Testing} {Statistical} {Significance} in {Natural} {Language} {Processing}},
	url = {https://aclanthology.org/P18-1128},
	doi = {10.18653/v1/P18-1128},
	abstract = {Statistical significance testing is a standard statistical tool designed to ensure that experimental results are not coincidental. In this opinion/ theoretical paper we discuss the role of statistical significance testing in Natural Language Processing (NLP) research. We establish the fundamental concepts of significance testing and discuss the specific aspects of NLP tasks, experimental setups and evaluation measures that affect the choice of significance tests in NLP research. Based on this discussion we propose a simple practical protocol for statistical significance test selection in NLP setups and accompany this protocol with a brief survey of the most relevant tests. We then survey recent empirical papers published in ACL and TACL during 2017 and show that while our community assigns great value to experimental results, statistical significance testing is often ignored or misused. We conclude with a brief discussion of open issues that should be properly addressed so that this important tool can be applied. in NLP research in a statistically sound manner.},
	urldate = {2024-06-06},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Dror, Rotem and Baumer, Gili and Shlomov, Segev and Reichart, Roi},
	editor = {Gurevych, Iryna and Miyao, Yusuke},
	month = jul,
	year = {2018},
	keywords = {Read},
	pages = {1383--1392},
}

@misc{de_moor_transformer-based_2024,
	title = {A {Transformer}-{Based} {Approach} for {Smart} {Invocation} of {Automatic} {Code} {Completion}},
	url = {http://arxiv.org/abs/2405.14753},
	doi = {10.1145/3664646.3664760},
	abstract = {Transformer-based language models are highly effective for code completion, with much research dedicated to enhancing the content of these completions. Despite their effectiveness, these models come with high operational costs and can be intrusive, especially when they suggest too often and interrupt developers who are concentrating on their work. Current research largely overlooks how these models interact with developers in practice and neglects to address when a developer should receive completion suggestions. To tackle this issue, we developed a machine learning model that can accurately predict when to invoke a code completion tool given the code context and available telemetry data. To do so, we collect a dataset of 200k developer interactions with our cross-IDE code completion plugin and train several invocation filtering models. Our results indicate that our small-scale transformer model significantly outperforms the baseline while maintaining low enough latency. We further explore the search space for integrating additional telemetry data into a pre-trained transformer directly and obtain promising results. To further demonstrate our approach's practical potential, we deployed the model in an online environment with 34 developers and provided real-world insights based on 74k actual invocations.},
	urldate = {2024-06-06},
	author = {de Moor, Aral and van Deursen, Arie and Izadi, Maliheh},
	month = may,
	year = {2024},
	note = {arXiv:2405.14753 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@misc{wang_superglue_2020-1,
	title = {{SuperGLUE}: {A} {Stickier} {Benchmark} for {General}-{Purpose} {Language} {Understanding} {Systems}},
	shorttitle = {{SuperGLUE}},
	url = {http://arxiv.org/abs/1905.00537},
	doi = {10.48550/arXiv.1905.00537},
	abstract = {In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at super.gluebenchmark.com.},
	urldate = {2024-06-03},
	publisher = {arXiv},
	author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
	month = feb,
	year = {2020},
	note = {arXiv:1905.00537 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be Ô¨Ånetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciÔ¨Åc architecture modiÔ¨Åcations.},
	language = {en},
	urldate = {2024-06-03},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{radford_language_nodate-1,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciÔ¨Åc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underÔ¨Åts WebText. Samples from the model reÔ¨Çect these improvements and contain coherent paragraphs of text. These Ô¨Åndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	language = {en},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
}

@article{thangarasa_mediswift_2024,
	title = {{MediSwift}: {Efficient} {Sparse} {Pre}-trained {Biomedical} {Language} {Models}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {{MediSwift}},
	url = {https://arxiv.org/abs/2403.00952},
	doi = {10.48550/ARXIV.2403.00952},
	abstract = {Large language models (LLMs) are typically trained on general source data for various domains, but a recent surge in domain-specific LLMs has shown their potential to outperform general-purpose models in domain-specific tasks (e.g., biomedicine). Although domain-specific pre-training enhances efficiency and leads to smaller models, the computational costs of training these LLMs remain high, posing budgeting challenges. We introduce MediSwift, a suite of biomedical LMs that leverage sparse pre-training on domain-specific biomedical text data. By inducing up to 75\% weight sparsity during the pre-training phase, MediSwift achieves a 2-2.5x reduction in training FLOPs. Notably, all sparse pre-training was performed on the Cerebras CS-2 system, which is specifically designed to realize the acceleration benefits from unstructured weight sparsity, thereby significantly enhancing the efficiency of the MediSwift models. Through subsequent dense fine-tuning and strategic soft prompting, MediSwift models outperform existing LLMs up to 7B parameters on biomedical tasks, setting new benchmarks w.r.t efficiency-accuracy on tasks such as PubMedQA. Our results show that sparse pre-training, along with dense fine-tuning and soft prompting, offers an effective method for creating high-performing, computationally efficient models in specialized domains.},
	urldate = {2024-06-03},
	author = {Thangarasa, Vithursan and Salem, Mahmoud and Saxena, Shreyas and Leong, Kevin and Hestness, Joel and Lie, Sean},
	year = {2024},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@misc{noauthor_pdf_nodate,
	title = {[{PDF}] {MediSwift}: {Efficient} {Sparse} {Pre}-trained {Biomedical} {Language} {Models} {\textbar} {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/reader/fb9dfabf5e91ed4e903da01f9c34dcbfecb11efc},
	urldate = {2024-06-03},
}

@misc{gorbett_sparse_2023,
	title = {Sparse {Binary} {Transformers} for {Multivariate} {Time} {Series} {Modeling}},
	url = {http://arxiv.org/abs/2308.04637},
	doi = {10.48550/arXiv.2308.04637},
	abstract = {Compressed Neural Networks have the potential to enable deep learning across new applications and smaller computational environments. However, understanding the range of learning tasks in which such models can succeed is not well studied. In this work, we apply sparse and binary-weighted Transformers to multivariate time series problems, showing that the lightweight models achieve accuracy comparable to that of dense floating-point Transformers of the same structure. Our model achieves favorable results across three time series learning tasks: classification, anomaly detection, and single-step forecasting. Additionally, to reduce the computational complexity of the attention mechanism, we apply two modifications, which show little to no decline in model performance: 1) in the classification task, we apply a fixed mask to the query, key, and value activations, and 2) for forecasting and anomaly detection, which rely on predicting outputs at a single point in time, we propose an attention mask to allow computation only at the current time step. Together, each compression technique and attention modification substantially reduces the number of non-zero operations necessary in the Transformer. We measure the computational savings of our approach over a range of metrics including parameter count, bit size, and floating point operation (FLOPs) count, showing up to a 53x reduction in storage size and up to 10.5x reduction in FLOPs.},
	urldate = {2024-06-03},
	publisher = {arXiv},
	author = {Gorbett, Matt and Shirazi, Hossein and Ray, Indrakshi},
	month = aug,
	year = {2023},
	note = {arXiv:2308.04637 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{noauthor_relu_nodate,
	title = {{ReLU} ‚Äî {PyTorch} 2.3 documentation},
	url = {https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html},
	urldate = {2024-06-03},
}

@misc{tay_efficient_2022,
	title = {Efficient {Transformers}: {A} {Survey}},
	shorttitle = {Efficient {Transformers}},
	url = {http://arxiv.org/abs/2009.06732},
	doi = {10.48550/arXiv.2009.06732},
	abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of "X-former" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored "X-former" models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
	urldate = {2024-06-03},
	publisher = {arXiv},
	author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
	month = mar,
	year = {2022},
	note = {arXiv:2009.06732 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval, Computer Science - Machine Learning},
}

@misc{hendrycks_gaussian_2023,
	title = {Gaussian {Error} {Linear} {Units} ({GELUs})},
	url = {http://arxiv.org/abs/1606.08415},
	doi = {10.48550/arXiv.1606.08415},
	abstract = {We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is \$x{\textbackslash}Phi(x)\$, where \${\textbackslash}Phi(x)\$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs (\$x{\textbackslash}mathbf\{1\}\_\{x{\textgreater}0\}\$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.},
	urldate = {2024-05-27},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Gimpel, Kevin},
	month = jun,
	year = {2023},
	note = {arXiv:1606.08415 [cs]},
	keywords = {Computer Science - Machine Learning, Read},
}

@misc{rae_scaling_2016,
	title = {Scaling {Memory}-{Augmented} {Neural} {Networks} with {Sparse} {Reads} and {Writes}},
	url = {http://arxiv.org/abs/1610.09027},
	abstract = {Neural networks augmented with external memory have the ability to learn algorithmic solutions to complex tasks. These models appear promising for applications such as language modeling and machine translation. However, they scale poorly in both space and time as the amount of memory grows ‚Äî limiting their applicability to real-world domains. Here, we present an end-to-end differentiable memory access scheme, which we call Sparse Access Memory (SAM), that retains the representational power of the original approaches whilst training efÔ¨Åciently with very large memories. We show that SAM achieves asymptotic lower bounds in space and time complexity, and Ô¨Ånd that an implementation runs 1,000√ó faster and with 3,000√ó less physical memory than non-sparse models. SAM learns with comparable data efÔ¨Åciency to existing models on a range of synthetic tasks and one-shot Omniglot character recognition, and can scale to tasks requiring 100,000s of time steps and memories. As well, we show how our approach can be adapted for models that maintain temporal associations between memories, as with the recently introduced Differentiable Neural Computer.},
	language = {en},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Rae, Jack W. and Hunt, Jonathan J. and Harley, Tim and Danihelka, Ivo and Senior, Andrew and Wayne, Greg and Graves, Alex and Lillicrap, Timothy P.},
	month = oct,
	year = {2016},
	note = {arXiv:1610.09027 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{rae_scaling_2016-1,
	title = {Scaling {Memory}-{Augmented} {Neural} {Networks} with {Sparse} {Reads} and {Writes}},
	url = {http://arxiv.org/abs/1610.09027},
	abstract = {Neural networks augmented with external memory have the ability to learn algorithmic solutions to complex tasks. These models appear promising for applications such as language modeling and machine translation. However, they scale poorly in both space and time as the amount of memory grows ‚Äî limiting their applicability to real-world domains. Here, we present an end-to-end differentiable memory access scheme, which we call Sparse Access Memory (SAM), that retains the representational power of the original approaches whilst training efÔ¨Åciently with very large memories. We show that SAM achieves asymptotic lower bounds in space and time complexity, and Ô¨Ånd that an implementation runs 1,000√ó faster and with 3,000√ó less physical memory than non-sparse models. SAM learns with comparable data efÔ¨Åciency to existing models on a range of synthetic tasks and one-shot Omniglot character recognition, and can scale to tasks requiring 100,000s of time steps and memories. As well, we show how our approach can be adapted for models that maintain temporal associations between memories, as with the recently introduced Differentiable Neural Computer.},
	language = {en},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Rae, Jack W. and Hunt, Jonathan J. and Harley, Tim and Danihelka, Ivo and Senior, Andrew and Wayne, Greg and Graves, Alex and Lillicrap, Timothy P.},
	month = oct,
	year = {2016},
	note = {arXiv:1610.09027 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{baykal_theoretical_2022,
	title = {A {Theoretical} {View} on {Sparsely} {Activated} {Networks}},
	url = {http://arxiv.org/abs/2208.04461},
	abstract = {Deep and wide neural networks successfully Ô¨Åt very complex functions today, but dense models are starting to be prohibitively expensive for inference. To mitigate this, one promising direction is networks that activate a sparse subgraph of the network. The subgraph is chosen by a data-dependent routing function, enforcing a Ô¨Åxed mapping of inputs to subnetworks (e.g., the Mixture of Experts (MoE) paradigm in Switch Transformers). However, prior work is largely empirical, and while existing routing functions work well in practice, they do not lead to theoretical guarantees on approximation ability. We aim to provide a theoretical explanation for the power of sparse networks. As our Ô¨Årst contribution, we present a formal model of data-dependent sparse networks that captures salient aspects of popular architectures. We then introduce a routing function based on locality sensitive hashing (LSH) that enables us to reason about how well sparse networks approximate target functions. After representing LSH-based sparse networks with our model, we prove that sparse networks can match the approximation power of dense networks on Lipschitz functions. Applying LSH on the input vectors means that the experts interpolate the target function in diÔ¨Äerent subregions of the input space. To support our theory, we deÔ¨Åne various datasets based on Lipschitz target functions, and we show that sparse networks give a favorable trade-oÔ¨Ä between number of active units and approximation quality.},
	language = {en},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Baykal, Cenk and Dikkala, Nishanth and Panigrahy, Rina and Rashtchian, Cyrus and Wang, Xin},
	month = aug,
	year = {2022},
	note = {arXiv:2208.04461 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{wang_--fly_2020,
	address = {Online},
	title = {On-{The}-{Fly} {Information} {Retrieval} {Augmentation} for {Language} {Models}},
	url = {https://www.aclweb.org/anthology/2020.nuse-1.14},
	doi = {10.18653/v1/2020.nuse-1.14},
	abstract = {Here we experiment with the use of information retrieval as an augmentation for pre-trained language models. The text corpus used in information retrieval can be viewed as form of episodic memory which grows over time. By augmenting GPT 2.0 with information retrieval we achieve a zero shot 15\% relative reduction in perplexity on Gigaword corpus without any re-training. We also validate our IR augmentation on an event co-reference task.},
	language = {en},
	urldate = {2024-05-23},
	booktitle = {Proceedings of the {First} {Joint} {Workshop} on {Narrative} {Understanding}, {Storylines}, and {Events}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Hai and McAllester, David},
	year = {2020},
	pages = {114--119},
}

@misc{fan_augmenting_2020,
	title = {Augmenting {Transformers} with {KNN}-{Based} {Composite} {Memory} for {Dialogue}},
	url = {http://arxiv.org/abs/2004.12744},
	abstract = {Various machine learning tasks can beneÔ¨Åt from access to external information of different modalities, such as text and images. Recent work has focused on learning architectures with large memories capable of storing this knowledge. We propose augmenting generative Transformer neural networks with KNN-based Information Fetching (KIF) modules. Each KIF module learns a read operation to access Ô¨Åxed external knowledge. We apply these modules to generative dialog modeling, a challenging task where information must be Ô¨Çexibly retrieved and incorporated to maintain the topic and Ô¨Çow of conversation. We demonstrate the effectiveness of our approach by identifying relevant knowledge required for knowledgeable but engaging dialog from Wikipedia, images, and human-written dialog utterances, and show that leveraging this retrieved information improves model performance, measured by automatic and human evaluation.},
	language = {en},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Fan, Angela and Gardent, Claire and Braud, Chloe and Bordes, Antoine},
	month = nov,
	year = {2020},
	note = {arXiv:2004.12744 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{fan_augmenting_2020-1,
	title = {Augmenting {Transformers} with {KNN}-{Based} {Composite} {Memory} for {Dialogue}},
	url = {http://arxiv.org/abs/2004.12744},
	abstract = {Various machine learning tasks can beneÔ¨Åt from access to external information of different modalities, such as text and images. Recent work has focused on learning architectures with large memories capable of storing this knowledge. We propose augmenting generative Transformer neural networks with KNN-based Information Fetching (KIF) modules. Each KIF module learns a read operation to access Ô¨Åxed external knowledge. We apply these modules to generative dialog modeling, a challenging task where information must be Ô¨Çexibly retrieved and incorporated to maintain the topic and Ô¨Çow of conversation. We demonstrate the effectiveness of our approach by identifying relevant knowledge required for knowledgeable but engaging dialog from Wikipedia, images, and human-written dialog utterances, and show that leveraging this retrieved information improves model performance, measured by automatic and human evaluation.},
	language = {en},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Fan, Angela and Gardent, Claire and Braud, Chloe and Bordes, Antoine},
	month = nov,
	year = {2020},
	note = {arXiv:2004.12744 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{csordas_approximating_2023,
	title = {Approximating {Two}-{Layer} {Feedforward} {Networks} for {Efficient} {Transformers}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2310.10837},
	doi = {10.48550/ARXIV.2310.10837},
	abstract = {How to reduce compute and memory requirements of neural networks (NNs) without sacrificing performance? Many recent works use sparse Mixtures of Experts (MoEs) to build resource-efficient large language models (LMs). Here we introduce several novel perspectives on MoEs, presenting a general framework that unifies various methods to approximate two-layer NNs (e.g., feedforward blocks of Transformers), including product-key memories (PKMs). Leveraging insights from this framework, we propose methods to improve both MoEs and PKMs. Unlike prior work that compares MoEs with dense baselines under the compute-equal condition, our evaluation condition is parameter-equal, which is crucial to properly evaluate LMs. We show that our MoEs are competitive with the dense Transformer-XL on both the WikiText-103 and enwiki8 datasets at two different scales, while being much more resource efficient. This demonstrates that MoEs are relevant not only to extremely large LMs but also to any-scale resource-efficient LMs. Our code is public.},
	urldate = {2024-05-23},
	author = {Csord√°s, R√≥bert and Irie, Kazuki and Schmidhuber, J√ºrgen},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE)},
}

@inproceedings{zhang_accelerating_2018,
	address = {Melbourne, Australia},
	title = {Accelerating {Neural} {Transformer} via an {Average} {Attention} {Network}},
	url = {http://aclweb.org/anthology/P18-1166},
	doi = {10.18653/v1/P18-1166},
	language = {en},
	urldate = {2024-05-01},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Biao and Xiong, Deyi and Su, Jinsong},
	year = {2018},
	pages = {1789--1798},
}

@article{williams_roofline_2009,
	title = {Roofline: an insightful visual performance model for multicore architectures},
	volume = {52},
	issn = {0001-0782, 1557-7317},
	shorttitle = {Roofline},
	url = {https://dl.acm.org/doi/10.1145/1498765.1498785},
	doi = {10.1145/1498765.1498785},
	abstract = {The Roofline model offers insight on how to improve the performance of software and hardware.},
	language = {en},
	number = {4},
	urldate = {2024-05-01},
	journal = {Communications of the ACM},
	author = {Williams, Samuel and Waterman, Andrew and Patterson, David},
	month = apr,
	year = {2009},
	pages = {65--76},
}

@misc{michel_are_2019,
	title = {Are {Sixteen} {Heads} {Really} {Better} than {One}?},
	url = {http://arxiv.org/abs/1905.10650},
	abstract = {Attention is a powerful and ubiquitous mechanism for allowing neural models to focus on particular salient pieces of information by taking their weighted average when making predictions. In particular, multi-headed attention is a driving force behind many recent state-of-the-art NLP models such as Transformer-based MT models and BERT. These models apply multiple attention mechanisms in parallel, with each attention "head" potentially focusing on different parts of the input, which makes it possible to express sophisticated functions beyond the simple weighted average. In this paper we make the surprising observation that even if models have been trained using multiple heads, in practice, a large percentage of attention heads can be removed at test time without significantly impacting performance. In fact, some layers can even be reduced to a single head. We further examine greedy algorithms for pruning down models, and the potential speed, memory efficiency, and accuracy improvements obtainable therefrom. Finally, we analyze the results with respect to which parts of the model are more reliant on having multiple heads, and provide precursory evidence that training dynamics play a role in the gains provided by multi-head attention.},
	urldate = {2024-05-01},
	publisher = {arXiv},
	author = {Michel, Paul and Levy, Omer and Neubig, Graham},
	month = nov,
	year = {2019},
	keywords = {Computer Science - Computation and Language},
}

@misc{de_jong_fido_2023,
	title = {{FiDO}: {Fusion}-in-{Decoder} optimized for stronger performance and faster inference},
	shorttitle = {{FiDO}},
	url = {http://arxiv.org/abs/2212.08153},
	abstract = {Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that sets the state-of-the-art on many knowledge-intensive NLP tasks. However, the architecture used for FiD was chosen by making minimal modifications to a standard T5 model, which our analysis shows to be highly suboptimal for a retrieval-augmented model. In particular, FiD allocates the bulk of FLOPs to the encoder, while the majority of inference time results from memory bandwidth constraints in the decoder. We propose two simple changes to the FiD architecture to alleviate memory bandwidth constraints, and speed up inference by 7x. This allows us to use a much larger decoder at modest cost. We denote FiD with the above modifications as FiDO, and show that it strongly improves performance over existing FiD models for a wide range of inference budgets. For example, FiDO-Large-XXL performs faster inference than FiD-Base and achieves better performance than FiD-Large.},
	urldate = {2024-05-01},
	publisher = {arXiv},
	author = {de Jong, Michiel and Zemlyanskiy, Yury and Ainslie, Joshua and FitzGerald, Nicholas and Sanghai, Sumit and Sha, Fei and Cohen, William},
	month = jun,
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{shazeer_fast_2019,
	title = {Fast {Transformer} {Decoding}: {One} {Write}-{Head} is {All} {You} {Need}},
	shorttitle = {Fast {Transformer} {Decoding}},
	url = {http://arxiv.org/abs/1911.02150},
	abstract = {Multi-head attention layers, as used in the Transformer neural sequence model, are a powerful alternative to RNNs for moving information across and between sequences. While training these layers is generally fast and simple, due to parallelizability across the length of the sequence, incremental inference (where such paralleization is impossible) is often slow, due to the memory-bandwidth cost of repeatedly loading the large "keys" and "values" tensors. We propose a variant called multi-query attention, where the keys and values are shared across all of the different attention "heads", greatly reducing the size of these tensors and hence the memory bandwidth requirements of incremental decoding. We verify experimentally that the resulting models can indeed be much faster to decode, and incur only minor quality degradation from the baseline.},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Shazeer, Noam},
	month = nov,
	year = {2019},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{ainslie_gqa_2023,
	title = {{GQA}: {Training} {Generalized} {Multi}-{Query} {Transformer} {Models} from {Multi}-{Head} {Checkpoints}},
	shorttitle = {{GQA}},
	url = {http://arxiv.org/abs/2305.13245},
	abstract = {Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5\% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr√≥n, Federico and Sanghai, Sumit},
	month = dec,
	year = {2023},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{bahdanau_neural_2016,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	urldate = {2024-01-25},
	publisher = {arXiv},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = may,
	year = {2016},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{pope_efficiently_2022,
	title = {Efficiently {Scaling} {Transformer} {Inference}},
	url = {http://arxiv.org/abs/2211.05102},
	abstract = {We study the problem of efficient generative inference for Transformer models, in one of its most challenging settings: large deep models, with tight latency targets and long sequence lengths. Better understanding of the engineering tradeoffs for inference for large Transformer-based models is important as use cases of these models are growing rapidly throughout application areas. We develop a simple analytical model for inference efficiency to select the best multi-dimensional partitioning techniques optimized for TPU v4 slices based on the application requirements. We combine these with a suite of low-level optimizations to achieve a new Pareto frontier on the latency and model FLOPS utilization (MFU) tradeoffs on 500B+ parameter models that outperforms the FasterTransformer suite of benchmarks. We further show that with appropriate partitioning, the lower memory requirements of multiquery attention (i.e. multiple query heads share single key/value head) enables scaling up to 32x larger context lengths. Finally, we achieve a low-batch-size latency of 29ms per token during generation (using int8 weight quantization) and a 76\% MFU during large-batch-size processing of input tokens, while supporting a long 2048-token context length on the PaLM 540B parameter model.},
	urldate = {2024-05-01},
	publisher = {arXiv},
	author = {Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Levskaya, Anselm and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
	month = nov,
	year = {2022},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{belouadi_bygpt5_2023,
	title = {{ByGPT5}: {End}-to-{End} {Style}-conditioned {Poetry} {Generation} with {Token}-free {Language} {Models}},
	shorttitle = {{ByGPT5}},
	url = {http://arxiv.org/abs/2212.10474},
	doi = {10.18653/v1/2023.acl-long.406},
	abstract = {State-of-the-art poetry generation systems are often complex. They either consist of task-specific model pipelines, incorporate prior knowledge in the form of manually created constraints, or both. In contrast, end-to-end models would not suffer from the overhead of having to model prior knowledge and could learn the nuances of poetry from data alone, reducing the degree of human supervision required. In this work, we investigate end-to-end poetry generation conditioned on styles such as rhyme, meter, and alliteration. We identify and address lack of training data and mismatching tokenization algorithms as possible limitations of past attempts. In particular, we successfully pre-train ByGPT5, a new token-free decoder-only language model, and fine-tune it on a large custom corpus of English and German quatrains annotated with our styles. We show that ByGPT5 outperforms other models such as mT5, ByT5, GPT-2 and ChatGPT, while also being more parameter efficient and performing favorably compared to humans. In addition, we analyze its runtime performance and demonstrate that it is not prone to memorization. We make our code, models, and datasets publicly available.},
	urldate = {2024-05-23},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	author = {Belouadi, Jonas and Eger, Steffen},
	year = {2023},
	note = {arXiv:2212.10474 [cs]},
	keywords = {Computer Science - Computation and Language},
	pages = {7364--7381},
}

@misc{xue_byt5_2022,
	title = {{ByT5}: {Towards} a token-free future with pre-trained byte-to-byte models},
	shorttitle = {{ByT5}},
	url = {http://arxiv.org/abs/2105.13626},
	abstract = {Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. By comparison, token-free models that operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Xue, Linting and Barua, Aditya and Constant, Noah and Al-Rfou, Rami and Narang, Sharan and Kale, Mihir and Roberts, Adam and Raffel, Colin},
	month = mar,
	year = {2022},
	note = {arXiv:2105.13626 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{ramachandran_searching_2017,
	title = {Searching for {Activation} {Functions}},
	url = {http://arxiv.org/abs/1710.05941},
	doi = {10.48550/arXiv.1710.05941},
	abstract = {The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, \$f(x) = x {\textbackslash}cdot {\textbackslash}text\{sigmoid\}({\textbackslash}beta x)\$, which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9{\textbackslash}\% for Mobile NASNet-A and 0.6{\textbackslash}\% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V.},
	month = oct,
	year = {2017},
	note = {arXiv:1710.05941 [cs]
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Read},
}

@misc{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	urldate = {2024-05-15},
	publisher = {arXiv},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv:1503.02531 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{dettmers_llmint8_2022,
	title = {{LLM}.int8(): 8-bit {Matrix} {Multiplication} for {Transformers} at {Scale}},
	shorttitle = {{LLM}.int8()},
	url = {http://arxiv.org/abs/2208.07339},
	abstract = {Large language models have been widely adopted but require significant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. This is made possible by understanding and working around properties of highly systematic emergent features in transformer language models that dominate attention and transformer predictive performance. To cope with these features, we develop a two-part quantization procedure, LLM.int8(). We first use vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9\% of values are multiplied in 8-bit. Using LLM.int8(), we show empirically it is possible to perform inference in LLMs with up to 175B parameters without any performance degradation. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs. We open-source our software.},
	urldate = {2024-05-15},
	publisher = {arXiv},
	author = {Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
	month = nov,
	year = {2022},
	note = {arXiv:2208.07339 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{liu_kan_2024,
	title = {{KAN}: {Kolmogorov}-{Arnold} {Networks}},
	shorttitle = {{KAN}},
	url = {http://arxiv.org/abs/2404.19756},
	doi = {10.48550/arXiv.2404.19756},
	abstract = {Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes ("neurons"), KANs have learnable activation functions on edges ("weights"). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability. For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful collaborators helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today's deep learning models which rely heavily on MLPs.},
	urldate = {2024-05-04},
	publisher = {arXiv},
	author = {Liu, Ziming and Wang, Yixuan and Vaidya, Sachin and Ruehle, Fabian and Halverson, James and Soljaƒçiƒá, Marin and Hou, Thomas Y. and Tegmark, Max},
	month = may,
	year = {2024},
	note = {arXiv:2404.19756 [cond-mat, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Read, Statistics - Machine Learning},
}

@misc{lee_gelu_2023,
	title = {{GELU} {Activation} {Function} in {Deep} {Learning}: {A} {Comprehensive} {Mathematical} {Analysis} and {Performance}},
	shorttitle = {{GELU} {Activation} {Function} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/2305.12073},
	doi = {10.48550/arXiv.2305.12073},
	abstract = {Selecting the most suitable activation function is a critical factor in the effectiveness of deep learning models, as it influences their learning capacity, stability, and computational efficiency. In recent years, the Gaussian Error Linear Unit (GELU) activation function has emerged as a dominant method, surpassing traditional functions such as the Rectified Linear Unit (ReLU) in various applications. This study presents a rigorous mathematical investigation of the GELU activation function, exploring its differentiability, boundedness, stationarity, and smoothness properties in detail. Additionally, we conduct an extensive experimental comparison of the GELU function against a broad range of alternative activation functions, utilizing a residual convolutional network trained on the CIFAR-10, CIFAR-100, and STL-10 datasets as the empirical testbed. Our results demonstrate the superior performance of GELU compared to other activation functions, establishing its suitability for a wide range of deep learning applications. This comprehensive study contributes to a more profound understanding of the underlying mathematical properties of GELU and provides valuable insights for practitioners aiming to select activation functions that optimally align with their specific objectives and constraints in deep learning.},
	urldate = {2024-05-08},
	publisher = {arXiv},
	author = {Lee, Minhyeok},
	month = aug,
	year = {2023},
	note = {arXiv:2305.12073 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{warstadt_learning_2020-1,
	title = {Learning {Which} {Features} {Matter}: {RoBERTa} {Acquires} a {Preference} for {Linguistic} {Generalizations} ({Eventually})},
	shorttitle = {Learning {Which} {Features} {Matter}},
	url = {http://arxiv.org/abs/2010.05358},
	abstract = {One reason pretraining on self-supervised linguistic tasks is effective is that it teaches models features that are helpful for language understanding. However, we want pretrained models to learn not only to represent linguistic features, but also to use those features preferentially during fine-turning. With this goal in mind, we introduce a new English-language diagnostic set called MSGS (the Mixed Signals Generalization Set), which consists of 20 ambiguous binary classification tasks that we use to test whether a pretrained model prefers linguistic or surface generalizations during fine-tuning. We pretrain RoBERTa models from scratch on quantities of data ranging from 1M to 1B words and compare their performance on MSGS to the publicly available RoBERTa-base. We find that models can learn to represent linguistic features with little pretraining data, but require far more data to learn to prefer linguistic generalizations over surface ones. Eventually, with about 30B words of pretraining data, RoBERTa-base does demonstrate a linguistic bias with some regularity. We conclude that while self-supervised pretraining is an effective way to learn helpful inductive biases, there is likely room to improve the rate at which models learn which features matter.},
	urldate = {2024-05-07},
	publisher = {arXiv},
	author = {Warstadt, Alex and Zhang, Yian and Li, Haau-Sing and Liu, Haokun and Bowman, Samuel R.},
	month = oct,
	year = {2020},
	note = {arXiv:2010.05358 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{gee_multi-word_2023,
	title = {Multi-word {Tokenization} for {Sequence} {Compression}},
	url = {http://arxiv.org/abs/2402.09949},
	doi = {10.18653/v1/2023.emnlp-industry.58},
	abstract = {Large Language Models have proven highly successful at modelling a variety of tasks. However, this comes at a steep computational cost that hinders wider industrial uptake. In this paper, we present MWT: a Multi-Word Tokenizer that goes beyond word boundaries by representing frequent multi-word expressions as single tokens. MWTs produce a more compact and efficient tokenization that yields two benefits: (1) Increase in performance due to a greater coverage of input data given a fixed sequence length budget; (2) Faster and lighter inference due to the ability to reduce the sequence length with negligible drops in performance. Our results show that MWT is more robust across shorter sequence lengths, thus allowing for major speedups via early sequence truncation.},
	urldate = {2024-05-07},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {Industry} {Track}},
	author = {Gee, Leonidas and Rigutini, Leonardo and Ernandes, Marco and Zugarini, Andrea},
	year = {2023},
	note = {arXiv:2402.09949 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	pages = {612--621},
}

@misc{goyal_think_2024,
	title = {Think before you speak: {Training} {Language} {Models} {With} {Pause} {Tokens}},
	shorttitle = {Think before you speak},
	url = {http://arxiv.org/abs/2310.02226},
	abstract = {Language models generate responses by producing a series of tokens in immediate succession: the \$(K+1){\textasciicircum}\{th\}\$ token is an outcome of manipulating \$K\$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, \$K+10\$ hidden vectors, before it outputs the \$(K+1){\textasciicircum}\{th\}\$ token? We operationalize this idea by performing training and inference on language models with a (learnable) \${\textbackslash}textit\{pause\}\$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate \${\textbackslash}textit\{pause-training\}\$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of \$18{\textbackslash}\%\$ EM score on the QA task of SQuAD, \$8{\textbackslash}\%\$ on CommonSenseQA and \$1{\textbackslash}\%\$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.},
	urldate = {2024-05-07},
	publisher = {arXiv},
	author = {Goyal, Sachin and Ji, Ziwei and Rawat, Ankit Singh and Menon, Aditya Krishna and Kumar, Sanjiv and Nagarajan, Vaishnavh},
	month = apr,
	year = {2024},
	note = {arXiv:2310.02226 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{wang_glue_2019-1,
	title = {{GLUE}: {A} {Multi}-{Task} {Benchmark} and {Analysis} {Platform} for {Natural} {Language} {Understanding}},
	shorttitle = {{GLUE}},
	url = {http://arxiv.org/abs/1804.07461},
	abstract = {For natural language understanding (NLU) technology to be maximally useful, both practically and as a scientific object of study, it must be general: it must be able to process language in a way that is not exclusively tailored to any one specific task or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation benchmark (GLUE), a tool for evaluating and analyzing the performance of models across a diverse range of existing NLU tasks. GLUE is model-agnostic, but it incentivizes sharing knowledge across tasks because certain tasks have very limited training data. We further provide a hand-crafted diagnostic test suite that enables detailed linguistic analysis of NLU models. We evaluate baselines based on current methods for multi-task and transfer learning and find that they do not immediately give substantial improvements over the aggregate performance of training a separate model per task, indicating room for improvement in developing general and robust NLU systems.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
	month = feb,
	year = {2019},
	note = {arXiv:1804.07461 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{wang_superglue_2020-2,
	title = {{SuperGLUE}: {A} {Stickier} {Benchmark} for {General}-{Purpose} {Language} {Understanding} {Systems}},
	shorttitle = {{SuperGLUE}},
	url = {http://arxiv.org/abs/1905.00537},
	abstract = {In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at super.gluebenchmark.com.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
	month = feb,
	year = {2020},
	note = {arXiv:1905.00537 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{kudo_sentencepiece_2018,
	title = {{SentencePiece}: {A} simple and language independent subword tokenizer and detokenizer for {Neural} {Text} {Processing}},
	shorttitle = {{SentencePiece}},
	url = {http://arxiv.org/abs/1808.06226},
	abstract = {This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at https://github.com/google/sentencepiece.},
	urldate = {2024-05-07},
	publisher = {arXiv},
	author = {Kudo, Taku and Richardson, John},
	month = aug,
	year = {2018},
	note = {arXiv:1808.06226 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{huh_low-rank_2023,
	title = {The {Low}-{Rank} {Simplicity} {Bias} in {Deep} {Networks}},
	url = {http://arxiv.org/abs/2103.10427},
	abstract = {Modern deep neural networks are highly over-parameterized compared to the data on which they are trained, yet they often generalize remarkably well. A flurry of recent work has asked: why do deep networks not overfit to their training data? In this work, we make a series of empirical observations that investigate and extend the hypothesis that deeper networks are inductively biased to find solutions with lower effective rank embeddings. We conjecture that this bias exists because the volume of functions that maps to low effective rank embedding increases with depth. We show empirically that our claim holds true on finite width linear and non-linear models on practical learning paradigms and show that on natural data, these are often the solutions that generalize well. We then show that the simplicity bias exists at both initialization and after training and is resilient to hyper-parameters and learning methods. We further demonstrate how linear over-parameterization of deep non-linear models can be used to induce low-rank bias, improving generalization performance on CIFAR and ImageNet without changing the modeling capacity.},
	urldate = {2024-02-13},
	publisher = {arXiv},
	author = {Huh, Minyoung and Mobahi, Hossein and Zhang, Richard and Cheung, Brian and Agrawal, Pulkit and Isola, Phillip},
	month = mar,
	year = {2023},
	note = {arXiv:2103.10427 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Read},
}

@inproceedings{warstadt_findings_2023-1,
	address = {Singapore},
	title = {Findings of the {BabyLM} {Challenge}: {Sample}-{Efficient} {Pretraining} on {Developmentally} {Plausible} {Corpora}},
	shorttitle = {Findings of the {BabyLM} {Challenge}},
	url = {https://aclanthology.org/2023.conll-babylm.1},
	doi = {10.18653/v1/2023.conll-babylm.1},
	language = {en},
	urldate = {2024-05-06},
	booktitle = {Proceedings of the {BabyLM} {Challenge} at the 27th {Conference} on {Computational} {Natural} {Language} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Warstadt, Alex and Mueller, Aaron and Choshen, Leshem and Wilcox, Ethan and Zhuang, Chengxu and Ciro, Juan and Mosquera, Rafael and Paranjabe, Bhargavi and Williams, Adina and Linzen, Tal and Cotterell, Ryan},
	year = {2023},
	pages = {1--6},
}

@misc{shleifer_normformer_2021,
	title = {{NormFormer}: {Improved} {Transformer} {Pretraining} with {Extra} {Normalization}},
	shorttitle = {{NormFormer}},
	url = {http://arxiv.org/abs/2110.09456},
	doi = {10.48550/arXiv.2110.09456},
	abstract = {During pretraining, the Pre-LayerNorm transformer suffers from a gradient magnitude mismatch: gradients at early layers are much larger than at later layers. These issues can be alleviated by our proposed NormFormer architecture, which adds three normalization operations to each layer: a Layer Norm after self attention, head-wise scaling of self-attention outputs, and a Layer Norm after the first fully connected layer. The extra operations incur negligible compute cost (+0.4\% parameter increase), but improve pretraining perplexity and downstream task performance for both causal and masked language models ranging from 125 Million to 2.7 Billion parameters. For example, adding NormFormer on top of our strongest 1.3B parameter baseline can reach equal perplexity 24\% faster, or converge 0.27 perplexity better in the same compute budget. This model reaches GPT3-Large (1.3B) zero shot performance 60\% faster. For masked language modeling, NormFormer improves fine-tuned GLUE performance by 1.9\% on average. Code to train NormFormer models is available in fairseq https://github.com/pytorch/fairseq/tree/main/examples/normformer .},
	urldate = {2024-05-05},
	publisher = {arXiv},
	author = {Shleifer, Sam and Weston, Jason and Ott, Myle},
	month = nov,
	year = {2021},
	note = {arXiv:2110.09456 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{ba_layer_2016,
	title = {Layer {Normalization}},
	url = {http://arxiv.org/abs/1607.06450},
	doi = {10.48550/arXiv.1607.06450},
	abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
	urldate = {2024-05-05},
	publisher = {arXiv},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	month = jul,
	year = {2016},
	note = {arXiv:1607.06450 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{noauthor_parameter-efficient_nodate,
	title = {Parameter-efficient fine-tuning of large-scale pre-trained language models {\textbar} {Nature} {Machine} {Intelligence}},
	url = {https://www.nature.com/articles/s42256-023-00626-4},
	urldate = {2024-05-05},
}

@article{rajanand_erfrelu_nodate,
	title = {{ErfReLU}: {Adaptive} {Activation} {Function} for {Deep} {Neural} {Network}},
	abstract = {Recent research has found that the activation function (AF) selected for adding non-linearity into the output can have a big impact on how effectively deep learning networks perform. Developing activation functions that can adapt simultaneously with learning is a need of time. Researchers recently started developing activation functions that can be trained throughout the learning process, known as trainable, or adaptive activation functions (AAF). Research on AAF that enhance the outcomes is still in its early stages. In this paper, a novel activation function ‚ÄòErfReLU‚Äô has been developed based on the erf function and ReLU. This function exploits the ReLU and the error function (erf) to its advantage. State of art activation functions like Sigmoid, ReLU, Tanh, and their properties have been briefly explained. Adaptive activation functions like Tanhsoft1, Tanhsoft2, Tanhsoft3, TanhLU, SAAF, ErfAct, Pserf, Smish, and Serf have also been described. Lastly, performance analysis of 9 trainable activation functions along with the proposed one namely Tanhsoft1, Tanhsoft2, Tanhsoft3, TanhLU, SAAF, ErfAct, Pserf, Smish, and Serf has been shown by applying these activation functions in MobileNet, VGG16, and ResNet models on CIFAR-10, MNIST, and FMNIST benchmark datasets.},
	language = {en},
	author = {Rajanand, Ashish and Singh, Pradeep},
}

@article{rajanand_erfrelu_nodate-1,
	title = {{ErfReLU}: {Adaptive} {Activation} {Function} for {Deep} {Neural} {Network}},
	abstract = {Recent research has found that the activation function (AF) selected for adding non-linearity into the output can have a big impact on how effectively deep learning networks perform. Developing activation functions that can adapt simultaneously with learning is a need of time. Researchers recently started developing activation functions that can be trained throughout the learning process, known as trainable, or adaptive activation functions (AAF). Research on AAF that enhance the outcomes is still in its early stages. In this paper, a novel activation function ‚ÄòErfReLU‚Äô has been developed based on the erf function and ReLU. This function exploits the ReLU and the error function (erf) to its advantage. State of art activation functions like Sigmoid, ReLU, Tanh, and their properties have been briefly explained. Adaptive activation functions like Tanhsoft1, Tanhsoft2, Tanhsoft3, TanhLU, SAAF, ErfAct, Pserf, Smish, and Serf have also been described. Lastly, performance analysis of 9 trainable activation functions along with the proposed one namely Tanhsoft1, Tanhsoft2, Tanhsoft3, TanhLU, SAAF, ErfAct, Pserf, Smish, and Serf has been shown by applying these activation functions in MobileNet, VGG16, and ResNet models on CIFAR-10, MNIST, and FMNIST benchmark datasets.},
	language = {en},
	author = {Rajanand, Ashish and Singh, Pradeep},
}

@article{shleifer_normformer_2021-1,
	title = {{NormFormer}: {Improved} {Transformer} {Pretraining} with {Extra} {Normalization}},
	shorttitle = {{NormFormer}},
	url = {https://openreview.net/forum?id=GMYWzWztDx5},
	abstract = {During pretraining, the Pre-LayerNorm transformer suffers from a gradient magnitude mismatch: gradients at early layers are much larger than at later layers, while the optimal weighting of residuals is larger at earlier than at later layers. These issues can be alleviated by the addition of two normalization and two new scaling operations inside each layer. The extra operations incur negligible compute cost (+0.5{\textbackslash}\% parameter increase), but improve pretraining perplexity and downstream task performance for both causal and masked language models of multiple sizes. Adding NormFormer on top of the GPT3-Medium architecture can reach the SOTA perplexity 22{\textbackslash}\% faster, or converge 0.33 perplexity better in the same compute budget. This results in significantly stronger zero shot performance. For masked language modeling, NormFormer improves fine-tuned GLUE performance by 1.9{\textbackslash}\% on average.},
	language = {en},
	urldate = {2024-05-05},
	author = {Shleifer, Sam and Ott, Myle},
	month = oct,
	year = {2021},
}

@misc{kamau_simplified_2024,
	title = {A {Simplified} {Explanation} {Of} {The} {New} {Kolmogorov}-{Arnold} {Network} ({KAN}) from {MIT}},
	url = {https://medium.com/@isaakmwangi2018/a-simplified-explanation-of-the-new-kolmogorov-arnold-network-kan-from-mit-cbb59793a040},
	abstract = {Exploring the Next Frontier in AI: The Kolmogorov-Arnold Network (KAN)},
	language = {en},
	urldate = {2024-05-04},
	journal = {Medium},
	author = {Kamau, Isaak},
	month = may,
	year = {2024},
}

@misc{shleifer_normformer_2021-2,
	title = {{NormFormer}: {Improved} {Transformer} {Pretraining} with {Extra} {Normalization}},
	shorttitle = {{NormFormer}},
	url = {http://arxiv.org/abs/2110.09456},
	doi = {10.48550/arXiv.2110.09456},
	abstract = {During pretraining, the Pre-LayerNorm transformer suffers from a gradient magnitude mismatch: gradients at early layers are much larger than at later layers. These issues can be alleviated by our proposed NormFormer architecture, which adds three normalization operations to each layer: a Layer Norm after self attention, head-wise scaling of self-attention outputs, and a Layer Norm after the first fully connected layer. The extra operations incur negligible compute cost (+0.4\% parameter increase), but improve pretraining perplexity and downstream task performance for both causal and masked language models ranging from 125 Million to 2.7 Billion parameters. For example, adding NormFormer on top of our strongest 1.3B parameter baseline can reach equal perplexity 24\% faster, or converge 0.27 perplexity better in the same compute budget. This model reaches GPT3-Large (1.3B) zero shot performance 60\% faster. For masked language modeling, NormFormer improves fine-tuned GLUE performance by 1.9\% on average. Code to train NormFormer models is available in fairseq https://github.com/pytorch/fairseq/tree/main/examples/normformer .},
	urldate = {2024-05-03},
	publisher = {arXiv},
	author = {Shleifer, Sam and Weston, Jason and Ott, Myle},
	month = nov,
	year = {2021},
	note = {arXiv:2110.09456 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{samuel_trained_2023,
	title = {Trained on 100 million words and still in shape: {BERT} meets {British} {National} {Corpus}},
	shorttitle = {Trained on 100 million words and still in shape},
	url = {http://arxiv.org/abs/2303.09859},
	doi = {10.48550/arXiv.2303.09859},
	abstract = {While modern masked language models (LMs) are trained on ever larger corpora, we here explore the effects of down-scaling training to a modestly-sized but representative, well-balanced, and publicly available English text source -- the British National Corpus. We show that pre-training on this carefully curated corpus can reach better performance than the original BERT model. We argue that this type of corpora has great potential as a language modeling benchmark. To showcase this potential, we present fair, reproducible and data-efficient comparative studies of LMs, in which we evaluate several training objectives and model architectures and replicate previous empirical results in a systematic way. We propose an optimized LM architecture called LTG-BERT.},
	urldate = {2024-05-02},
	publisher = {arXiv},
	author = {Samuel, David and Kutuzov, Andrey and √òvrelid, Lilja and Velldal, Erik},
	month = may,
	year = {2023},
	note = {arXiv:2303.09859 [cs]},
	keywords = {Computer Science - Computation and Language, Read},
}

@misc{kudo_subword_2018,
	title = {Subword {Regularization}: {Improving} {Neural} {Network} {Translation} {Models} with {Multiple} {Subword} {Candidates}},
	shorttitle = {Subword {Regularization}},
	url = {http://arxiv.org/abs/1804.10959},
	abstract = {Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT). While sentences are usually converted into unique subword sequences, subword segmentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robustness of NMT. We present a simple regularization method, subword regularization, which trains the model with multiple subword segmentations probabilistically sampled during training. In addition, for better subword sampling, we propose a new subword segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings.},
	urldate = {2024-05-02},
	publisher = {arXiv},
	author = {Kudo, Taku},
	month = apr,
	year = {2018},
	note = {arXiv:1804.10959 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{povey_time-restricted_2018,
	address = {Calgary, AB},
	title = {A {Time}-{Restricted} {Self}-{Attention} {Layer} for {ASR}},
	isbn = {978-1-5386-4658-8},
	url = {https://ieeexplore.ieee.org/document/8462497/},
	doi = {10.1109/ICASSP.2018.8462497},
	urldate = {2024-05-02},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Povey, Daniel and Hadian, Hossein and Ghahremani, Pegah and Li, Ke and Khudanpur, Sanjeev},
	month = apr,
	year = {2018},
	pages = {5874--5878},
}

@inproceedings{schuster_japanese_2012,
	title = {Japanese and {Korean} voice search},
	url = {https://ieeexplore.ieee.org/document/6289079/;jsessionid=814AEDE8C401505471D3D1E47E765AB6},
	doi = {10.1109/ICASSP.2012.6289079},
	abstract = {This paper describes challenges and solutions for building a successful voice search system as applied to Japanese and Korean at Google. We describe the techniques used to deal with an infinite vocabulary, how modeling completely in the written domain for language model and dictionary can avoid some system complexity, and how we built dictionaries, language and acoustic models in this framework. We show how to deal with the difficulty of scoring results for multiple script languages because of ambiguities. The development of voice search for these languages led to a significant simplification of the original process to build a system for any new language which in in parts became our default process for internationalization of voice search.},
	urldate = {2024-05-02},
	booktitle = {2012 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Schuster, Mike and Nakajima, Kaisuke},
	month = mar,
	year = {2012},
	note = {ISSN: 2379-190X},
	keywords = {Decision support systems, Helium, Japanese, Korean, Speech recognition, voice search},
	pages = {5149--5152},
}

@inproceedings{murray_auto-sizing_2019,
	address = {Hong Kong},
	title = {Auto-{Sizing} the {Transformer} {Network}: {Improving} {Speed}, {Efficiency}, and {Performance} for {Low}-{Resource} {Machine} {Translation}},
	shorttitle = {Auto-{Sizing} the {Transformer} {Network}},
	url = {https://aclanthology.org/D19-5625},
	doi = {10.18653/v1/D19-5625},
	abstract = {Neural sequence-to-sequence models, particularly the Transformer, are the state of the art in machine translation. Yet these neural networks are very sensitive to architecture and hyperparameter settings. Optimizing these settings by grid or random search is computationally expensive because it requires many training runs. In this paper, we incorporate architecture search into a single training run through auto-sizing, which uses regularization to delete neurons in a network over the course of training. On very low-resource language pairs, we show that auto-sizing can improve BLEU scores by up to 3.9 points while removing one-third of the parameters from the model.},
	urldate = {2024-05-02},
	booktitle = {Proceedings of the 3rd {Workshop} on {Neural} {Generation} and {Translation}},
	publisher = {Association for Computational Linguistics},
	author = {Murray, Kenton and Kinnison, Jeffery and Nguyen, Toan Q. and Scheirer, Walter and Chiang, David},
	editor = {Birch, Alexandra and Finch, Andrew and Hayashi, Hiroaki and Konstas, Ioannis and Luong, Thang and Neubig, Graham and Oda, Yusuke and Sudoh, Katsuhito},
	month = nov,
	year = {2019},
	pages = {231--240},
}

@inproceedings{zhang_accelerating_2018-1,
	address = {Melbourne, Australia},
	title = {Accelerating {Neural} {Transformer} via an {Average} {Attention} {Network}},
	url = {http://aclweb.org/anthology/P18-1166},
	doi = {10.18653/v1/P18-1166},
	language = {en},
	urldate = {2024-05-01},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Biao and Xiong, Deyi and Su, Jinsong},
	year = {2018},
	pages = {1789--1798},
}

@misc{michel_are_2019-1,
	title = {Are {Sixteen} {Heads} {Really} {Better} than {One}?},
	url = {http://arxiv.org/abs/1905.10650},
	abstract = {Attention is a powerful and ubiquitous mechanism for allowing neural models to focus on particular salient pieces of information by taking their weighted average when making predictions. In particular, multi-headed attention is a driving force behind many recent state-of-the-art NLP models such as Transformer-based MT models and BERT. These models apply multiple attention mechanisms in parallel, with each attention "head" potentially focusing on different parts of the input, which makes it possible to express sophisticated functions beyond the simple weighted average. In this paper we make the surprising observation that even if models have been trained using multiple heads, in practice, a large percentage of attention heads can be removed at test time without significantly impacting performance. In fact, some layers can even be reduced to a single head. We further examine greedy algorithms for pruning down models, and the potential speed, memory efficiency, and accuracy improvements obtainable therefrom. Finally, we analyze the results with respect to which parts of the model are more reliant on having multiple heads, and provide precursory evidence that training dynamics play a role in the gains provided by multi-head attention.},
	urldate = {2024-05-01},
	publisher = {arXiv},
	author = {Michel, Paul and Levy, Omer and Neubig, Graham},
	month = nov,
	year = {2019},
	note = {arXiv:1905.10650 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{williams_roofline_2009-1,
	title = {Roofline: an insightful visual performance model for multicore architectures},
	volume = {52},
	issn = {0001-0782, 1557-7317},
	shorttitle = {Roofline},
	url = {https://dl.acm.org/doi/10.1145/1498765.1498785},
	doi = {10.1145/1498765.1498785},
	abstract = {The Roofline model offers insight on how to improve the performance of software and hardware.},
	language = {en},
	number = {4},
	urldate = {2024-05-01},
	journal = {Communications of the ACM},
	author = {Williams, Samuel and Waterman, Andrew and Patterson, David},
	month = apr,
	year = {2009},
	pages = {65--76},
}

@misc{de_jong_fido_2023-1,
	title = {{FiDO}: {Fusion}-in-{Decoder} optimized for stronger performance and faster inference},
	shorttitle = {{FiDO}},
	url = {http://arxiv.org/abs/2212.08153},
	abstract = {Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that sets the state-of-the-art on many knowledge-intensive NLP tasks. However, the architecture used for FiD was chosen by making minimal modifications to a standard T5 model, which our analysis shows to be highly suboptimal for a retrieval-augmented model. In particular, FiD allocates the bulk of FLOPs to the encoder, while the majority of inference time results from memory bandwidth constraints in the decoder. We propose two simple changes to the FiD architecture to alleviate memory bandwidth constraints, and speed up inference by 7x. This allows us to use a much larger decoder at modest cost. We denote FiD with the above modifications as FiDO, and show that it strongly improves performance over existing FiD models for a wide range of inference budgets. For example, FiDO-Large-XXL performs faster inference than FiD-Base and achieves better performance than FiD-Large.},
	urldate = {2024-05-01},
	publisher = {arXiv},
	author = {de Jong, Michiel and Zemlyanskiy, Yury and Ainslie, Joshua and FitzGerald, Nicholas and Sanghai, Sumit and Sha, Fei and Cohen, William},
	month = jun,
	year = {2023},
	note = {arXiv:2212.08153 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{pope_efficiently_2022-1,
	title = {Efficiently {Scaling} {Transformer} {Inference}},
	url = {http://arxiv.org/abs/2211.05102},
	abstract = {We study the problem of efficient generative inference for Transformer models, in one of its most challenging settings: large deep models, with tight latency targets and long sequence lengths. Better understanding of the engineering tradeoffs for inference for large Transformer-based models is important as use cases of these models are growing rapidly throughout application areas. We develop a simple analytical model for inference efficiency to select the best multi-dimensional partitioning techniques optimized for TPU v4 slices based on the application requirements. We combine these with a suite of low-level optimizations to achieve a new Pareto frontier on the latency and model FLOPS utilization (MFU) tradeoffs on 500B+ parameter models that outperforms the FasterTransformer suite of benchmarks. We further show that with appropriate partitioning, the lower memory requirements of multiquery attention (i.e. multiple query heads share single key/value head) enables scaling up to 32x larger context lengths. Finally, we achieve a low-batch-size latency of 29ms per token during generation (using int8 weight quantization) and a 76\% MFU during large-batch-size processing of input tokens, while supporting a long 2048-token context length on the PaLM 540B parameter model.},
	urldate = {2024-05-01},
	publisher = {arXiv},
	author = {Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Levskaya, Anselm and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
	month = nov,
	year = {2022},
	note = {arXiv:2211.05102 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{luo_understanding_2024,
	title = {From {Understanding} to {Utilization}: {A} {Survey} on {Explainability} for {Large} {Language} {Models}},
	shorttitle = {From {Understanding} to {Utilization}},
	url = {http://arxiv.org/abs/2401.12874},
	abstract = {Explainability for Large Language Models (LLMs) is a critical yet challenging aspect of natural language processing. As LLMs are increasingly integral to diverse applications, their ‚Äúblack-box‚Äù nature sparks significant concerns regarding transparency and ethical use. This survey underscores the imperative for increased explainability in LLMs, delving into both the research on explainability and the various methodologies and tasks that utilize an understanding of these models. Our focus is primarily on pre-trained Transformerbased LLMs, such as LLaMA (Touvron et al., 2023), which pose distinctive interpretability challenges due to their scale and complexity. In terms of existing methods, we classify them into local and global analyses, based on their explanatory objectives. When considering the utilization of explainability, we explore several compelling methods that concentrate on model editing, control generation, and model enhancement. Additionally, we examine representative evaluation metrics and datasets, elucidating their advantages and limitations. Our goal is to reconcile theoretical and empirical understanding with practical implementation, proposing exciting avenues for explanatory techniques and their applications in the LLMs era.},
	language = {en},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Luo, Haoyan and Specia, Lucia},
	month = feb,
	year = {2024},
	note = {arXiv:2401.12874 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{zhao_explainability_2023,
	title = {Explainability for {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Explainability for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2309.01029},
	abstract = {Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this paper, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations, and discuss how explanations can be leveraged to debug models and improve performance. Lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional deep learning models.},
	language = {en},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Zhao, Haiyan and Chen, Hanjie and Yang, Fan and Liu, Ninghao and Deng, Huiqi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Du, Mengnan},
	month = nov,
	year = {2023},
	note = {arXiv:2309.01029 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{luo_understanding_2024-1,
	title = {From {Understanding} to {Utilization}: {A} {Survey} on {Explainability} for {Large} {Language} {Models}},
	shorttitle = {From {Understanding} to {Utilization}},
	url = {http://arxiv.org/abs/2401.12874},
	abstract = {Explainability for Large Language Models (LLMs) is a critical yet challenging aspect of natural language processing. As LLMs are increasingly integral to diverse applications, their ‚Äúblack-box‚Äù nature sparks significant concerns regarding transparency and ethical use. This survey underscores the imperative for increased explainability in LLMs, delving into both the research on explainability and the various methodologies and tasks that utilize an understanding of these models. Our focus is primarily on pre-trained Transformerbased LLMs, such as LLaMA (Touvron et al., 2023), which pose distinctive interpretability challenges due to their scale and complexity. In terms of existing methods, we classify them into local and global analyses, based on their explanatory objectives. When considering the utilization of explainability, we explore several compelling methods that concentrate on model editing, control generation, and model enhancement. Additionally, we examine representative evaluation metrics and datasets, elucidating their advantages and limitations. Our goal is to reconcile theoretical and empirical understanding with practical implementation, proposing exciting avenues for explanatory techniques and their applications in the LLMs era.},
	language = {en},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Luo, Haoyan and Specia, Lucia},
	month = feb,
	year = {2024},
	note = {arXiv:2401.12874 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{nag_serf_2023,
	title = {{SERF}: {Towards} {Better} {Training} of {Deep} {Neural} {Networks} {Using} {Log}-{Softplus} {ERror} {Activation} {Function}},
	shorttitle = {{SERF}},
	url = {https://openaccess.thecvf.com/content/WACV2023/html/Nag_SERF_Towards_Better_Training_of_Deep_Neural_Networks_Using_Log-Softplus_WACV_2023_paper.html},
	language = {en},
	urldate = {2024-04-28},
	author = {Nag, Sayan and Bhattacharyya, Mayukh and Mukherjee, Anuraag and Kundu, Rohit},
	year = {2023},
	pages = {5324--5333},
}

@misc{dubey_activation_2022,
	title = {Activation {Functions} in {Deep} {Learning}: {A} {Comprehensive} {Survey} and {Benchmark}},
	shorttitle = {Activation {Functions} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/2109.14545},
	doi = {10.48550/arXiv.2109.14545},
	abstract = {Neural networks have shown tremendous growth in recent years to solve numerous problems. Various types of neural networks have been introduced to deal with different types of problems. However, the main goal of any neural network is to transform the non-linearly separable input data into more linearly separable abstract features using a hierarchy of layers. These layers are combinations of linear and nonlinear functions. The most popular and common non-linearity layers are activation functions (AFs), such as Logistic Sigmoid, Tanh, ReLU, ELU, Swish and Mish. In this paper, a comprehensive overview and survey is presented for AFs in neural networks for deep learning. Different classes of AFs such as Logistic Sigmoid and Tanh based, ReLU based, ELU based, and Learning based are covered. Several characteristics of AFs such as output range, monotonicity, and smoothness are also pointed out. A performance comparison is also performed among 18 state-of-the-art AFs with different networks on different types of data. The insights of AFs are presented to benefit the researchers for doing further research and practitioners to select among different choices. The code used for experimental comparison is released at: {\textbackslash}url\{https://github.com/shivram1987/ActivationFunctions\}.},
	urldate = {2024-04-28},
	publisher = {arXiv},
	author = {Dubey, Shiv Ram and Singh, Satish Kumar and Chaudhuri, Bidyut Baran},
	month = jun,
	year = {2022},
	note = {arXiv:2109.14545 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{dubey_activation_2022-1,
	title = {Activation {Functions} in {Deep} {Learning}: {A} {Comprehensive} {Survey} and {Benchmark}},
	shorttitle = {Activation {Functions} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/2109.14545},
	abstract = {Neural networks have shown tremendous growth in recent years to solve numerous problems. Various types of neural networks have been introduced to deal with different types of problems. However, the main goal of any neural network is to transform the non-linearly separable input data into more linearly separable abstract features using a hierarchy of layers. These layers are combinations of linear and nonlinear functions. The most popular and common non-linearity layers are activation functions (AFs), such as Logistic Sigmoid, Tanh, ReLU, ELU, Swish and Mish. In this paper, a comprehensive overview and survey is presented for AFs in neural networks for deep learning. Different classes of AFs such as Logistic Sigmoid and Tanh based, ReLU based, ELU based, and Learning based are covered. Several characteristics of AFs such as output range, monotonicity, and smoothness are also pointed out. A performance comparison is also performed among 18 state-of-the-art AFs with different networks on different types of data. The insights of AFs are presented to beneÔ¨Åt the researchers for doing further research and practitioners to select among different choices. The code used for experimental comparison is released at: https://github.com/shivram1987/ ActivationFunctions.},
	language = {en},
	urldate = {2024-04-28},
	publisher = {arXiv},
	author = {Dubey, Shiv Ram and Singh, Satish Kumar and Chaudhuri, Bidyut Baran},
	month = jun,
	year = {2022},
	note = {arXiv:2109.14545 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{noauthor_weights_nodate,
	title = {Weights \& {Biases}},
	url = {https://wandb.ai/ayush-thakur/dl-question-bank/reports/ReLU-vs-Sigmoid-Function-in-Deep-Neural-Networks--VmlldzoyMDk0MzI},
	abstract = {Weights \& Biases, developer tools for machine learning},
	language = {en},
	urldate = {2024-04-28},
	journal = {W\&B},
}

@misc{noauthor_transformerssrctransformersmodelsrobertamodeling_robertapy_nodate,
	title = {transformers/src/transformers/models/roberta/modeling\_roberta.py at main ¬∑ huggingface/transformers},
	url = {https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py},
	abstract = {ü§ó Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX. - huggingface/transformers},
	language = {en},
	urldate = {2024-04-28},
	journal = {GitHub},
}

@misc{noauthor_transformerssrctransformersmodelsgpt_neomodeling_gpt_neopy_nodate,
	title = {transformers/src/transformers/models/gpt\_neo/modeling\_gpt\_neo.py at main ¬∑ huggingface/transformers},
	url = {https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt_neo/modeling_gpt_neo.py},
	abstract = {ü§ó Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX. - huggingface/transformers},
	language = {en},
	urldate = {2024-04-28},
	journal = {GitHub},
}

@misc{zhang_moefication_2022,
	title = {{MoEfication}: {Transformer} {Feed}-forward {Layers} are {Mixtures} of {Experts}},
	shorttitle = {{MoEfication}},
	url = {http://arxiv.org/abs/2110.01786},
	abstract = {Recent work has shown that feed-forward networks (FFNs) in pre-trained Transformers are a key component, storing various linguistic and factual knowledge. However, the computational patterns of FFNs are still unclear. In this work, we study the computational patterns of FFNs and observe that most inputs only activate a tiny ratio of neurons of FFNs. This phenomenon is similar to the sparsity of the human brain, which drives research on functional partitions of the human brain. To verify whether functional partitions also emerge in FFNs, we propose to convert a model into its MoE version with the same parameters, namely MoEÔ¨Åcation. SpeciÔ¨Åcally, MoEÔ¨Åcation consists of two phases: (1) splitting the parameters of FFNs into multiple functional partitions as experts, and (2) building expert routers to decide which experts will be used for each input. Experimental results show that MoEÔ¨Åcation can conditionally use 10\% to 30\% of FFN parameters while maintaining over 95\% original performance for different models on various downstream tasks. Besides, MoEÔ¨Åcation brings two advantages: (1) it signiÔ¨Åcantly reduces the FLOPS of inference, i.e., 2x speedup with 25\% of FFN parameters, and (2) it provides a Ô¨Åne-grained perspective to study the inner mechanism of FFNs. The source code of this paper can be obtained from https://github.com/ thunlp/MoEfication.},
	language = {en},
	urldate = {2024-04-28},
	publisher = {arXiv},
	author = {Zhang, Zhengyan and Lin, Yankai and Liu, Zhiyuan and Li, Peng and Sun, Maosong and Zhou, Jie},
	month = apr,
	year = {2022},
	note = {arXiv:2110.01786 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{noauthor_papers_nodate,
	title = {Papers with {Code} - {GELU} {Explained}},
	url = {https://paperswithcode.com/method/gelu},
	abstract = {The Gaussian Error Linear Unit, or GELU,  is an activation function. The GELU activation function is \$x{\textbackslash}Phi(x)\$, where \${\textbackslash}Phi(x)\$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their percentile, rather than gates inputs by their sign as in ReLUs (\$x{\textbackslash}mathbf\{1\}\_\{x{\textgreater}0\}\$). Consequently the GELU can be thought of as a smoother ReLU.

\$\${\textbackslash}text\{GELU\}{\textbackslash}left(x{\textbackslash}right) = x\{P\}{\textbackslash}left(X{\textbackslash}leq\{x\}{\textbackslash}right) = x{\textbackslash}Phi{\textbackslash}left(x{\textbackslash}right) = x {\textbackslash}cdot {\textbackslash}frac\{1\}\{2\}{\textbackslash}left[1 + {\textbackslash}text\{erf\}(x/{\textbackslash}sqrt\{2\}){\textbackslash}right],\$\$
if \$X{\textbackslash}sim {\textbackslash}mathcal\{N\}(0,1)\$.

One can approximate the GELU with
\$0.5x{\textbackslash}left(1+{\textbackslash}tanh{\textbackslash}left[{\textbackslash}sqrt\{2/{\textbackslash}pi\}{\textbackslash}left(x + 0.044715x{\textasciicircum}\{3\}{\textbackslash}right){\textbackslash}right]{\textbackslash}right)\$ or \$x{\textbackslash}sigma{\textbackslash}left(1.702x{\textbackslash}right),\$
but PyTorch's exact implementation is sufficiently fast such that these approximations may be unnecessary. (See also the SiLU \$x{\textbackslash}sigma(x)\$ which was also coined in the paper that introduced the GELU.)

GELUs are used in GPT-3, BERT, and most other Transformers.},
	language = {en},
	urldate = {2024-04-28},
}

@misc{brown_wide_2022,
	title = {Wide {Attention} {Is} {The} {Way} {Forward} {For} {Transformers}?},
	url = {http://arxiv.org/abs/2210.00640},
	doi = {10.48550/arXiv.2210.00640},
	abstract = {The Transformer is an extremely powerful and prominent deep learning architecture. In this work, we challenge the commonly held belief in deep learning that going deeper is better, and show an alternative design approach that is building wider attention Transformers. We demonstrate that wide single layer Transformer models can compete with or outperform deeper ones in a variety of Natural Language Processing (NLP) tasks when both are trained from scratch. The impact of changing the model aspect ratio on Transformers is then studied systematically. This ratio balances the number of layers and the number of attention heads per layer while keeping the total number of attention heads and all other hyperparameters constant. On average, across 4 NLP tasks and 10 attention types, single layer wide models perform 0.3\% better than their deep counterparts. We show an in-depth evaluation and demonstrate how wide models require a far smaller memory footprint and can run faster on commodity hardware, in addition, these wider models are also more interpretable. For example, a single layer Transformer on the IMDb byte level text classification has 3.1x faster inference latency on a CPU than its equally accurate deeper counterpart, and is half the size. We therefore put forward wider and shallower models as a viable and desirable alternative for small models on NLP tasks, and as an important area of research for domains beyond this.},
	urldate = {2024-04-28},
	publisher = {arXiv},
	author = {Brown, Jason Ross and Zhao, Yiren and Shumailov, Ilia and Mullins, Robert D.},
	month = nov,
	year = {2022},
	note = {arXiv:2210.00640 [cs]},
	keywords = {Computer Science - Machine Learning, I.2.7},
}

@misc{noauthor_wide_nodate,
	title = {Wide {Attention} {Is} {The} {Way} {Forward} {For} {Transformers}?},
	url = {https://ar5iv.labs.arxiv.org/html/2210.00640},
	abstract = {The Transformer is an extremely powerful and prominent deep learning architecture.
In this work, we challenge the commonly held belief in deep learning that going deeper is better, and show an alternative approach that‚Ä¶},
	language = {en},
	urldate = {2024-04-28},
	journal = {ar5iv},
}

@misc{winata_lightweight_2020,
	title = {Lightweight and {Efficient} {End}-to-{End} {Speech} {Recognition} {Using} {Low}-{Rank} {Transformer}},
	url = {http://arxiv.org/abs/1910.13923},
	doi = {10.48550/arXiv.1910.13923},
	abstract = {Highly performing deep neural networks come at the cost of computational complexity that limits their practicality for deployment on portable devices. We propose the low-rank transformer (LRT), a memory-efficient and fast neural architecture that significantly reduces the parameters and boosts the speed of training and inference for end-to-end speech recognition. Our approach reduces the number of parameters of the network by more than 50\% and speeds up the inference time by around 1.35x compared to the baseline transformer model. The experiments show that our LRT model generalizes better and yields lower error rates on both validation and test sets compared to an uncompressed transformer model. The LRT model outperforms those from existing works on several datasets in an end-to-end setting without using an external language model or acoustic data.},
	urldate = {2024-04-28},
	publisher = {arXiv},
	author = {Winata, Genta Indra and Cahyawijaya, Samuel and Lin, Zhaojiang and Liu, Zihan and Fung, Pascale},
	month = feb,
	year = {2020},
	note = {arXiv:1910.13923 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{sennrich_neural_2016,
	address = {Berlin, Germany},
	title = {Neural {Machine} {Translation} of {Rare} {Words} with {Subword} {Units}},
	url = {https://aclanthology.org/P16-1162},
	doi = {10.18653/v1/P16-1162},
	urldate = {2024-04-28},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	editor = {Erk, Katrin and Smith, Noah A.},
	month = aug,
	year = {2016},
	keywords = {F},
	pages = {1715--1725},
}

@misc{warstadt_blimp_2023,
	title = {{BLiMP}: {The} {Benchmark} of {Linguistic} {Minimal} {Pairs} for {English}},
	shorttitle = {{BLiMP}},
	url = {http://arxiv.org/abs/1912.00582},
	abstract = {We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP),1 a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairs, i.e. pairs of minimally different sentences that contrast in grammatical acceptability and isolate speciÔ¨Åc phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4\%. We evaluate n-gram, LSTM, and Transformer (GPT-2 and TransformerXL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We Ô¨Ånd that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands.},
	language = {en},
	urldate = {2024-04-28},
	publisher = {arXiv},
	author = {Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey, Anhad and Peng, Wei and Wang, Sheng-Fu and Bowman, Samuel R.},
	month = feb,
	year = {2023},
	note = {arXiv:1912.00582 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{lample_large_2019,
	title = {Large {Memory} {Layers} with {Product} {Keys}},
	url = {http://arxiv.org/abs/1907.05242},
	abstract = {This paper introduces a structured memory which can be easily integrated into a neural network. The memory is very large by design and significantly increases the capacity of the architecture, by up to a billion parameters with a negligible computational overhead. Its design and access pattern is based on product keys, which enable fast and exact nearest neighbor search. The ability to increase the number of parameters while keeping the same computational budget lets the overall system strike a better trade-off between prediction accuracy and computation efficiency both at training and test time. This memory layer allows us to tackle very large scale language modeling tasks. In our experiments we consider a dataset with up to 30 billion words, and we plug our memory layer in a state-of-the-art transformer-based architecture. In particular, we found that a memory augmented model with only 12 layers outperforms a baseline transformer model with 24 layers, while being twice faster at inference time. We release our code for reproducibility purposes.},
	language = {en},
	urldate = {2024-04-28},
	publisher = {arXiv},
	author = {Lample, Guillaume and Sablayrolles, Alexandre and Ranzato, Marc'Aurelio and Denoyer, Ludovic and J√©gou, Herv√©},
	month = dec,
	year = {2019},
	note = {arXiv:1907.05242 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{shazeer_outrageously_2017,
	title = {Outrageously {Large} {Neural} {Networks}: {The} {Sparsely}-{Gated} {Mixture}-of-{Experts} {Layer}},
	shorttitle = {Outrageously {Large} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1701.06538},
	abstract = {The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are signiÔ¨Åcant algorithmic and performance challenges. In this work, we address these challenges and Ô¨Ånally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efÔ¨Åciency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve signiÔ¨Åcantly better results than state-of-the-art at lower computational cost.},
	language = {en},
	urldate = {2024-04-28},
	publisher = {arXiv},
	author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
	month = jan,
	year = {2017},
	note = {arXiv:1701.06538 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{hoffmann_training_2022,
	title = {Training {Compute}-{Optimal} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2203.15556},
	abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
	language = {en},
	urldate = {2024-04-27},
	publisher = {arXiv},
	author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and Driessche, George van den and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
	month = mar,
	year = {2022},
	note = {arXiv:2203.15556 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by Ô¨Åne-tuning on a speciÔ¨Åc task. While typically task-agnostic in architecture, this method still requires task-speciÔ¨Åc Ô¨Åne-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions ‚Äì something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art Ô¨Ånetuning approaches. SpeciÔ¨Åcally, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or Ô¨Åne-tuning, with tasks and few-shot demonstrations speciÔ¨Åed purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-Ô¨Çy reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3‚Äôs few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we Ô¨Ånd that GPT-3 can generate samples of news articles which human evaluators have difÔ¨Åculty distinguishing from articles written by humans. We discuss broader societal impacts of this Ô¨Ånding and of GPT-3 in general.},
	language = {en},
	urldate = {2024-04-27},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{rae_scaling_2022,
	title = {Scaling {Language} {Models}: {Methods}, {Analysis} \& {Insights} from {Training} {Gopher}},
	shorttitle = {Scaling {Language} {Models}},
	url = {http://arxiv.org/abs/2112.11446},
	abstract = {Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales ‚Äî from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identiÔ¨Åcation of toxic language, but logical and mathematical reasoning see less beneÔ¨Åt. We provide a holistic analysis of the training dataset and model‚Äôs behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.},
	language = {en},
	urldate = {2024-04-27},
	publisher = {arXiv},
	author = {Rae, Jack W. and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and Rutherford, Eliza and Hennigan, Tom and Menick, Jacob and Cassirer, Albin and Powell, Richard and Driessche, George van den and Hendricks, Lisa Anne and Rauh, Maribeth and Huang, Po-Sen and Glaese, Amelia and Welbl, Johannes and Dathathri, Sumanth and Huang, Saffron and Uesato, Jonathan and Mellor, John and Higgins, Irina and Creswell, Antonia and McAleese, Nat and Wu, Amy and Elsen, Erich and Jayakumar, Siddhant and Buchatskaya, Elena and Budden, David and Sutherland, Esme and Simonyan, Karen and Paganini, Michela and Sifre, Laurent and Martens, Lena and Li, Xiang Lorraine and Kuncoro, Adhiguna and Nematzadeh, Aida and Gribovskaya, Elena and Donato, Domenic and Lazaridou, Angeliki and Mensch, Arthur and Lespiau, Jean-Baptiste and Tsimpoukelli, Maria and Grigorev, Nikolai and Fritz, Doug and Sottiaux, Thibault and Pajarskas, Mantas and Pohlen, Toby and Gong, Zhitao and Toyama, Daniel and d'Autume, Cyprien de Masson and Li, Yujia and Terzi, Tayfun and Mikulik, Vladimir and Babuschkin, Igor and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Jones, Chris and Bradbury, James and Johnson, Matthew and Hechtman, Blake and Weidinger, Laura and Gabriel, Iason and Isaac, William and Lockhart, Ed and Osindero, Simon and Rimell, Laura and Dyer, Chris and Vinyals, Oriol and Ayoub, Kareem and Stanway, Jeff and Bennett, Lorrayne and Hassabis, Demis and Kavukcuoglu, Koray and Irving, Geoffrey},
	month = jan,
	year = {2022},
	note = {arXiv:2112.11446 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{kamalakara_exploring_2022,
	title = {Exploring {Low} {Rank} {Training} of {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2209.13569},
	abstract = {Training deep neural networks in low rank, i.e. with factorised layers, is of particular interest to the community: it offers efÔ¨Åciency over unfactorised training in terms of both memory consumption and training time. Prior work has focused on low rank approximations of pre-trained networks and training in low rank space with additional objectives, offering various ad hoc explanations for chosen practice. We analyse techniques that work well in practice, and through extensive ablations on models such as GPT2 we provide evidence falsifying common beliefs in the Ô¨Åeld, hinting in the process at exciting research opportunities that still need answering.},
	language = {en},
	urldate = {2024-04-27},
	publisher = {arXiv},
	author = {Kamalakara, Siddhartha Rao and Locatelli, Acyr and Venkitesh, Bharat and Ba, Jimmy and Gal, Yarin and Gomez, Aidan N.},
	month = sep,
	year = {2022},
	note = {arXiv:2209.13569 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{winata_lightweight_2020-1,
	title = {Lightweight and {Efficient} {End}-to-{End} {Speech} {Recognition} {Using} {Low}-{Rank} {Transformer}},
	url = {http://arxiv.org/abs/1910.13923},
	abstract = {Highly performing deep neural networks come at the cost of computational complexity that limits their practicality for deployment on portable devices. We propose the low-rank transformer (LRT), a memory-efÔ¨Åcient and fast neural architecture that signiÔ¨Åcantly reduces the parameters and boosts the speed of training and inference for end-to-end speech recognition. Our approach reduces the number of parameters of the network by more than 50\% and speeds up the inference time by around 1.35x compared to the baseline transformer model. The experiments show that our LRT model generalizes better and yields lower error rates on both validation and test sets compared to an uncompressed transformer model. The LRT model outperforms those from existing works on several datasets in an end-to-end setting without using an external language model or acoustic data.},
	language = {en},
	urldate = {2024-04-27},
	publisher = {arXiv},
	author = {Winata, Genta Indra and Cahyawijaya, Samuel and Lin, Zhaojiang and Liu, Zihan and Fung, Pascale},
	month = feb,
	year = {2020},
	note = {arXiv:1910.13923 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{eger_is_2019,
	title = {Is it {Time} to {Swish}? {Comparing} {Deep} {Learning} {Activation} {Functions} {Across} {NLP} tasks},
	shorttitle = {Is it {Time} to {Swish}?},
	url = {http://arxiv.org/abs/1901.02671},
	doi = {10.48550/arXiv.1901.02671},
	abstract = {Activation functions play a crucial role in neural networks because they are the nonlinearities which have been attributed to the success story of deep learning. One of the currently most popular activation functions is ReLU, but several competitors have recently been proposed or 'discovered', including LReLU functions and swish. While most works compare newly proposed activation functions on few tasks (usually from image classification) and against few competitors (usually ReLU), we perform the first large-scale comparison of 21 activation functions across eight different NLP tasks. We find that a largely unknown activation function performs most stably across all tasks, the so-called penalized tanh function. We also show that it can successfully replace the sigmoid and tanh gates in LSTM cells, leading to a 2 percentage point (pp) improvement over the standard choices on a challenging NLP task.},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Eger, Steffen and Youssef, Paul and Gurevych, Iryna},
	month = jan,
	year = {2019},
	note = {arXiv:1901.02671 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Read},
}

@misc{nwankpa_activation_2018,
	title = {Activation {Functions}: {Comparison} of trends in {Practice} and {Research} for {Deep} {Learning}},
	shorttitle = {Activation {Functions}},
	url = {http://arxiv.org/abs/1811.03378},
	doi = {10.48550/arXiv.1811.03378},
	abstract = {Deep neural networks have been successfully used in diverse emerging domains to solve real world complex problems with may more deep learning(DL) architectures, being developed to date. To achieve these state-of-the-art performances, the DL architectures use activation functions (AFs), to perform diverse computations between the hidden layers and the output layers of any given DL architecture. This paper presents a survey on the existing AFs used in deep learning applications and highlights the recent trends in the use of the activation functions for deep learning applications. The novelty of this paper is that it compiles majority of the AFs used in DL and outlines the current trends in the applications and usage of these functions in practical deep learning deployments against the state-of-the-art research results. This compilation will aid in making effective decisions in the choice of the most suitable and appropriate activation function for any given application, ready for deployment. This paper is timely because most research papers on AF highlights similar works and results while this paper will be the first, to compile the trends in AF applications in practice against the research results from literature, found in deep learning research to date.},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Nwankpa, Chigozie and Ijomah, Winifred and Gachagan, Anthony and Marshall, Stephen},
	month = nov,
	year = {2018},
	note = {arXiv:1811.03378 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Read},
}

@misc{dubey_activation_2022-2,
	title = {Activation {Functions} in {Deep} {Learning}: {A} {Comprehensive} {Survey} and {Benchmark}},
	shorttitle = {Activation {Functions} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/2109.14545},
	doi = {10.48550/arXiv.2109.14545},
	abstract = {Neural networks have shown tremendous growth in recent years to solve numerous problems. Various types of neural networks have been introduced to deal with different types of problems. However, the main goal of any neural network is to transform the non-linearly separable input data into more linearly separable abstract features using a hierarchy of layers. These layers are combinations of linear and nonlinear functions. The most popular and common non-linearity layers are activation functions (AFs), such as Logistic Sigmoid, Tanh, ReLU, ELU, Swish and Mish. In this paper, a comprehensive overview and survey is presented for AFs in neural networks for deep learning. Different classes of AFs such as Logistic Sigmoid and Tanh based, ReLU based, ELU based, and Learning based are covered. Several characteristics of AFs such as output range, monotonicity, and smoothness are also pointed out. A performance comparison is also performed among 18 state-of-the-art AFs with different networks on different types of data. The insights of AFs are presented to benefit the researchers for doing further research and practitioners to select among different choices. The code used for experimental comparison is released at: {\textbackslash}url\{https://github.com/shivram1987/ActivationFunctions\}.},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Dubey, Shiv Ram and Singh, Satish Kumar and Chaudhuri, Bidyut Baran},
	month = jun,
	year = {2022},
	note = {arXiv:2109.14545 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Read},
}

@article{kunc_three_2024,
	title = {Three {Decades} of {Activations}: {A} {Comprehensive} {Survey} of 400 {Activation} {Functions} for {Neural} {Networks}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Three {Decades} of {Activations}},
	url = {https://arxiv.org/abs/2402.09092},
	doi = {10.48550/ARXIV.2402.09092},
	abstract = {Neural networks have proven to be a highly effective tool for solving complex problems in many areas of life. Recently, their importance and practical usability have further been reinforced with the advent of deep learning. One of the important conditions for the success of neural networks is the choice of an appropriate activation function introducing non-linearity into the model. Many types of these functions have been proposed in the literature in the past, but there is no single comprehensive source containing their exhaustive overview. The absence of this overview, even in our experience, leads to redundancy and the unintentional rediscovery of already existing activation functions. To bridge this gap, our paper presents an extensive survey involving 400 activation functions, which is several times larger in scale than previous surveys. Our comprehensive compilation also references these surveys; however, its main goal is to provide the most comprehensive overview and systematization of previously published activation functions with links to their original sources. The secondary aim is to update the current understanding of this family of functions.},
	urldate = {2024-04-25},
	author = {Kunc, Vladim√≠r and Kl√©ma, Ji≈ô√≠},
	year = {2024},
	note = {Publisher: [object Object]
Version Number: 1},
	keywords = {FOS: Computer and information sciences, I.5.1, Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE)},
}

@misc{csordas_approximating_2023,
	title = {Approximating {Two}-{Layer} {Feedforward} {Networks} for {Efficient} {Transformers}},
	url = {http://arxiv.org/abs/2310.10837},
	doi = {10.48550/arXiv.2310.10837},
	abstract = {How to reduce compute and memory requirements of neural networks (NNs) without sacrificing performance? Many recent works use sparse Mixtures of Experts (MoEs) to build resource-efficient large language models (LMs). Here we introduce several novel perspectives on MoEs, presenting a general framework that unifies various methods to approximate two-layer NNs (e.g., feedforward blocks of Transformers), including product-key memories (PKMs). Leveraging insights from this framework, we propose methods to improve both MoEs and PKMs. Unlike prior work that compares MoEs with dense baselines under the compute-equal condition, our evaluation condition is parameter-equal, which is crucial to properly evaluate LMs. We show that our MoEs are competitive with the dense Transformer-XL on both the WikiText-103 and enwiki8 datasets at two different scales, while being much more resource efficient. This demonstrates that MoEs are relevant not only to extremely large LMs but also to any-scale resource-efficient LMs. Our code is public.},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Csord√°s, R√≥bert and Irie, Kazuki and Schmidhuber, J√ºrgen},
	month = nov,
	year = {2023},
	note = {arXiv:2310.10837 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{gustineli_survey_2022,
	title = {A survey on recently proposed activation functions for {Deep} {Learning}},
	url = {http://arxiv.org/abs/2204.02921},
	doi = {10.48550/arXiv.2204.02921},
	abstract = {Artificial neural networks (ANN), typically referred to as neural networks, are a class of Machine Learning algorithms and have achieved widespread success, having been inspired by the biological structure of the human brain. Neural networks are inherently powerful due to their ability to learn complex function approximations from data. This generalization ability has been able to impact multidisciplinary areas involving image recognition, speech recognition, natural language processing, and others. Activation functions are a crucial sub-component of neural networks. They define the output of a node in the network given a set of inputs. This survey discusses the main concepts of activation functions in neural networks, including; a brief introduction to deep neural networks, a summary of what are activation functions and how they are used in neural networks, their most common properties, the different types of activation functions, some of the challenges, limitations, and alternative solutions faced by activation functions, concluding with the final remarks.},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Gustineli, Murilo},
	month = apr,
	year = {2022},
	note = {arXiv:2204.02921 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{mercioni_brief_2023,
	title = {A {Brief} {Review} of the {Most} {Recent} {Activation} {Functions} for {Neural} {Networks}},
	copyright = {https://doi.org/10.15223/policy-029},
	url = {https://ieeexplore.ieee.org/document/10171705/},
	doi = {10.1109/EMES58375.2023.10171705},
	abstract = {Even though the majority of the current functions have shortcomings, this study looks at a few activation function capabilities that might lead to performance enhancement. The research on neural networks still shows a lot of interest in the activation function since it can enhance performance. With or without trainable parameters, other adaptive activation functions have been put forth that have demonstrated to lead to better results than the benchmark. These studies outline the characteristics, benefits, constraints, and directions of such types of applications. Due to their shortcomings, several of those functions are now regarded as deprecated. The primary emphasis of such functions is on fundamental elements that are thought to be necessary for learning, such as monocity, derivatives, and finite of their range. The goal of this research article is to present and assess the most popular and recent activation functions. This will go through their characteristics, advantages and disadvantages, formulation, and usage.},
	urldate = {2024-04-25},
	journal = {2023 17th International Conference on Engineering of Modern Electric Systems (EMES)},
	author = {Mercioni, Marina Adriana and Holban, Stefan},
	month = jun,
	year = {2023},
	note = {Conference Name: 2023 17th International Conference on Engineering of Modern Electric Systems (EMES)
ISBN: 9798350310634
Place: Oradea, Romania
Publisher: IEEE},
	pages = {1--4},
}

@misc{shazeer_glu_2020,
	title = {{GLU} {Variants} {Improve} {Transformer}},
	url = {http://arxiv.org/abs/2002.05202},
	doi = {10.48550/arXiv.2002.05202},
	abstract = {Gated Linear Units (arXiv:1612.08083) consist of the component-wise product of two linear projections, one of which is first passed through a sigmoid function. Variations on GLU are possible, using different nonlinear (or even linear) functions in place of sigmoid. We test these variants in the feed-forward sublayers of the Transformer (arXiv:1706.03762) sequence-to-sequence model, and find that some of them yield quality improvements over the typically-used ReLU or GELU activations.},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Shazeer, Noam},
	month = feb,
	year = {2020},
	note = {arXiv:2002.05202 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{zhang_relu2_2024,
	title = {{ReLU}\${\textasciicircum}2\$ {Wins}: {Discovering} {Efficient} {Activation} {Functions} for {Sparse} {LLMs}},
	shorttitle = {{ReLU}\${\textasciicircum}2\$ {Wins}},
	url = {http://arxiv.org/abs/2402.03804},
	doi = {10.48550/arXiv.2402.03804},
	abstract = {Sparse computation offers a compelling solution for the inference of Large Language Models (LLMs) in low-resource scenarios by dynamically skipping the computation of inactive neurons. While traditional approaches focus on ReLU-based LLMs, leveraging zeros in activation values, we broaden the scope of sparse LLMs beyond zero activation values. We introduce a general method that defines neuron activation through neuron output magnitudes and a tailored magnitude threshold, demonstrating that non-ReLU LLMs also exhibit sparse activation. To find the most efficient activation function for sparse computation, we propose a systematic framework to examine the sparsity of LLMs from three aspects: the trade-off between sparsity and performance, the predictivity of sparsity, and the hardware affinity. We conduct thorough experiments on LLMs utilizing different activation functions, including ReLU, SwiGLU, ReGLU, and ReLU\${\textasciicircum}2\$. The results indicate that models employing ReLU\${\textasciicircum}2\$ excel across all three evaluation aspects, highlighting its potential as an efficient activation function for sparse LLMs. We will release the code to facilitate future research.},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Zhang, Zhengyan and Song, Yixin and Yu, Guanghui and Han, Xu and Lin, Yankai and Xiao, Chaojun and Song, Chenyang and Liu, Zhiyuan and Mi, Zeyu and Sun, Maosong},
	month = feb,
	year = {2024},
	note = {arXiv:2402.03804 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{fang_transformers_2023,
	title = {Transformers with {Learnable} {Activation} {Functions}},
	url = {http://arxiv.org/abs/2208.14111},
	doi = {10.48550/arXiv.2208.14111},
	abstract = {Activation functions can have a significant impact on reducing the topological complexity of input data and therefore improve the performance of the model. Selecting a suitable activation function is an essential step in neural model design. However, the choice of activation function is seldom discussed or explored in Transformer-based language models. Their activation functions are chosen beforehand and then remain fixed from pre-training to fine-tuning. As a result, the inductive biases they imposed on models cannot be adjusted during this long life cycle. Moreover, subsequently developed models (e.g., RoBERTa, BART, and GPT-3) often follow up prior work (e.g., BERT) to use the same activation function without justification. In this paper, we investigate the effectiveness of using Rational Activation Function (RAF), a learnable activation function, in the Transformer architecture. In contrast to conventional, predefined activation functions, RAFs can adaptively learn optimal activation functions during training according to input data. Our experiments show the RAF-based Transformer (RAFT) achieves a lower validation perplexity than a vanilla BERT with the GELU function. We further evaluate RAFT on downstream tasks in low- and full-data settings. Our results show that RAFT outperforms the counterpart model across the majority of tasks and settings. For instance, RAFT outperforms vanilla BERT on the GLUE benchmark by 5.71 points on average in low-data scenario (where 100 training examples are available) and by 2.05 points on SQuAD in full-data setting. Analysis of the shapes of learned RAFs further unveils that they substantially vary between different layers of the pre-trained model and mostly look very different from conventional activation functions. RAFT opens a new research direction for analyzing and interpreting pre-trained models according to the learned activation functions.},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Fang, Haishuo and Lee, Ji-Ung and Moosavi, Nafise Sadat and Gurevych, Iryna},
	month = feb,
	year = {2023},
	note = {arXiv:2208.14111 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{noauthor_pdf_nodate,
	title = {[{PDF}] {Quantized} {Neural} {Networks}: {Training} {Neural} {Networks} with {Low} {Precision} {Weights} and {Activations} {\textbar} {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/paper/Quantized-Neural-Networks%3A-Training-Neural-Networks-Hubara-Courbariaux/d2e4147eecae6f914e9e1e9aece8fdd2eaed809f},
	urldate = {2024-04-25},
}

@inproceedings{li_lazy_2022,
	title = {The {Lazy} {Neuron} {Phenomenon}: {On} {Emergence} of {Activation} {Sparsity} in {Transformers}},
	shorttitle = {The {Lazy} {Neuron} {Phenomenon}},
	url = {https://www.semanticscholar.org/paper/The-Lazy-Neuron-Phenomenon%3A-On-Emergence-of-in-Li-You/e0271cb75087ccfd4a8c3351e0f5189a6de04c03},
	abstract = {This paper studies the curious phenomenon for machine learning models with Transformer architectures that their activation maps are sparse. By activation map we refer to the intermediate output of the multi-layer perceptrons (MLPs) after a ReLU activation function, and by sparse we mean that on average very few entries (e.g., 3.0\% for T5-Base and 6.3\% for ViT-B16) are nonzero for each input to MLP. Moreover, larger Transformers with more layers and wider MLP hidden dimensions are sparser as measured by the percentage of nonzero entries. Through extensive experiments we demonstrate that the emergence of sparsity is a prevalent phenomenon that occurs for both natural language processing and vision tasks, on both training and evaluation data, for Transformers of various configurations, at layers of all depth levels, as well as for other architectures including MLP-mixers and 2-layer MLPs. We show that sparsity also emerges using training datasets with random labels, or with random inputs, or with infinite amount of data, demonstrating that sparsity is not a result of a specific family of datasets. We discuss how sparsity immediately implies a way to significantly reduce the FLOP count and improve efficiency for Transformers. Moreover, we demonstrate perhaps surprisingly that enforcing an even sparser activation via Top-k thresholding with a small value of k brings a collection of desired but missing properties for Transformers, namely less sensitivity to noisy training data, more robustness to input corruptions, and better calibration for their prediction confidence.},
	urldate = {2024-04-25},
	author = {Li, Zong-xiao and You, Chong and Bhojanapalli, Srinadh and Li, Daliang and Rawat, A. and Reddi, Sashank J. and Ye, Kenneth Q. and Chern, Felix and Yu, Felix X. and Guo, Ruiqi and Kumar, Surinder},
	month = oct,
	year = {2022},
}

@article{shen_study_2023,
	title = {A {Study} on {ReLU} and {Softmax} in {Transformer}},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	url = {https://arxiv.org/abs/2302.06461},
	doi = {10.48550/ARXIV.2302.06461},
	abstract = {The Transformer architecture consists of self-attention and feed-forward networks (FFNs) which can be viewed as key-value memories according to previous works. However, FFN and traditional memory utilize different activation functions (i.e., ReLU and Softmax respectively), which makes them not equivalent. In this paper, we first rebuild the connections between FFN and key-value memory by conducting extensive studies on ReLU and Softmax, and find they are equivalent when adding an additional layer normalization module on Softmax. In addition, ReLU outperforms Softmax on both FFN and key-value memory when the number of value slots is large. We analyze the reasons and then explore this good property of ReLU on the self-attention network where the original Softmax activation performs poorly on long input sequences. We then propose a full ReLU architecture named ReLUFormer which performs better than the baseline Transformer on long sequence tasks such as document translation. This paper sheds light on the following points: 1) Softmax and ReLU use different normalization methods over elements which lead to different variances of results, and ReLU is good at dealing with a large number of key-value slots; 2) FFN and key-value memory are equivalent, and thus the Transformer can be viewed as a memory network where FFNs and self-attention networks are both key-value memories.},
	urldate = {2024-04-25},
	author = {Shen, Kai and Guo, Junliang and Tan, Xu and Tang, Siliang and Wang, Rui and Bian, Jiang},
	year = {2023},
	note = {Publisher: [object Object]
Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@article{zhang_relu2_2024-1,
	title = {{ReLU}\${\textasciicircum}2\$ {Wins}: {Discovering} {Efficient} {Activation} {Functions} for {Sparse} {LLMs}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{ReLU}\${\textasciicircum}2\$ {Wins}},
	url = {https://arxiv.org/abs/2402.03804},
	doi = {10.48550/ARXIV.2402.03804},
	abstract = {Sparse computation offers a compelling solution for the inference of Large Language Models (LLMs) in low-resource scenarios by dynamically skipping the computation of inactive neurons. While traditional approaches focus on ReLU-based LLMs, leveraging zeros in activation values, we broaden the scope of sparse LLMs beyond zero activation values. We introduce a general method that defines neuron activation through neuron output magnitudes and a tailored magnitude threshold, demonstrating that non-ReLU LLMs also exhibit sparse activation. To find the most efficient activation function for sparse computation, we propose a systematic framework to examine the sparsity of LLMs from three aspects: the trade-off between sparsity and performance, the predictivity of sparsity, and the hardware affinity. We conduct thorough experiments on LLMs utilizing different activation functions, including ReLU, SwiGLU, ReGLU, and ReLU\${\textasciicircum}2\$. The results indicate that models employing ReLU\${\textasciicircum}2\$ excel across all three evaluation aspects, highlighting its potential as an efficient activation function for sparse LLMs. We will release the code to facilitate future research.},
	urldate = {2024-04-25},
	author = {Zhang, Zhengyan and Song, Yixin and Yu, Guanghui and Han, Xu and Lin, Yankai and Xiao, Chaojun and Song, Chenyang and Liu, Zhiyuan and Mi, Zeyu and Sun, Maosong},
	year = {2024},
	note = {Publisher: [object Object]
Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@article{williams_explicit_2023,
	title = {Explicit {Foundation} {Model} {Optimization} with {Self}-{Attentive} {Feed}-{Forward} {Neural} {Units}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2311.07510},
	doi = {10.48550/ARXIV.2311.07510},
	abstract = {Iterative approximation methods using backpropagation enable the optimization of neural networks, but they remain computationally expensive, especially when used at scale. This paper presents an efficient alternative for optimizing neural networks that reduces the costs of scaling neural networks and provides high-efficiency optimizations for low-resource applications. We will discuss a general result about feed-forward neural networks and then extend this solution to compositional (mult-layer) networks, which are applied to a simplified transformer block containing feed-forward and self-attention layers. These models are used to train highly-specified and complex multi-layer neural architectures that we refer to as self-attentive feed-forward unit (SAFFU) layers, which we use to develop a transformer that appears to generalize well over small, cognitively-feasible, volumes of data. Testing demonstrates explicit solutions outperform models optimized by backpropagation alone. Moreover, further application of backpropagation after explicit solutions leads to better optima from smaller scales of data, training effective models from much less data is enabled by explicit solution warm starts. We then carry out ablation experiments training a roadmap of about 250 transformer models over 1-million tokens to determine ideal settings. We find that multiple different architectural variants produce highly-performant models, and discover from this ablation that some of the best are not the most parameterized. This appears to indicate well-generalized models could be reached using less data by using explicit solutions, and that architectural exploration using explicit solutions pays dividends in guiding the search for efficient variants with fewer parameters, and which could be incorporated into low-resource hardware where AI might be embodied.},
	urldate = {2024-04-25},
	author = {Williams, Jake Ryland and Zhao, Haoran},
	year = {2023},
	note = {Publisher: [object Object]
Version Number: 1},
	keywords = {Data Analysis, Statistics and Probability (physics.data-an), FOS: Computer and information sciences, FOS: Mathematics, FOS: Physical sciences, Machine Learning (cs.LG), Machine Learning (stat.ML), Probability (math.PR)},
}

@misc{charpentier_not_2023,
	title = {Not all layers are equally as important: {Every} {Layer} {Counts} {BERT}},
	shorttitle = {Not all layers are equally as important},
	url = {http://arxiv.org/abs/2311.02265},
	doi = {10.48550/arXiv.2311.02265},
	abstract = {This paper introduces a novel modification of the transformer architecture, tailored for the data-efficient pretraining of language models. This aspect is evaluated by participating in the BabyLM challenge, where our solution won both the strict and strict-small tracks. Our approach allows each transformer layer to select which outputs of previous layers to process. The empirical results verify the potential of this simple modification and show that not all layers are equally as important.},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Charpentier, Lucas Georges Gabriel and Samuel, David},
	month = nov,
	year = {2023},
	note = {arXiv:2311.02265 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{mirzadeh_relu_2023,
	title = {{ReLU} {Strikes} {Back}: {Exploiting} {Activation} {Sparsity} in {Large} {Language} {Models}},
	shorttitle = {{ReLU} {Strikes} {Back}},
	url = {http://arxiv.org/abs/2310.04564},
	doi = {10.48550/arXiv.2310.04564},
	abstract = {Large Language Models (LLMs) with billions of parameters have drastically transformed AI applications. However, their demanding computation during inference has raised significant challenges for deployment on resource-constrained devices. Despite recent trends favoring alternative activation functions such as GELU or SiLU, known for increased computation, this study strongly advocates for reinstating ReLU activation in LLMs. We demonstrate that using the ReLU activation function has a negligible impact on convergence and performance while significantly reducing computation and weight transfer. This reduction is particularly valuable during the memory-bound inference step, where efficiency is paramount. Exploring sparsity patterns in ReLU-based LLMs, we unveil the reutilization of activated neurons for generating new tokens and leveraging these insights, we propose practical strategies to substantially reduce LLM inference computation up to three times, using ReLU activations with minimal performance trade-offs.},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Mirzadeh, Iman and Alizadeh, Keivan and Mehta, Sachin and Del Mundo, Carlo C. and Tuzel, Oncel and Samei, Golnoosh and Rastegari, Mohammad and Farajtabar, Mehrdad},
	month = oct,
	year = {2023},
	note = {arXiv:2310.04564 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{noauthor_illustrated_nodate,
	title = {The {Illustrated} {Transformer} ‚Äì {Jay} {Alammar} ‚Äì {Visualizing} machine learning one concept at a time.},
	url = {https://jalammar.github.io/illustrated-transformer/},
	urldate = {2024-04-25},
}

@misc{davis_low-rank_2014,
	title = {Low-{Rank} {Approximations} for {Conditional} {Feedforward} {Computation} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1312.4461},
	abstract = {Scalability properties of deep neural networks raise key research questions, particularly as the problems considered become larger and more challenging. This paper expands on the idea of conditional computation introduced in [2], where the nodes of a deep network are augmented by a set of gating units that determine when a node should be calculated. By factorizing the weight matrix into a low-rank approximation, an estimation of the sign of the pre-nonlinearity activation can be efÔ¨Åciently obtained. For networks using rectiÔ¨Åed-linear hidden units, this implies that the computation of a hidden unit with an estimated negative pre-nonlinearity can be omitted altogether, as its value will become zero when nonlinearity is applied. For sparse neural networks, this can result in considerable speed gains. Experimental results using the MNIST and SVHN data sets with a fully-connected deep neural network demonstrate the performance robustness of the proposed scheme with respect to the error introduced by the conditional computation process.},
	language = {en},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Davis, Andrew and Arel, Itamar},
	month = jan,
	year = {2014},
	note = {arXiv:1312.4461 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{liu_towards_2023,
	title = {Towards {A} {Unified} {View} of {Sparse} {Feed}-{Forward} {Network} in {Pretraining} {Large} {Language} {Model}},
	url = {http://arxiv.org/abs/2305.13999},
	abstract = {Large and sparse feed-forward layers (S-FFN) such as Mixture-of-Experts (MoE) have proven effective in scaling up Transformers model size for pretraining large language models. By only activating part of the FFN parameters conditioning on input, S-FFN improves generalization performance while keeping training and inference costs (in FLOPs) fixed. In this work, we analyzed two major design choices of SFFN: the memory block (a.k.a. expert) size and the memory block selection method under a general conceptual framework of sparse neural memory. Using this unified framework, we compare several S-FFN architectures for language modeling and provide insights into their relative efficacy and efficiency. We found a simpler selection method ‚Äî Avg-K that selects blocks through their mean aggregated hidden states, achieving lower perplexity in language model pretraining compared to existing MoE architectures including Switch Transformer (Fedus et al., 2021) and HashLayer (Roller et al., 2021).},
	language = {en},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Liu, Zeyu Leo and Dettmers, Tim and Lin, Xi Victoria and Stoyanov, Veselin and Li, Xian},
	month = oct,
	year = {2023},
	note = {arXiv:2305.13999 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{yang_structure_2016,
	title = {A structure optimization framework for feed-forward neural networks using sparse representation},
	volume = {109},
	issn = {0950-7051},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705116301952},
	doi = {10.1016/j.knosys.2016.06.026},
	abstract = {Traditionally, optimizing the structure of a feed-forward neural-network is time-consuming and it needs to balance the trade-off between the network size and network performance. In this paper, a sparse-representation based framework, termed SRS, is introduced to generate a small-sized network structure without compromising the network performance. Based on the forward selection strategy, the SRS framework selects significant elements (weights or hidden neurons) from the initial network that minimize the residual output error. The main advantage of the SRS framework is that it is able to optimize the network structure and training performance simultaneously. As a result, the training error is reduced while the number of selected elements increases. The efficiency and robustness of the SRS framework are evaluated based on several benchmark datasets. Experimental results indicate that the SRS framework performs favourably compared to alternative structure optimization algorithms.},
	urldate = {2024-04-25},
	journal = {Knowledge-Based Systems},
	author = {Yang, Jie and Ma, Jun},
	month = oct,
	year = {2016},
	keywords = {Multiple measurement vector, Network construction, Network pruning, Neural networks, Single measurement vector, Sparse representation, Structure optimization},
	pages = {61--70},
}

@article{bolcskei_optimal_2019,
	title = {Optimal {Approximation} with {Sparsely} {Connected} {Deep} {Neural} {Networks}},
	volume = {1},
	issn = {2577-0187},
	url = {https://epubs.siam.org/doi/10.1137/18M118709X},
	doi = {10.1137/18M118709X},
	abstract = {We derive fundamental lower bounds on the connectivity and the memory requirements of deep neural networks guaranteeing uniform approximation rates for arbitrary function classes in L2(Rd). In other words, we establish a connection between the complexity of a function class and the complexity of deep neural networks approximating functions from this class to within a prescribed accuracy.},
	language = {en},
	number = {1},
	urldate = {2024-04-25},
	journal = {SIAM Journal on Mathematics of Data Science},
	author = {B√∂lcskei, Helmut and Grohs, Philipp and Kutyniok, Gitta and Petersen, Philipp},
	month = jan,
	year = {2019},
	pages = {8--45},
}

@article{yang_feed-forward_2019,
	title = {Feed-forward neural network training using sparse representation},
	volume = {116},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418305530},
	doi = {10.1016/j.eswa.2018.08.038},
	abstract = {The feed-forward neural network (FNN) has drawn great interest in many applications due to its universal approximation capability. In this paper, a novel algorithm for training FNNs is proposed using the concept of sparse representation. The major advantage of the proposed algorithm is that it is capable of training the initial network and optimizing the network structure simultaneously. The proposed algorithm consists of two core stages: structure optimization and weight update. In the structure optimization stage, the sparse representation technique is employed to select important hidden neurons that minimize the residual output error. In the weight update stage, a dictionary learning based method is implemented to update network weights by maximizing the output diversity from hidden neurons. This weight-updating process is designed to improve the performance of the structure optimization. Based on several benchmark classification and regression problems, we present experimental results comparing the proposed algorithm with state-of-the-art methods. Simulation results show that the proposed algorithm offers comparative performance in terms of the final network size and generalization ability.},
	urldate = {2024-04-25},
	journal = {Expert Systems with Applications},
	author = {Yang, Jie and Ma, Jun},
	month = feb,
	year = {2019},
	keywords = {Dictionary learning, Feed-forward neural network, Multiple measurement vector, Sparse representation, Structure optimization},
	pages = {255--264},
}

@misc{du_glam_2022,
	title = {{GLaM}: {Efficient} {Scaling} of {Language} {Models} with {Mixture}-of-{Experts}},
	shorttitle = {{GLaM}},
	url = {http://arxiv.org/abs/2112.06905},
	doi = {10.48550/arXiv.2112.06905},
	abstract = {Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong results on in-context learning tasks. However, training these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks.},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Du, Nan and Huang, Yanping and Dai, Andrew M. and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and Zoph, Barret and Fedus, Liam and Bosma, Maarten and Zhou, Zongwei and Wang, Tao and Wang, Yu Emma and Webster, Kellie and Pellat, Marie and Robinson, Kevin and Meier-Hellstern, Kathleen and Duke, Toju and Dixon, Lucas and Zhang, Kun and Le, Quoc V. and Wu, Yonghui and Chen, Zhifeng and Cui, Claire},
	month = aug,
	year = {2022},
	note = {arXiv:2112.06905 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{fedus_switch_2022,
	title = {Switch {Transformers}: {Scaling} to {Trillion} {Parameter} {Models} with {Simple} and {Efficient} {Sparsity}},
	shorttitle = {Switch {Transformers}},
	url = {http://arxiv.org/abs/2101.03961},
	doi = {10.48550/arXiv.2101.03961},
	abstract = {In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the "Colossal Clean Crawled Corpus" and achieve a 4x speedup over the T5-XXL model.},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
	month = jun,
	year = {2022},
	note = {arXiv:2101.03961 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{lepikhin_gshard_2020,
	title = {{GShard}: {Scaling} {Giant} {Models} with {Conditional} {Computation} and {Automatic} {Sharding}},
	shorttitle = {{GShard}},
	url = {http://arxiv.org/abs/2006.16668},
	doi = {10.48550/arXiv.2006.16668},
	abstract = {Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
	month = jun,
	year = {2020},
	note = {arXiv:2006.16668 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{tay_efficient_2023,
	title = {Efficient {Transformers}: {A} {Survey}},
	volume = {55},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Efficient {Transformers}},
	url = {https://dl.acm.org/doi/10.1145/3530811},
	doi = {10.1145/3530811},
	abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision, and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of
              ‚ÄúX-former‚Äù
              models have been proposed‚ÄîReformer, Linformer, Performer, Longformer, to name a few‚Äîwhich improve upon the original Transformer architecture, many of which make improvements around computational and memory
              efficiency
              . With the aim of helping the avid researcher navigate this flurry, this article characterizes a large and thoughtful selection of recent efficiency-flavored ‚ÄúX-former‚Äù models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
	language = {en},
	number = {6},
	urldate = {2024-04-25},
	journal = {ACM Computing Surveys},
	author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
	month = jun,
	year = {2023},
	pages = {1--28},
}

@misc{munkhdalai_leave_2024,
	title = {Leave {No} {Context} {Behind}: {Efficient} {Infinite} {Context} {Transformers} with {Infini}-attention},
	shorttitle = {Leave {No} {Context} {Behind}},
	url = {http://arxiv.org/abs/2404.07143},
	abstract = {This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Munkhdalai, Tsendsuren and Faruqui, Manaal and Gopal, Siddharth},
	month = apr,
	year = {2024},
	note = {arXiv:2404.07143 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{ho_axial_2019,
	title = {Axial {Attention} in {Multidimensional} {Transformers}},
	url = {http://arxiv.org/abs/1912.12180},
	abstract = {We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness over joint distributions over data and ease of implementation with standard deep learning frameworks, while requiring reasonable memory and computation and achieving state-of-the-art results on standard generative modeling benchmarks. Our models are based on axial attention, a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. Notably the proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. This semi-parallel structure goes a long way to making decoding from even a very large Axial Transformer broadly applicable. We demonstrate state-of-the-art results for the Axial Transformer on the ImageNet-32 and ImageNet-64 image benchmarks as well as on the BAIR Robotic Pushing video benchmark. We open source the implementation of Axial Transformers.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Ho, Jonathan and Kalchbrenner, Nal and Weissenborn, Dirk and Salimans, Tim},
	month = dec,
	year = {2019},
	note = {arXiv:1912.12180 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{shen_efficient_2024,
	title = {Efficient {Attention}: {Attention} with {Linear} {Complexities}},
	shorttitle = {Efficient {Attention}},
	url = {http://arxiv.org/abs/1812.01243},
	abstract = {Dot-product attention has wide applications in computer vision and natural language processing. However, its memory and computational costs grow quadratically with the input size. Such growth prohibits its application on high-resolution inputs. To remedy this drawback, this paper proposes a novel efficient attention mechanism equivalent to dot-product attention but with substantially less memory and computational costs. Its resource efficiency allows more widespread and flexible integration of attention modules into a network, which leads to better accuracies. Empirical evaluations demonstrated the effectiveness of its advantages. Efficient attention modules brought significant performance boosts to object detectors and instance segmenters on MS-COCO 2017. Further, the resource efficiency democratizes attention to complex models, where high costs prohibit the use of dot-product attention. As an exemplar, a model with efficient attention achieved state-of-the-art accuracies for stereo depth estimation on the Scene Flow dataset. Code is available at https://github.com/cmsflash/efficient-attention.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Shen, Zhuoran and Zhang, Mingyuan and Zhao, Haiyu and Yi, Shuai and Li, Hongsheng},
	month = jan,
	year = {2024},
	note = {arXiv:1812.01243 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, I.2.10, I.2.6, I.4.6, I.4.8},
}

@misc{beltagy_longformer_2020,
	title = {Longformer: {The} {Long}-{Document} {Transformer}},
	shorttitle = {Longformer},
	url = {http://arxiv.org/abs/2004.05150},
	abstract = {Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
	month = dec,
	year = {2020},
	note = {arXiv:2004.05150 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{black_gpt-neo_2021,
	title = {{GPT}-{Neo}: {Large} {Scale} {Autoregressive} {Language} {Modeling} with {Mesh}-{Tensorflow}},
	copyright = {Apache License 2.0, Open Access},
	shorttitle = {{GPT}-{Neo}},
	url = {https://zenodo.org/record/5297715},
	abstract = {GPT-Neo is an implementation of model \&amp; data-parallel GPT-2 and GPT-3-like models, utilizing Mesh Tensorflow for distributed support. This codebase is designed for TPUs. It should also work on GPUs, though we do not recommend this hardware configuration.},
	urldate = {2024-04-24},
	publisher = {[object Object]},
	author = {Black, Sid and Leo, Gao and Wang, Phil and Leahy, Connor and Biderman, Stella},
	month = mar,
	year = {2021},
	doi = {10.5281/ZENODO.5297715},
	keywords = {Autoregressive language model, Massive language model, Transformers},
}

@article{devlin_bert_2018,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{BERT}},
	url = {https://arxiv.org/abs/1810.04805},
	doi = {10.48550/ARXIV.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2024-04-24},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2018},
	note = {Publisher: [object Object]
Version Number: 2},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@misc{munkhdalai_leave_2024-1,
	title = {Leave {No} {Context} {Behind}: {Efficient} {Infinite} {Context} {Transformers} with {Infini}-attention},
	shorttitle = {Leave {No} {Context} {Behind}},
	url = {http://arxiv.org/abs/2404.07143},
	abstract = {This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Munkhdalai, Tsendsuren and Faruqui, Manaal and Gopal, Siddharth},
	month = apr,
	year = {2024},
	note = {arXiv:2404.07143 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{hwang_transformerfam_2024,
	title = {{TransformerFAM}: {Feedback} attention is working memory},
	shorttitle = {{TransformerFAM}},
	url = {http://arxiv.org/abs/2404.09173},
	abstract = {While Transformers have revolutionized deep learning, their quadratic attention complexity hinders their ability to process infinitely long inputs. We propose Feedback Attention Memory (FAM), a novel Transformer architecture that leverages a feedback loop to enable the network to attend to its own latent representations. This design fosters the emergence of working memory within the Transformer, allowing it to process indefinitely long sequences. TransformerFAM requires no additional weights, enabling seamless integration with pre-trained models. Our experiments show that TransformerFAM significantly improves Transformer performance on long-context tasks across various model sizes (1B, 8B, and 24B). These results showcase the potential to empower Large Language Models (LLMs) to process sequences of unlimited length.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Hwang, Dongseong and Wang, Weiran and Huo, Zhuoyuan and Sim, Khe Chai and Mengibar, Pedro Moreno},
	month = apr,
	year = {2024},
	note = {arXiv:2404.09173 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{zhang_moefication_2022-1,
	title = {{MoEfication}: {Transformer} {Feed}-forward {Layers} are {Mixtures} of {Experts}},
	shorttitle = {{MoEfication}},
	url = {http://arxiv.org/abs/2110.01786},
	doi = {10.48550/arXiv.2110.01786},
	abstract = {Recent work has shown that feed-forward networks (FFNs) in pre-trained Transformers are a key component, storing various linguistic and factual knowledge. However, the computational patterns of FFNs are still unclear. In this work, we study the computational patterns of FFNs and observe that most inputs only activate a tiny ratio of neurons of FFNs. This phenomenon is similar to the sparsity of the human brain, which drives research on functional partitions of the human brain. To verify whether functional partitions also emerge in FFNs, we propose to convert a model into its MoE version with the same parameters, namely MoEfication. Specifically, MoEfication consists of two phases: (1) splitting the parameters of FFNs into multiple functional partitions as experts, and (2) building expert routers to decide which experts will be used for each input. Experimental results show that MoEfication can conditionally use 10\% to 30\% of FFN parameters while maintaining over 95\% original performance for different models on various downstream tasks. Besides, MoEfication brings two advantages: (1) it significantly reduces the FLOPS of inference, i.e., 2x speedup with 25\% of FFN parameters, and (2) it provides a fine-grained perspective to study the inner mechanism of FFNs. The source code of this paper can be obtained from https://github.com/thunlp/MoEfication.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Zhang, Zhengyan and Lin, Yankai and Liu, Zhiyuan and Li, Peng and Sun, Maosong and Zhou, Jie},
	month = apr,
	year = {2022},
	note = {arXiv:2110.01786 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{csordas_approximating_2023-1,
	address = {Singapore},
	title = {Approximating {Two}-{Layer} {Feedforward} {Networks} for {Efficient} {Transformers}},
	url = {https://aclanthology.org/2023.findings-emnlp.49},
	doi = {10.18653/v1/2023.findings-emnlp.49},
	abstract = {How to reduce compute and memory requirements of neural networks (NNs) without sacrificing performance? Many recent works use sparse Mixtures of Experts (MoEs) to build resource-efficient large language models (LMs). Here we introduce several novel perspectives on MoEs, presenting a general framework that *unifies* various methods to *approximate two-layer NNs* (e.g., feedforward blocks of Transformers), including product-key memories (PKMs). Leveraging insights from this framework, we propose methods to improve both MoEs and PKMs. Unlike prior work that compares MoEs with dense baselines under the *compute-equal* condition, our evaluation condition is *parameter-equal*, which is crucial to properly evaluate LMs. We show that our MoEs are competitive with the *dense* Transformer-XL on both the WikiText-103 and enwiki8 datasets at two different scales, while being much more resource efficient. This demonstrates that MoEs are relevant not only to extremely large LMs but also to any-scale resource-efficient LMs. Our code is public.},
	urldate = {2024-04-24},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Csord√°s, R√≥bert and Irie, Kazuki and Schmidhuber, J√ºrgen},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {674--692},
}

@inproceedings{chen_learning_2023,
	title = {Learning a {Sparse} {Transformer} {Network} for {Effective} {Image} {Deraining}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Learning_a_Sparse_Transformer_Network_for_Effective_Image_Deraining_CVPR_2023_paper.html},
	language = {en},
	urldate = {2024-04-24},
	author = {Chen, Xiang and Li, Hao and Li, Mingqiang and Pan, Jinshan},
	year = {2023},
	pages = {5896--5905},
}

@inproceedings{winata_lightweight_2020-2,
	title = {Lightweight and {Efficient} {End}-{To}-{End} {Speech} {Recognition} {Using} {Low}-{Rank} {Transformer}},
	url = {https://ieeexplore.ieee.org/abstract/document/9053878?casa_token=vr_E3t-_vYQAAAAA:_Ow2ipLtLBqCSpVB1fAthU3uPnraIjIBTyLp3gpx-UTCtfAAB_ZdEdeIDHAFUfRiHu7jLnI},
	doi = {10.1109/ICASSP40776.2020.9053878},
	abstract = {Highly performing deep neural networks come at the cost of computational complexity that limits their practicality for deployment on portable devices. We propose the low-rank transformer (LRT), a memory-efficient and fast neural architecture that significantly reduces the parameters and boosts the speed of training and inference for end-to-end speech recognition. Our approach reduces the number of parameters of the network by more than 50\% and speeds up the inference time by around 1.35x compared to the baseline transformer model. The experiments show that our LRT model generalizes better and yields lower error rates on both validation and test sets compared to an uncompressed transformer model. The LRT model outperforms those from existing works on several datasets in an end-to-end setting without using an external language model or acoustic data.},
	urldate = {2024-04-24},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Winata, Genta Indra and Cahyawijaya, Samuel and Lin, Zhaojiang and Liu, Zihan and Fung, Pascale},
	month = may,
	year = {2020},
	note = {ISSN: 2379-190X},
	keywords = {Acoustics, Computer architecture, Data models, Light rail systems, Speech processing, Speech recognition, Training, end-to-end, low-rank, model compression, speech recognition, transformer},
	pages = {6144--6148},
}

@article{tay_efficient_2022,
	title = {Efficient {Transformers}: {A} {Survey}},
	volume = {55},
	issn = {0360-0300},
	shorttitle = {Efficient {Transformers}},
	url = {https://dl.acm.org/doi/10.1145/3530811},
	doi = {10.1145/3530811},
	abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision, and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of ‚ÄúX-former‚Äù models have been proposed‚ÄîReformer, Linformer, Performer, Longformer, to name a few‚Äîwhich improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this article characterizes a large and thoughtful selection of recent efficiency-flavored ‚ÄúX-former‚Äù models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
	number = {6},
	urldate = {2024-04-24},
	journal = {ACM Computing Surveys},
	author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
	month = dec,
	year = {2022},
	keywords = {Transformers, attention, deep learning, neural networks},
	pages = {109:1--109:28},
}

@misc{zhao_inrank_2023,
	title = {{InRank}: {Incremental} {Low}-{Rank} {Learning}},
	shorttitle = {{InRank}},
	url = {http://arxiv.org/abs/2306.11250},
	doi = {10.48550/arXiv.2306.11250},
	abstract = {The theory of greedy low-rank learning (GLRL) aims to explain the impressive generalization capabilities of deep learning. It proves that stochastic gradient-based training implicitly regularizes neural networks towards low-rank solutions through a gradual increase of the rank during training. However, there is a gap between theory and practice since GLRL requires an infinitesimal initialization of the weights, which is not practical due to the fact that it is a saddle point. In this work, we remove the assumption of infinitesimal initialization by focusing on cumulative weight updates. We prove the cumulative weight updates follow an incremental low-rank trajectory for arbitrary orthogonal initialization of weights in a three-layer linear network. Empirically, we demonstrate that our theory holds on a broad range of neural networks (e.g., transformers) and standard training algorithms (e.g., SGD, Adam). However, existing training algorithms do not exploit the low-rank property to improve computational efficiency as the networks are not parameterized in low-rank. To remedy this, we design a new training algorithm Incremental Low-Rank Learning (InRank), which explicitly expresses cumulative weight updates as low-rank matrices while incrementally augmenting their ranks during training. We evaluate InRank on GPT-2, and our results indicate that InRank achieves comparable prediction performance as the full-rank counterpart while requiring at most 33\% of the total ranks throughout training. We also propose an efficient version of InRank that achieves a reduction of 37\% in total training time and 36\% in model size when training GPT-medium on WikiText-103 from scratch.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Zhao, Jiawei and Zhang, Yifei and Chen, Beidi and Sch√§fer, Florian and Anandkumar, Anima},
	month = dec,
	year = {2023},
	note = {arXiv:2306.11250 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{noauthor_introduction_nodate,
	title = {Introduction {\textbar} {Machine} {Learning} {\textbar} {Google} for {Developers}},
	url = {https://developers.google.com/machine-learning/gan},
	language = {en},
	urldate = {2024-04-24},
}

@misc{noauthor_background_nodate,
	title = {Background: {What} is a {Generative} {Model}? {\textbar} {Machine} {Learning}},
	shorttitle = {Background},
	url = {https://developers.google.com/machine-learning/gan/generative},
	language = {en},
	urldate = {2024-04-24},
	journal = {Google for Developers},
}

@misc{mirzadeh_relu_2023-1,
	title = {{ReLU} {Strikes} {Back}: {Exploiting} {Activation} {Sparsity} in {Large} {Language} {Models}},
	shorttitle = {{ReLU} {Strikes} {Back}},
	url = {http://arxiv.org/abs/2310.04564},
	abstract = {Large Language Models (LLMs) with billions of parameters have drastically transformed AI applications. However, their demanding computation during inference has raised significant challenges for deployment on resource-constrained devices. Despite recent trends favoring alternative activation functions such as GELU or SiLU, known for increased computation, this study strongly advocates for reinstating ReLU activation in LLMs. We demonstrate that using the ReLU activation function has a negligible impact on convergence and performance while significantly reducing computation and weight transfer. This reduction is particularly valuable during the memory-bound inference step, where efficiency is paramount. Exploring sparsity patterns in ReLU-based LLMs, we unveil the reutilization of activated neurons for generating new tokens and leveraging these insights, we propose practical strategies to substantially reduce LLM inference computation up to three times, using ReLU activations with minimal performance trade-offs.},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Mirzadeh, Iman and Alizadeh, Keivan and Mehta, Sachin and Del Mundo, Carlo C. and Tuzel, Oncel and Samei, Golnoosh and Rastegari, Mohammad and Farajtabar, Mehrdad},
	month = oct,
	year = {2023},
	note = {arXiv:2310.04564 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, FFNs and activations},
}

@misc{warstadt_call_2023,
	title = {Call for {Papers} -- {The} {BabyLM} {Challenge}: {Sample}-efficient pretraining on a developmentally plausible corpus},
	shorttitle = {Call for {Papers} -- {The} {BabyLM} {Challenge}},
	url = {http://arxiv.org/abs/2301.11796},
	doi = {10.48550/arXiv.2301.11796},
	abstract = {We present the call for papers for the BabyLM Challenge: Sample-efficient pretraining on a developmentally plausible corpus. This shared task is intended for participants with an interest in small scale language modeling, human language acquisition, low-resource NLP, and cognitive modeling. In partnership with CoNLL and CMCL, we provide a platform for approaches to pretraining with a limited-size corpus sourced from data inspired by the input to children. The task has three tracks, two of which restrict the training data to pre-released datasets of 10M and 100M words and are dedicated to explorations of approaches such as architectural variations, self-supervised objectives, or curriculum learning. The final track only restricts the amount of text used, allowing innovation in the choice of the data, its domain, and even its modality (i.e., data from sources other than text is welcome). We will release a shared evaluation pipeline which scores models on a variety of benchmarks and tasks, including targeted syntactic evaluations and natural language understanding.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Warstadt, Alex and Choshen, Leshem and Mueller, Aaron and Williams, Adina and Wilcox, Ethan and Zhuang, Chengxu},
	month = jan,
	year = {2023},
	note = {arXiv:2301.11796 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{warstadt_findings_2023,
	address = {Singapore},
	title = {Findings of the {BabyLM} {Challenge}: {Sample}-{Efficient} {Pretraining} on {Developmentally} {Plausible} {Corpora}},
	shorttitle = {Findings of the {BabyLM} {Challenge}},
	url = {https://aclanthology.org/2023.conll-babylm.1},
	doi = {10.18653/v1/2023.conll-babylm.1},
	urldate = {2024-04-24},
	booktitle = {Proceedings of the {BabyLM} {Challenge} at the 27th {Conference} on {Computational} {Natural} {Language} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Warstadt, Alex and Mueller, Aaron and Choshen, Leshem and Wilcox, Ethan and Zhuang, Chengxu and Ciro, Juan and Mosquera, Rafael and Paranjabe, Bhargavi and Williams, Adina and Linzen, Tal and Cotterell, Ryan},
	editor = {Warstadt, Alex and Mueller, Aaron and Choshen, Leshem and Wilcox, Ethan and Zhuang, Chengxu and Ciro, Juan and Mosquera, Rafael and Paranjabe, Bhargavi and Williams, Adina and Linzen, Tal and Cotterell, Ryan},
	month = dec,
	year = {2023},
	pages = {1--34},
}

@misc{wang_glue_2019,
	title = {{GLUE}: {A} {Multi}-{Task} {Benchmark} and {Analysis} {Platform} for {Natural} {Language} {Understanding}},
	shorttitle = {{GLUE}},
	url = {http://arxiv.org/abs/1804.07461},
	doi = {10.48550/arXiv.1804.07461},
	abstract = {For natural language understanding (NLU) technology to be maximally useful, both practically and as a scientific object of study, it must be general: it must be able to process language in a way that is not exclusively tailored to any one specific task or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation benchmark (GLUE), a tool for evaluating and analyzing the performance of models across a diverse range of existing NLU tasks. GLUE is model-agnostic, but it incentivizes sharing knowledge across tasks because certain tasks have very limited training data. We further provide a hand-crafted diagnostic test suite that enables detailed linguistic analysis of NLU models. We evaluate baselines based on current methods for multi-task and transfer learning and find that they do not immediately give substantial improvements over the aggregate performance of training a separate model per task, indicating room for improvement in developing general and robust NLU systems.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
	month = feb,
	year = {2019},
	note = {arXiv:1804.07461 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{mckenzie_inverse_2023,
	title = {Inverse {Scaling}: {When} {Bigger} {Isn}'t {Better}},
	shorttitle = {Inverse {Scaling}},
	url = {http://arxiv.org/abs/2306.09479},
	doi = {10.48550/arXiv.2306.09479},
	abstract = {Work on scaling laws has found that large language models (LMs) show predictable improvements to overall loss with increased scale (model size, training data, and compute). Here, we present evidence for the claim that LMs may show inverse scaling, or worse task performance with increased scale, e.g., due to flaws in the training objective and data. We present empirical evidence of inverse scaling on 11 datasets collected by running a public contest, the Inverse Scaling Prize, with a substantial prize pool. Through analysis of the datasets, along with other examples found in the literature, we identify four potential causes of inverse scaling: (i) preference to repeat memorized sequences over following in-context instructions, (ii) imitation of undesirable patterns in the training data, (iii) tasks containing an easy distractor task which LMs could focus on, rather than the harder real task, and (iv) correct but misleading few-shot demonstrations of the task. We release the winning datasets at https://inversescaling.com/data to allow for further investigation of inverse scaling. Our tasks have helped drive the discovery of U-shaped and inverted-U scaling trends, where an initial trend reverses, suggesting that scaling trends are less reliable at predicting the behavior of larger-scale models than previously understood. Overall, our results suggest that there are tasks for which increased model scale alone may not lead to progress, and that more careful thought needs to go into the data and objectives for training language models.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {McKenzie, Ian R. and Lyzhov, Alexander and Pieler, Michael and Parrish, Alicia and Mueller, Aaron and Prabhu, Ameya and McLean, Euan and Kirtland, Aaron and Ross, Alexis and Liu, Alisa and Gritsevskiy, Andrew and Wurgaft, Daniel and Kauffman, Derik and Recchia, Gabriel and Liu, Jiacheng and Cavanagh, Joe and Weiss, Max and Huang, Sicong and Droid, The Floating and Tseng, Tom and Korbak, Tomasz and Shen, Xudong and Zhang, Yuhui and Zhou, Zhengping and Kim, Najoung and Bowman, Samuel R. and Perez, Ethan},
	month = jun,
	year = {2023},
	note = {arXiv:2306.09479 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
}

@misc{zou_representation_2023,
	title = {Representation {Engineering}: {A} {Top}-{Down} {Approach} to {AI} {Transparency}},
	shorttitle = {Representation {Engineering}},
	url = {http://arxiv.org/abs/2310.01405},
	doi = {10.48550/arXiv.2310.01405},
	abstract = {In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and Goel, Shashwat and Li, Nathaniel and Byun, Michael J. and Wang, Zifan and Mallen, Alex and Basart, Steven and Koyejo, Sanmi and Song, Dawn and Fredrikson, Matt and Kolter, J. Zico and Hendrycks, Dan},
	month = oct,
	year = {2023},
	note = {arXiv:2310.01405 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@misc{eldan_tinystories_2023,
	title = {{TinyStories}: {How} {Small} {Can} {Language} {Models} {Be} and {Still} {Speak} {Coherent} {English}?},
	shorttitle = {{TinyStories}},
	url = {http://arxiv.org/abs/2305.07759},
	doi = {10.48550/arXiv.2305.07759},
	abstract = {Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention). In this work, we introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities. We also introduce a new paradigm for the evaluation of language models: We suggest a framework which uses GPT-4 to grade the content generated by these models as if those were stories written by students and graded by a (human) teacher. This new paradigm overcomes the flaws of standard benchmarks which often requires the model's output to be very structures, and moreover provides a multidimensional score for the model, providing scores for different capabilities such as grammar, creativity and consistency. We hope that TinyStories can facilitate the development, analysis and research of LMs, especially for low-resource or specialized domains, and shed light on the emergence of language capabilities in LMs.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Eldan, Ronen and Li, Yuanzhi},
	month = may,
	year = {2023},
	note = {arXiv:2305.07759 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{warstadt_blimp_2023-1,
	title = {{BLiMP}: {The} {Benchmark} of {Linguistic} {Minimal} {Pairs} for {English}},
	shorttitle = {{BLiMP}},
	url = {http://arxiv.org/abs/1912.00582},
	doi = {10.48550/arXiv.1912.00582},
	abstract = {We introduce The Benchmark of Linguistic Minimal Pairs (shortened to BLiMP), a challenge set for evaluating what language models (LMs) know about major grammatical phenomena in English. BLiMP consists of 67 sub-datasets, each containing 1000 minimal pairs isolating specific contrasts in syntax, morphology, or semantics. The data is automatically generated according to expert-crafted grammars, and aggregate human agreement with the labels is 96.4\%. We use it to evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs. We find that state-of-the-art models identify morphological contrasts reliably, but they struggle with semantic restrictions on the distribution of quantifiers and negative polarity items and subtle syntactic phenomena such as extraction islands.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey, Anhad and Peng, Wei and Wang, Sheng-Fu and Bowman, Samuel R.},
	month = feb,
	year = {2023},
	note = {arXiv:1912.00582 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]
version: 5},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{noauthor_notitle_nodate,
	url = {http://arxiv.org/abs/2310.04564},
}

@article{noauthor_notitle_nodate-1,
}

@article{levine_depth--width_2020,
	title = {The {Depth}-to-{Width} {Interplay} in {Self}-{Attention}},
	copyright = {Creative Commons Zero v1.0 Universal},
	url = {https://arxiv.org/abs/2006.12467},
	doi = {10.48550/ARXIV.2006.12467},
	abstract = {Self-attention architectures, which are rapidly pushing the frontier in natural language processing, demonstrate a surprising depth-inefficient behavior: previous works indicate that increasing the internal representation (network width) is just as useful as increasing the number of self-attention layers (network depth). We theoretically predict a width-dependent transition between depth-efficiency and depth-inefficiency in self-attention. We conduct systematic empirical ablations on networks of depths 6 to 48 that clearly reveal the theoretically predicted behaviors, and provide explicit quantitative suggestions regarding the optimal depth-to-width allocation for a given self-attention network size. The race towards beyond 1-Trillion parameter language models renders informed guidelines for increasing self-attention depth and width in tandem an essential ingredient. Our guidelines elucidate the depth-to-width trade-off in self-attention networks of sizes up to the scale of GPT3 (which we project to be too deep for its size), and beyond, marking an unprecedented width of 30K as optimal for a 1-Trillion parameter network.},
	urldate = {2024-04-23},
	author = {Levine, Yoav and Wies, Noam and Sharir, Or and Bata, Hofit and Shashua, Amnon},
	year = {2020},
	note = {Publisher: [object Object]
Version Number: 3},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

@misc{dong_attention_2023,
	title = {Attention is {Not} {All} {You} {Need}: {Pure} {Attention} {Loses} {Rank} {Doubly} {Exponentially} with {Depth}},
	shorttitle = {Attention is {Not} {All} {You} {Need}},
	url = {http://arxiv.org/abs/2103.03404},
	doi = {10.48550/arXiv.2103.03404},
	abstract = {Attention-based architectures have become ubiquitous in machine learning, yet our understanding of the reasons for their effectiveness remains limited. This work proposes a new way to understand self-attention networks: we show that their output can be decomposed into a sum of smaller terms, each involving the operation of a sequence of attention heads across layers. Using this decomposition, we prove that self-attention possesses a strong inductive bias towards "token uniformity". Specifically, without skip connections or multi-layer perceptrons (MLPs), the output converges doubly exponentially to a rank-1 matrix. On the other hand, skip connections and MLPs stop the output from degeneration. Our experiments verify the identified convergence phenomena on different variants of standard transformer architectures.},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Dong, Yihe and Cordonnier, Jean-Baptiste and Loukas, Andreas},
	month = aug,
	year = {2023},
	note = {arXiv:2103.03404 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{brown_wide_2022-1,
	title = {Wide {Attention} {Is} {The} {Way} {Forward} {For} {Transformers}?},
	url = {http://arxiv.org/abs/2210.00640},
	abstract = {The Transformer is an extremely powerful and prominent deep learning architecture. In this work, we challenge the commonly held belief in deep learning that going deeper is better, and show an alternative design approach that is building wider attention Transformers. We demonstrate that wide single layer Transformer models can compete with or outperform deeper ones in a variety of Natural Language Processing (NLP) tasks when both are trained from scratch. The impact of changing the model aspect ratio on Transformers is then studied systematically. This ratio balances the number of layers and the number of attention heads per layer while keeping the total number of attention heads and all other hyperparameters constant. On average, across 4 NLP tasks and 10 attention types, single layer wide models perform 0.3\% better than their deep counterparts. We show an in-depth evaluation and demonstrate how wide models require a far smaller memory footprint and can run faster on commodity hardware, in addition, these wider models are also more interpretable. For example, a single layer Transformer on the IMDb byte level text classification has 3.1x faster inference latency on a CPU than its equally accurate deeper counterpart, and is half the size. We therefore put forward wider and shallower models as a viable and desirable alternative for small models on NLP tasks, and as an important area of research for domains beyond this.},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Brown, Jason Ross and Zhao, Yiren and Shumailov, Ilia and Mullins, Robert D.},
	month = nov,
	year = {2022},
	note = {arXiv:2210.00640 [cs]},
	keywords = {Computer Science - Machine Learning, I.2.7},
}

@article{lee_deep_2018,
	title = {{DEEP} {NEURAL} {NETWORKS} {AS} {GAUSSIAN} {PROCESSES}},
	abstract = {It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of inÔ¨Ånite network width. This correspondence enables exact Bayesian inference for inÔ¨Ånite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identiÔ¨Åed that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network.},
	language = {en},
	author = {Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
	year = {2018},
}

@misc{nguyen_wide_2021,
	title = {Do {Wide} and {Deep} {Networks} {Learn} the {Same} {Things}? {Uncovering} {How} {Neural} {Network} {Representations} {Vary} with {Width} and {Depth}},
	shorttitle = {Do {Wide} and {Deep} {Networks} {Learn} the {Same} {Things}?},
	url = {http://arxiv.org/abs/2010.15327},
	doi = {10.48550/arXiv.2010.15327},
	abstract = {A key factor in the success of deep neural networks is the ability to scale models to improve performance by varying the architecture depth and width. This simple property of neural network design has resulted in highly effective architectures for a variety of tasks. Nevertheless, there is limited understanding of effects of depth and width on the learned representations. In this paper, we study this fundamental question. We begin by investigating how varying depth and width affects model hidden representations, finding a characteristic block structure in the hidden representations of larger capacity (wider or deeper) models. We demonstrate that this block structure arises when model capacity is large relative to the size of the training set, and is indicative of the underlying layers preserving and propagating the dominant principal component of their representations. This discovery has important ramifications for features learned by different models, namely, representations outside the block structure are often similar across architectures with varying widths and depths, but the block structure is unique to each model. We analyze the output predictions of different model architectures, finding that even when the overall accuracy is similar, wide and deep models exhibit distinctive error patterns and variations across classes.},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Nguyen, Thao and Raghu, Maithra and Kornblith, Simon},
	month = apr,
	year = {2021},
	note = {arXiv:2010.15327 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{kobayashi_analyzing_2024,
	title = {Analyzing {Feed}-{Forward} {Blocks} in {Transformers} through the {Lens} of {Attention} {Maps}},
	url = {http://arxiv.org/abs/2302.00456},
	doi = {10.48550/arXiv.2302.00456},
	abstract = {Transformers are ubiquitous in wide tasks. Interpreting their internals is a pivotal goal. Nevertheless, their particular components, feed-forward (FF) blocks, have typically been less analyzed despite their substantial parameter amounts. We analyze the input contextualization effects of FF blocks by rendering them in the attention maps as a human-friendly visualization scheme. Our experiments with both masked- and causal-language models reveal that FF networks modify the input contextualization to emphasize specific types of linguistic compositions. In addition, FF and its surrounding components tend to cancel out each other's effects, suggesting potential redundancy in the processing of the Transformer layer.},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Kobayashi, Goro and Kuribayashi, Tatsuki and Yokoi, Sho and Inui, Kentaro},
	month = apr,
	year = {2024},
	note = {arXiv:2302.00456 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{pires_one_2023,
	title = {One {Wide} {Feedforward} is {All} {You} {Need}},
	url = {http://arxiv.org/abs/2309.01826},
	doi = {10.48550/arXiv.2309.01826},
	abstract = {The Transformer architecture has two main non-embedding components: Attention and the Feed Forward Network (FFN). Attention captures interdependencies between words regardless of their position, while the FFN non-linearly transforms each input token independently. In this work we explore the role of the FFN, and find that despite taking up a significant fraction of the model's parameters, it is highly redundant. Concretely, we are able to substantially reduce the number of parameters with only a modest drop in accuracy by removing the FFN on the decoder layers and sharing a single FFN across the encoder. Finally we scale this architecture back to its original size by increasing the hidden dimension of the shared FFN, achieving substantial gains in both accuracy and latency with respect to the original Transformer Big.},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Pires, Telmo Pessoa and Lopes, Ant√≥nio V. and Assogba, Yannick and Setiawan, Hendra},
	month = oct,
	year = {2023},
	note = {arXiv:2309.01826 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{geva_transformer_2021,
	title = {Transformer {Feed}-{Forward} {Layers} {Are} {Key}-{Value} {Memories}},
	url = {http://arxiv.org/abs/2012.14913},
	doi = {10.48550/arXiv.2012.14913},
	abstract = {Feed-forward layers constitute two-thirds of a transformer model's parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys' input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model's layers via residual connections to produce the final output distribution.},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
	month = sep,
	year = {2021},
	note = {arXiv:2012.14913 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{hassibi_second_1992,
	title = {Second order derivatives for network pruning: {Optimal} {Brain} {Surgeon}},
	volume = {5},
	shorttitle = {Second order derivatives for network pruning},
	url = {https://proceedings.neurips.cc/paper/1992/hash/303ed4c69846ab36c2904d3ba8573050-Abstract.html},
	abstract = {We investigate the use of information from all second order derivatives of the error  function to perfonn network pruning (i.e., removing unimportant weights from a trained  network) in order to improve generalization, simplify networks, reduce hardware or  storage requirements, increase the speed of further training, and in some cases enable rule  extraction. Our method, Optimal Brain Surgeon (OBS), is Significantly better than  magnitude-based methods and Optimal Brain Damage [Le Cun, Denker and Sol1a, 1990],  which often remove the wrong weights. OBS permits the pruning of more weights than  other methods (for the same error on the training set), and thus yields better  generalization on test data. Crucial to OBS is a recursion relation for calculating the  inverse Hessian matrix H-I from training data and structural information of the net. OBS  permits a 90\%, a 76\%, and a 62\% reduction in weights over backpropagation with weighL  decay on three benchmark MONK's problems [Thrun et aI., 1991]. Of OBS, Optimal  Brain Damage, and magnitude-based methods, only OBS deletes the correct weights from  a trained XOR network in every case. Finally, whereas Sejnowski and Rosenberg [1987J  used 18,000 weights in their NETtalk network, we used OBS to prune a network to just  1560 weights, yielding better generalization.},
	urldate = {2024-04-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Morgan-Kaufmann},
	author = {Hassibi, Babak and Stork, David},
	year = {1992},
}

@misc{noauthor_pdf_nodate-1,
	title = {[{PDF}] {ToddlerBERTa}: {Exploiting} {BabyBERTa} for {Grammar} {Learning} and {Language} {Understanding} {\textbar} {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/reader/1dca108e88dbf69859b329b9e5cb00f36851a738},
	urldate = {2024-04-22},
}

@inproceedings{fields_exploring_2023,
	address = {Singapore},
	title = {Exploring {Transformers} as {Compact}, {Data}-efficient {Language} {Models}},
	url = {https://aclanthology.org/2023.conll-1.35},
	doi = {10.18653/v1/2023.conll-1.35},
	abstract = {Large scale transformer models, trained with massive datasets have become the standard in natural language processing. The huge size of most transformers make research with these models impossible for those with limited computational resources. Additionally, the enormous pretraining data requirements of transformers exclude pretraining them with many smaller datasets that might provide enlightening results. In this study, we show that transformers can be significantly reduced in size, with as few as 5.7 million parameters, and still retain most of their downstream capability. Further we show that transformer models can retain comparable results when trained on human-scale datasets, as few as 5 million words of pretraining data. Overall, the results of our study suggest transformers function well as compact, data efficient language models and that complex model compression methods, such as model distillation are not necessarily superior to pretraining reduced size transformer models from scratch.},
	urldate = {2024-04-22},
	booktitle = {Proceedings of the 27th {Conference} on {Computational} {Natural} {Language} {Learning} ({CoNLL})},
	publisher = {Association for Computational Linguistics},
	author = {Fields, Clayton and Kennington, Casey},
	editor = {Jiang, Jing and Reitter, David and Deng, Shumin},
	month = dec,
	year = {2023},
	pages = {521--531},
}

@article{hochreiter_flat_1997,
	title = {Flat minima},
	volume = {9},
	issn = {0899-7667},
	doi = {10.1162/neco.1997.9.1.1},
	abstract = {We present a new algorithm for finding low-complexity neural networks with high generalization capability. The algorithm searches for a "flat" minimum of the error function. A flat minimum is a large connected region in weight space where the error remains approximately constant. An MDL-based, Bayesian argument suggests that flat minima correspond to "simple" networks and low expected overfitting. The argument is based on a Gibbs algorithm variant and a novel way of splitting generalization error into underfitting and overfitting error. Unlike many previous approaches, ours does not require gaussian assumptions and does not depend on a "good" weight prior. Instead we have a prior over input-output functions, thus taking into account net architecture and training set. Although our algorithm requires the computation of second-order derivatives, it has backpropagation's order of complexity. Automatically, it effectively prunes units, weights, and input lines. Various experiments with feedforward and recurrent nets are described. In an application to stock market prediction, flat minimum search outperforms conventional backprop, weight decay, and "optimal brain surgeon/optimal brain damage".},
	language = {eng},
	number = {1},
	journal = {Neural Computation},
	author = {Hochreiter, S. and Schmidhuber, J.},
	month = jan,
	year = {1997},
	pmid = {9117894},
	keywords = {Algorithms, Artifacts, Bayes Theorem, Mathematics, Models, Econometric, Neural Networks, Computer, Probability, Reproducibility of Results},
	pages = {1--42},
}

@article{balderas_optimizing_2024,
	title = {Optimizing dense feed-forward neural networks},
	volume = {171},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608023007219},
	doi = {10.1016/j.neunet.2023.12.015},
	abstract = {Deep learning models have been widely used during the last decade due to their outstanding learning and abstraction capacities. However, one of the main challenges any scientist has to face using deep learning models is to establish the network‚Äôs architecture. Due to this difficulty, data scientists usually build over complex models and, as a result, most of them result computationally intensive and impose a large memory footprint, generating huge costs, contributing to climate change and hindering their use in computational-limited devices. In this paper, we propose a novel dense feed-forward neural network constructing method based on pruning and transfer learning. Its performance has been thoroughly assessed in classification and regression problems. Without any accuracy loss, our approach can compress the number of parameters by more than 70\%. Even further, choosing the pruning parameter carefully, most of the refined models outperform original ones. Furthermore, we have verified that our method not only identifies a better network architecture but also facilitates knowledge transfer between the original and refined models. The results obtained show that our constructing method not only helps in the design of more efficient models but also more effective ones.},
	urldate = {2024-04-22},
	journal = {Neural Networks},
	author = {Balderas, Luis and Lastra, Miguel and Ben√≠tez, Jos√© M.},
	month = mar,
	year = {2024},
	keywords = {Dense feed-forward neural network optimization, Neural network pruning},
	pages = {229--241},
}

@inproceedings{warstadt_findings_2023-1,
	address = {Singapore},
	title = {Findings of the {BabyLM} {Challenge}: {Sample}-{Efficient} {Pretraining} on {Developmentally} {Plausible} {Corpora}},
	shorttitle = {Findings of the {BabyLM} {Challenge}},
	url = {https://aclanthology.org/2023.conll-babylm.1},
	doi = {10.18653/v1/2023.conll-babylm.1},
	urldate = {2024-04-22},
	booktitle = {Proceedings of the {BabyLM} {Challenge} at the 27th {Conference} on {Computational} {Natural} {Language} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Warstadt, Alex and Mueller, Aaron and Choshen, Leshem and Wilcox, Ethan and Zhuang, Chengxu and Ciro, Juan and Mosquera, Rafael and Paranjabe, Bhargavi and Williams, Adina and Linzen, Tal and Cotterell, Ryan},
	editor = {Warstadt, Alex and Mueller, Aaron and Choshen, Leshem and Wilcox, Ethan and Zhuang, Chengxu and Ciro, Juan and Mosquera, Rafael and Paranjabe, Bhargavi and Williams, Adina and Linzen, Tal and Cotterell, Ryan},
	month = dec,
	year = {2023},
	pages = {1--34},
}

@misc{chandy_dyad_2023,
	title = {{DYAD}: {A} {Descriptive} {Yet} {Abjuring} {Density} efficient approximation to linear neural network layers},
	shorttitle = {{DYAD}},
	url = {http://arxiv.org/abs/2312.06881},
	abstract = {We devise, implement and performance-asses DYAD, a layer which can serve as a faster and more memory-efficient approximate replacement for linear layers, (nn.Linear() in Pytorch). These layers appear in common subcomponents, such as in the ff module of Transformers. DYAD is based on a bespoke near-sparse matrix structure which approximates the dense "weight" matrix W that matrix-multiplies the input in the typical realization of such a layer, a.k.a DENSE. Our alternative near-sparse matrix structure is decomposable to a sum of 2 matrices permutable to a block-sparse counterpart. These can be represented as 3D tensors, which in unison allow a faster execution of matrix multiplication with the mini-batched input matrix X compared to DENSE (O(rows(W ) x cols(W )) --{\textgreater} O( rows(W ) x cols(W ) \# of blocks )). As the crux of our experiments, we pretrain both DYAD and DENSE variants of 2 sizes of the OPT arch and 1 size of the Pythia arch, including at different token scales of the babyLM benchmark. We find DYAD to be competitive ({\textgreater}= 90\%) of DENSE performance on zero-shot (e.g. BLIMP), few-shot (OPENLM) and finetuning (GLUE) benchmarks, while being {\textgreater}=7-15\% faster to train on-GPU even at 125m scale, besides surfacing larger speedups at increasing scale and model width.},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Chandy, Sarin and Gangal, Varun and Yang, Yi and Maggiotti, Gabriel},
	month = dec,
	year = {2023},
	note = {arXiv:2312.06881 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{nguyen_transformers_2019,
	title = {Transformers without {Tears}: {Improving} the {Normalization} of {Self}-{Attention}},
	shorttitle = {Transformers without {Tears}},
	url = {http://arxiv.org/abs/1910.05895},
	doi = {10.5281/zenodo.3525484},
	abstract = {We evaluate three simple, normalization-centric changes to improve Transformer training. First, we show that pre-norm residual connections (PreNorm) and smaller initializations enable warmup-free, validation-based training with large learning rates. Second, we propose \${\textbackslash}ell\_2\$ normalization with a single scale parameter (ScaleNorm) for faster training and better performance. Finally, we reaffirm the effectiveness of normalizing word embeddings to a fixed length (FixNorm). On five low-resource translation pairs from TED Talks-based corpora, these changes always converge, giving an average +1.1 BLEU over state-of-the-art bilingual baselines and a new 32.8 BLEU on IWSLT'15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Surprisingly, in the high-resource setting (WMT'14 English-German), ScaleNorm and FixNorm remain competitive but PreNorm degrades performance.},
	urldate = {2024-04-15},
	author = {Nguyen, Toan Q. and Salazar, Julian},
	month = nov,
	year = {2019},
	note = {arXiv:1910.05895 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{choshen_call_2024,
	title = {[{Call} for {Papers}] {The} 2nd {BabyLM} {Challenge}: {Sample}-efficient pretraining on a developmentally plausible corpus},
	shorttitle = {[{Call} for {Papers}] {The} 2nd {BabyLM} {Challenge}},
	url = {http://arxiv.org/abs/2404.06214},
	abstract = {After last year's successful BabyLM Challenge, the competition will be hosted again in 2024/2025. The overarching goals of the challenge remain the same; however, some of the competition rules will be different. The big changes for this year's competition are as follows: First, we replace the loose track with a paper track, which allows (for example) non-model-based submissions, novel cognitively-inspired benchmarks, or analysis techniques. Second, we are relaxing the rules around pretraining data, and will now allow participants to construct their own datasets provided they stay within the 100M-word or 10M-word budget. Third, we introduce a multimodal vision-and-language track, and will release a corpus of 50\% text-only and 50\% image-text multimodal data as a starting point for LM model training. The purpose of this CfP is to provide rules for this year's challenge, explain these rule changes and their rationale in greater detail, give a timeline of this year's competition, and provide answers to frequently asked questions from last year's challenge.},
	urldate = {2024-04-15},
	publisher = {arXiv},
	author = {Choshen, Leshem and Cotterell, Ryan and Hu, Michael Y. and Linzen, Tal and Mueller, Aaron and Ross, Candace and Warstadt, Alex and Wilcox, Ethan and Williams, Adina and Zhuang, Chengxu},
	month = apr,
	year = {2024},
	note = {arXiv:2404.06214 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{warstadt_call_2023-1,
	title = {Call for {Papers} -- {The} {BabyLM} {Challenge}: {Sample}-efficient pretraining on a developmentally plausible corpus},
	shorttitle = {Call for {Papers} -- {The} {BabyLM} {Challenge}},
	url = {http://arxiv.org/abs/2301.11796},
	abstract = {We present the call for papers for the BabyLM Challenge: Sample-efficient pretraining on a developmentally plausible corpus. This shared task is intended for participants with an interest in small scale language modeling, human language acquisition, low-resource NLP, and cognitive modeling. In partnership with CoNLL and CMCL, we provide a platform for approaches to pretraining with a limited-size corpus sourced from data inspired by the input to children. The task has three tracks, two of which restrict the training data to pre-released datasets of 10M and 100M words and are dedicated to explorations of approaches such as architectural variations, self-supervised objectives, or curriculum learning. The final track only restricts the amount of text used, allowing innovation in the choice of the data, its domain, and even its modality (i.e., data from sources other than text is welcome). We will release a shared evaluation pipeline which scores models on a variety of benchmarks and tasks, including targeted syntactic evaluations and natural language understanding.},
	urldate = {2024-04-15},
	publisher = {arXiv},
	author = {Warstadt, Alex and Choshen, Leshem and Mueller, Aaron and Williams, Adina and Wilcox, Ethan and Zhuang, Chengxu},
	month = jan,
	year = {2023},
	note = {arXiv:2301.11796 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{mireshghallah_smaller_2024,
	title = {Smaller {Language} {Models} are {Better} {Black}-box {Machine}-{Generated} {Text} {Detectors}},
	url = {http://arxiv.org/abs/2305.09859},
	abstract = {With the advent of fluent generative language models that can produce convincing utterances very similar to those written by humans, distinguishing whether a piece of text is machine-generated or human-written becomes more challenging and more important, as such models could be used to spread misinformation, fake news, fake reviews and to mimic certain authors and figures. To this end, there have been a slew of methods proposed to detect machine-generated text. Most of these methods need access to the logits of the target model or need the ability to sample from the target. One such black-box detection method relies on the observation that generated text is locally optimal under the likelihood function of the generator, while human-written text is not. We find that overall, smaller and partially-trained models are better universal text detectors: they can more precisely detect text generated from both small and larger models. Interestingly, we find that whether the detector and generator were trained on the same data is not critically important to the detection success. For instance the OPT-125M model has an AUC of 0.81 in detecting ChatGPT generations, whereas a larger model from the GPT family, GPTJ-6B, has AUC of 0.45.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Mireshghallah, Niloofar and Mattern, Justus and Gao, Sicun and Shokri, Reza and Berg-Kirkpatrick, Taylor},
	month = feb,
	year = {2024},
	note = {arXiv:2305.09859 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{belrose_eliciting_2023,
	title = {Eliciting {Latent} {Predictions} from {Transformers} with the {Tuned} {Lens}},
	url = {http://arxiv.org/abs/2303.08112},
	abstract = {We analyze transformers from the perspective of iterative inference, seeking to understand how model predictions are refined layer by layer. To do so, we train an affine probe for each block in a frozen pretrained model, making it possible to decode every hidden state into a distribution over the vocabulary. Our method, the {\textbackslash}emph\{tuned lens\}, is a refinement of the earlier ``logit lens'' technique, which yielded useful insights but is often brittle. We test our method on various autoregressive language models with up to 20B parameters, showing it to be more predictive, reliable and unbiased than the logit lens. With causal experiments, we show the tuned lens uses similar features to the model itself. We also find the trajectory of latent predictions can be used to detect malicious inputs with high accuracy. All code needed to reproduce our results can be found at https://github.com/AlignmentResearch/tuned-lens.},
	urldate = {2024-02-01},
	publisher = {arXiv},
	author = {Belrose, Nora and Furman, Zach and Smith, Logan and Halawi, Danny and Ostrovsky, Igor and McKinney, Lev and Biderman, Stella and Steinhardt, Jacob},
	month = nov,
	year = {2023},
	note = {arXiv:2303.08112 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{gurnee_finding_2023,
	title = {Finding {Neurons} in a {Haystack}: {Case} {Studies} with {Sparse} {Probing}},
	shorttitle = {Finding {Neurons} in a {Haystack}},
	url = {http://arxiv.org/abs/2305.01610},
	abstract = {Despite rapid adoption and deployment of large language models (LLMs), the internal computations of these models remain opaque and poorly understood. In this work, we seek to understand how high-level human-interpretable features are represented within the internal neuron activations of LLMs. We train \$k\$-sparse linear classifiers (probes) on these internal activations to predict the presence of features in the input; by varying the value of \$k\$ we study the sparsity of learned representations and how this varies with model scale. With \$k=1\$, we localize individual neurons which are highly relevant for a particular feature, and perform a number of case studies to illustrate general properties of LLMs. In particular, we show that early layers make use of sparse combinations of neurons to represent many features in superposition, that middle layers have seemingly dedicated neurons to represent higher-level contextual features, and that increasing scale causes representational sparsity to increase on average, but there are multiple types of scaling dynamics. In all, we probe for over 100 unique features comprising 10 different categories in 7 different models spanning 70 million to 6.9 billion parameters.},
	urldate = {2024-02-01},
	publisher = {arXiv},
	author = {Gurnee, Wes and Nanda, Neel and Pauly, Matthew and Harvey, Katherine and Troitskii, Dmitrii and Bertsimas, Dimitris},
	month = jun,
	year = {2023},
	note = {arXiv:2305.01610 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{din_jump_2023,
	title = {Jump to {Conclusions}: {Short}-{Cutting} {Transformers} {With} {Linear} {Transformations}},
	shorttitle = {Jump to {Conclusions}},
	url = {http://arxiv.org/abs/2303.09435},
	abstract = {Transformer-based language models (LMs) create hidden representations of their inputs at every layer, but only use final-layer representations for prediction. This obscures the internal decision-making process of the model and the utility of its intermediate representations. One way to elucidate this is to cast the hidden representations as final representations, bypassing the transformer computation in-between. In this work, we suggest a simple method for such casting, by using linear transformations. We show that our approach produces more accurate approximations than the prevailing practice of inspecting hidden representations from all layers in the space of the final layer. Moreover, in the context of language modeling, our method allows "peeking" into early layer representations of GPT-2 and BERT, showing that often LMs already predict the final output in early layers. We then demonstrate the practicality of our method to recent early exit strategies, showing that when aiming, for example, at retention of 95\% accuracy, our approach saves additional 7.9\% layers for GPT-2 and 5.4\% layers for BERT, on top of the savings of the original approach. Last, we extend our method to linearly approximate sub-modules, finding that attention is most tolerant to this change.},
	urldate = {2024-02-01},
	publisher = {arXiv},
	author = {Din, Alexander Yom and Karidi, Taelin and Choshen, Leshem and Geva, Mor},
	month = mar,
	year = {2023},
	note = {arXiv:2303.09435 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{dagan_getting_2024,
	title = {Getting the most out of your tokenizer for pre-training and domain adaptation},
	url = {http://arxiv.org/abs/2402.01035},
	abstract = {Tokenization is an understudied and often neglected component of modern LLMs. Most published works use a single tokenizer for all experiments, often borrowed from another model, without performing ablations or analysis to optimize tokenization. Moreover, the tokenizer is generally kept unchanged when fine-tuning a base model. In this paper, we show that the size, pre-tokenization regular expression, and training data of a tokenizer can significantly impact the model's generation speed, effective context size, memory usage, and downstream performance. We train specialized Byte-Pair Encoding code tokenizers, and conduct extensive ablations on the impact of tokenizer design on the performance of LLMs for code generation tasks such as HumanEval and MBPP, and provide recommendations for tokenizer hyper-parameters selection and switching the tokenizer in a pre-trained LLM. We perform our experiments on models trained from scratch and from pre-trained models, verifying their applicability to a wide range of use-cases. We find that when fine-tuning on more than 50 billion tokens, we can specialize the tokenizer of a pre-trained LLM to obtain large gains in generation speed and effective context size.},
	urldate = {2024-02-09},
	publisher = {arXiv},
	author = {Dagan, Gautier and Synnaeve, Gabriel and Rozi√®re, Baptiste},
	month = feb,
	year = {2024},
	note = {arXiv:2402.01035 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{bostrom_byte_2020,
	title = {Byte {Pair} {Encoding} is {Suboptimal} for {Language} {Model} {Pretraining}},
	url = {http://arxiv.org/abs/2004.03720},
	abstract = {The success of pretrained transformer language models (LMs) in natural language processing has led to a wide range of pretraining setups. In particular, these models employ a variety of subword tokenization methods, most notably byte-pair encoding (BPE) (Sennrich et al., 2016; Gage, 1994), the WordPiece method (Schuster and Nakajima, 2012), and unigram language modeling (Kudo, 2018), to segment text. However, to the best of our knowledge, the literature does not contain a direct evaluation of the impact of tokenization on language model pretraining. We analyze differences between BPE and unigram LM tokenization, finding that the latter method recovers subword units that align more closely with morphology and avoids problems stemming from BPE's greedy construction procedure. We then compare the fine-tuned task performance of identical transformer masked language models pretrained with these tokenizations. Across downstream tasks and two languages (English and Japanese), we find that the unigram LM tokenization method matches or outperforms BPE. We hope that developers of future pretrained LMs will consider adopting the unigram LM method over the more prevalent BPE.},
	urldate = {2024-04-12},
	publisher = {arXiv},
	author = {Bostrom, Kaj and Durrett, Greg},
	month = oct,
	year = {2020},
	note = {arXiv:2004.03720 [cs]},
	keywords = {Computer Science - Computation and Language, I.2.7},
}

@misc{dong_attention_2023-1,
	title = {Attention is {Not} {All} {You} {Need}: {Pure} {Attention} {Loses} {Rank} {Doubly} {Exponentially} with {Depth}},
	shorttitle = {Attention is {Not} {All} {You} {Need}},
	url = {http://arxiv.org/abs/2103.03404},
	abstract = {Attention-based architectures have become ubiquitous in machine learning, yet our understanding of the reasons for their effectiveness remains limited. This work proposes a new way to understand self-attention networks: we show that their output can be decomposed into a sum of smaller terms, each involving the operation of a sequence of attention heads across layers. Using this decomposition, we prove that self-attention possesses a strong inductive bias towards "token uniformity". Specifically, without skip connections or multi-layer perceptrons (MLPs), the output converges doubly exponentially to a rank-1 matrix. On the other hand, skip connections and MLPs stop the output from degeneration. Our experiments verify the identified convergence phenomena on different variants of standard transformer architectures.},
	urldate = {2024-03-27},
	publisher = {arXiv},
	author = {Dong, Yihe and Cordonnier, Jean-Baptiste and Loukas, Andreas},
	month = aug,
	year = {2023},
	note = {arXiv:2103.03404 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{chen_attention_2023,
	title = {Attention {Is} {Not} {All} {You} {Need} {Anymore}},
	url = {http://arxiv.org/abs/2308.07661},
	abstract = {In recent years, the popular Transformer architecture has achieved great success in many application areas, including natural language processing and computer vision. Many existing works aim to reduce the computational and memory complexity of the self-attention mechanism in the Transformer by trading off performance. However, performance is key for the continuing success of the Transformer. In this paper, a family of drop-in replacements for the self-attention mechanism in the Transformer, called the Extractors, is proposed. Four types of the Extractors, namely the super high-performance Extractor (SHE), the higher-performance Extractor (HE), the worthwhile Extractor (WE), and the minimalist Extractor (ME), are proposed as examples. Experimental results show that replacing the self-attention mechanism with the SHE evidently improves the performance of the Transformer, whereas the simplified versions of the SHE, i.e., the HE, the WE, and the ME, perform close to or better than the self-attention mechanism with less computational and memory complexity. Furthermore, the proposed Extractors have the potential or are able to run faster than the self-attention mechanism since their critical paths of computation are much shorter. Additionally, the sequence prediction problem in the context of text generation is formulated using variable-length discrete-time Markov chains, and the Transformer is reviewed based on our understanding.},
	urldate = {2024-03-27},
	publisher = {arXiv},
	author = {Chen, Zhe},
	month = sep,
	year = {2023},
	note = {arXiv:2308.07661 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{zou_representation_2023,
	title = {Representation {Engineering}: {A} {Top}-{Down} {Approach} to {AI} {Transparency}},
	shorttitle = {Representation {Engineering}},
	url = {http://arxiv.org/abs/2310.01405},
	abstract = {In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.},
	urldate = {2024-01-26},
	publisher = {arXiv},
	author = {Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and Goel, Shashwat and Li, Nathaniel and Byun, Michael J. and Wang, Zifan and Mallen, Alex and Basart, Steven and Koyejo, Sanmi and Song, Dawn and Fredrikson, Matt and Kolter, J. Zico and Hendrycks, Dan},
	month = oct,
	year = {2023},
	note = {arXiv:2310.01405 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@misc{ainslie_gqa_2023,
	title = {{GQA}: {Training} {Generalized} {Multi}-{Query} {Transformer} {Models} from {Multi}-{Head} {Checkpoints}},
	shorttitle = {{GQA}},
	url = {http://arxiv.org/abs/2305.13245},
	abstract = {Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5\% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr√≥n, Federico and Sanghai, Sumit},
	month = dec,
	year = {2023},
	note = {arXiv:2305.13245 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{shazeer_fast_2019,
	title = {Fast {Transformer} {Decoding}: {One} {Write}-{Head} is {All} {You} {Need}},
	shorttitle = {Fast {Transformer} {Decoding}},
	url = {http://arxiv.org/abs/1911.02150},
	abstract = {Multi-head attention layers, as used in the Transformer neural sequence model, are a powerful alternative to RNNs for moving information across and between sequences. While training these layers is generally fast and simple, due to parallelizability across the length of the sequence, incremental inference (where such paralleization is impossible) is often slow, due to the memory-bandwidth cost of repeatedly loading the large "keys" and "values" tensors. We propose a variant called multi-query attention, where the keys and values are shared across all of the different attention "heads", greatly reducing the size of these tensors and hence the memory bandwidth requirements of incremental decoding. We verify experimentally that the resulting models can indeed be much faster to decode, and incur only minor quality degradation from the baseline.},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Shazeer, Noam},
	month = nov,
	year = {2019},
	note = {arXiv:1911.02150 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{wu_group_2018,
	title = {Group {Normalization}},
	url = {http://arxiv.org/abs/1803.08494},
	abstract = {Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems --- BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6\% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.},
	urldate = {2024-02-13},
	publisher = {arXiv},
	author = {Wu, Yuxin and He, Kaiming},
	month = jun,
	year = {2018},
	note = {arXiv:1803.08494 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{ba_layer_2016,
	title = {Layer {Normalization}},
	url = {http://arxiv.org/abs/1607.06450},
	abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
	urldate = {2024-02-19},
	publisher = {arXiv},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	month = jul,
	year = {2016},
	note = {arXiv:1607.06450 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{wang_mambabyte_2024,
	title = {{MambaByte}: {Token}-free {Selective} {State} {Space} {Model}},
	shorttitle = {{MambaByte}},
	url = {http://arxiv.org/abs/2401.13660},
	abstract = {Token-free language models learn directly from raw bytes and remove the bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences, and standard autoregressive Transformers scale poorly in such settings. We experiment with MambaByte, a token-free adaptation of the Mamba state space model, trained autoregressively on byte sequences. Our experiments indicate the computational efficiency of MambaByte compared to other byte-level models. We also find MambaByte to be competitive with and even outperform state-of-the-art subword Transformers. Furthermore, owing to linear scaling in length, MambaByte benefits from fast inference compared to Transformers. Our findings establish the viability of MambaByte in enabling token-free language modeling.},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Wang, Junxiong and Gangavarapu, Tushaar and Yan, Jing Nathan and Rush, Alexander M.},
	month = jan,
	year = {2024},
	note = {arXiv:2401.13660 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{yu_megabyte_2023,
	title = {{MEGABYTE}: {Predicting} {Million}-byte {Sequences} with {Multiscale} {Transformers}},
	shorttitle = {{MEGABYTE}},
	url = {http://arxiv.org/abs/2305.07185},
	abstract = {Autoregressive transformers are spectacular models for short sequences but scale poorly to long sequences such as high-resolution images, podcasts, code, or books. We proposed Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes. Megabyte segments sequences into patches and uses a local submodel within patches and a global model between patches. This enables sub-quadratic self-attention, much larger feedforward layers for the same compute, and improved parallelism during decoding -- unlocking better performance at reduced cost for both training and generation. Extensive experiments show that Megabyte allows byte-level models to perform competitively with subword models on long context language modeling, achieve state-of-the-art density estimation on ImageNet, and model audio from raw files. Together, these results establish the viability of tokenization-free autoregressive sequence modeling at scale.},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Yu, Lili and Simig, D√°niel and Flaherty, Colin and Aghajanyan, Armen and Zettlemoyer, Luke and Lewis, Mike},
	month = may,
	year = {2023},
	note = {arXiv:2305.07185 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{srivastava_dropout_2014,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overfitting}},
	volume = {15},
	url = {http://jmlr.org/papers/v15/srivastava14a.html},
	number = {56},
	journal = {Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	pages = {1929--1958},
}

@misc{jaszczur_sparse_2021,
	title = {Sparse is {Enough} in {Scaling} {Transformers}},
	url = {http://arxiv.org/abs/2111.12763},
	abstract = {Large Transformer models yield impressive results on many tasks, but are expensive to train, or even fine-tune, and so slow at decoding that their use and study becomes out of reach. We address this problem by leveraging sparsity. We study sparse variants for all layers in the Transformer and propose Scaling Transformers, a family of next generation Transformer models that use sparse layers to scale efficiently and perform unbatched decoding much faster than the standard Transformer as we scale up the model size. Surprisingly, the sparse layers are enough to obtain the same perplexity as the standard Transformer with the same number of parameters. We also integrate with prior sparsity approaches to attention and enable fast inference on long sequences even with limited memory. This results in performance competitive to the state-of-the-art on long text summarization.},
	urldate = {2024-03-28},
	publisher = {arXiv},
	author = {Jaszczur, Sebastian and Chowdhery, Aakanksha and Mohiuddin, Afroz and Kaiser, ≈Åukasz and Gajewski, Wojciech and Michalewski, Henryk and Kanerva, Jonni},
	month = nov,
	year = {2021},
	note = {arXiv:2111.12763 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{charpentier_not_2023,
	title = {Not all layers are equally as important: {Every} {Layer} {Counts} {BERT}},
	shorttitle = {Not all layers are equally as important},
	url = {http://arxiv.org/abs/2311.02265},
	abstract = {This paper introduces a novel modification of the transformer architecture, tailored for the data-efficient pretraining of language models. This aspect is evaluated by participating in the BabyLM challenge, where our solution won both the strict and strict-small tracks. Our approach allows each transformer layer to select which outputs of previous layers to process. The empirical results verify the potential of this simple modification and show that not all layers are equally as important.},
	urldate = {2024-01-25},
	publisher = {arXiv},
	author = {Charpentier, Lucas Georges Gabriel and Samuel, David},
	month = nov,
	year = {2023},
	note = {arXiv:2311.02265 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{soviany_curriculum_2022,
	title = {Curriculum {Learning}: {A} {Survey}},
	shorttitle = {Curriculum {Learning}},
	url = {http://arxiv.org/abs/2101.10382},
	abstract = {Training machine learning models in a meaningful order, from the easy samples to the hard ones, using curriculum learning can provide performance improvements over the standard training approach based on random data shuffling, without any additional computational costs. Curriculum learning strategies have been successfully employed in all areas of machine learning, in a wide range of tasks. However, the necessity of finding a way to rank the samples from easy to hard, as well as the right pacing function for introducing more difficult data can limit the usage of the curriculum approaches. In this survey, we show how these limits have been tackled in the literature, and we present different curriculum learning instantiations for various tasks in machine learning. We construct a multi-perspective taxonomy of curriculum learning approaches by hand, considering various classification criteria. We further build a hierarchical tree of curriculum learning methods using an agglomerative clustering algorithm, linking the discovered clusters with our taxonomy. At the end, we provide some interesting directions for future work.},
	urldate = {2024-03-27},
	publisher = {arXiv},
	author = {Soviany, Petru and Ionescu, Radu Tudor and Rota, Paolo and Sebe, Nicu},
	month = apr,
	year = {2022},
	note = {arXiv:2101.10382 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{zhang_lifting_2023,
	title = {Lifting the {Curse} of {Capacity} {Gap} in {Distilling} {Language} {Models}},
	url = {http://arxiv.org/abs/2305.12129},
	abstract = {Pretrained language models (LMs) have shown compelling performance on various downstream tasks, but unfortunately they require a tremendous amount of inference compute. Knowledge distillation finds a path to compress LMs to small ones with a teacher-student paradigm. However, when the capacity gap between the teacher and the student is large, a curse of capacity gap appears, invoking a deficiency in distilling LMs. While a few studies have been carried out to fill the gap, the curse is not yet well tackled. In this paper, we aim at lifting the curse of capacity gap via enlarging the capacity of the student without notably increasing the inference compute. Largely motivated by sparse activation regime of mixture of experts (MoE), we propose a mixture of minimal experts (MiniMoE), which imposes extra parameters to the student but introduces almost no additional inference compute. Experimental results on GLUE and CoNLL demonstrate the curse of capacity gap is lifted by the magic of MiniMoE to a large extent. MiniMoE also achieves the state-of-the-art performance at small FLOPs compared with a range of competitive baselines. With a compression rate as much as \${\textbackslash}sim\$50\${\textbackslash}times\$, MiniMoE preserves \${\textbackslash}sim\$95{\textbackslash}\% GLUE score of the teacher.},
	urldate = {2024-01-21},
	publisher = {arXiv},
	author = {Zhang, Chen and Yang, Yang and Liu, Jiahao and Wang, Jingang and Xian, Yunsen and Wang, Benyou and Song, Dawei},
	month = may,
	year = {2023},
	note = {arXiv:2305.12129 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{bahdanau_neural_2016,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	urldate = {2024-01-25},
	publisher = {arXiv},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = may,
	year = {2016},
	note = {arXiv:1409.0473 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{lecun_optimal_1989,
	title = {Optimal {Brain} {Damage}},
	volume = {2},
	url = {https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Morgan-Kaufmann},
	author = {LeCun, Yann and Denker, John and Solla, Sara},
	editor = {Touretzky, D.},
	year = {1989},
}

@misc{shleifer_normformer_2021,
	title = {{NormFormer}: {Improved} {Transformer} {Pretraining} with {Extra} {Normalization}},
	shorttitle = {{NormFormer}},
	url = {http://arxiv.org/abs/2110.09456},
	abstract = {During pretraining, the Pre-LayerNorm transformer suffers from a gradient magnitude mismatch: gradients at early layers are much larger than at later layers. These issues can be alleviated by our proposed NormFormer architecture, which adds three normalization operations to each layer: a Layer Norm after self attention, head-wise scaling of self-attention outputs, and a Layer Norm after the first fully connected layer. The extra operations incur negligible compute cost (+0.4\% parameter increase), but improve pretraining perplexity and downstream task performance for both causal and masked language models ranging from 125 Million to 2.7 Billion parameters. For example, adding NormFormer on top of our strongest 1.3B parameter baseline can reach equal perplexity 24\% faster, or converge 0.27 perplexity better in the same compute budget. This model reaches GPT3-Large (1.3B) zero shot performance 60\% faster. For masked language modeling, NormFormer improves fine-tuned GLUE performance by 1.9\% on average. Code to train NormFormer models is available in fairseq https://github.com/pytorch/fairseq/tree/main/examples/normformer .},
	urldate = {2024-01-25},
	publisher = {arXiv},
	author = {Shleifer, Sam and Weston, Jason and Ott, Myle},
	month = nov,
	year = {2021},
	note = {arXiv:2110.09456 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{mckenzie_inverse_2023,
	title = {Inverse {Scaling}: {When} {Bigger} {Isn}'t {Better}},
	shorttitle = {Inverse {Scaling}},
	url = {http://arxiv.org/abs/2306.09479},
	abstract = {Work on scaling laws has found that large language models (LMs) show predictable improvements to overall loss with increased scale (model size, training data, and compute). Here, we present evidence for the claim that LMs may show inverse scaling, or worse task performance with increased scale, e.g., due to flaws in the training objective and data. We present empirical evidence of inverse scaling on 11 datasets collected by running a public contest, the Inverse Scaling Prize, with a substantial prize pool. Through analysis of the datasets, along with other examples found in the literature, we identify four potential causes of inverse scaling: (i) preference to repeat memorized sequences over following in-context instructions, (ii) imitation of undesirable patterns in the training data, (iii) tasks containing an easy distractor task which LMs could focus on, rather than the harder real task, and (iv) correct but misleading few-shot demonstrations of the task. We release the winning datasets at https://inversescaling.com/data to allow for further investigation of inverse scaling. Our tasks have helped drive the discovery of U-shaped and inverted-U scaling trends, where an initial trend reverses, suggesting that scaling trends are less reliable at predicting the behavior of larger-scale models than previously understood. Overall, our results suggest that there are tasks for which increased model scale alone may not lead to progress, and that more careful thought needs to go into the data and objectives for training language models.},
	urldate = {2024-01-22},
	publisher = {arXiv},
	author = {McKenzie, Ian R. and Lyzhov, Alexander and Pieler, Michael and Parrish, Alicia and Mueller, Aaron and Prabhu, Ameya and McLean, Euan and Kirtland, Aaron and Ross, Alexis and Liu, Alisa and Gritsevskiy, Andrew and Wurgaft, Daniel and Kauffman, Derik and Recchia, Gabriel and Liu, Jiacheng and Cavanagh, Joe and Weiss, Max and Huang, Sicong and Droid, The Floating and Tseng, Tom and Korbak, Tomasz and Shen, Xudong and Zhang, Yuhui and Zhou, Zhengping and Kim, Najoung and Bowman, Samuel R. and Perez, Ethan},
	month = jun,
	year = {2023},
	note = {arXiv:2306.09479 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
}

@misc{michel_are_2019,
	title = {Are {Sixteen} {Heads} {Really} {Better} than {One}?},
	url = {http://arxiv.org/abs/1905.10650},
	abstract = {Attention is a powerful and ubiquitous mechanism for allowing neural models to focus on particular salient pieces of information by taking their weighted average when making predictions. In particular, multi-headed attention is a driving force behind many recent state-of-the-art NLP models such as Transformer-based MT models and BERT. These models apply multiple attention mechanisms in parallel, with each attention "head" potentially focusing on different parts of the input, which makes it possible to express sophisticated functions beyond the simple weighted average. In this paper we make the surprising observation that even if models have been trained using multiple heads, in practice, a large percentage of attention heads can be removed at test time without significantly impacting performance. In fact, some layers can even be reduced to a single head. We further examine greedy algorithms for pruning down models, and the potential speed, memory efficiency, and accuracy improvements obtainable therefrom. Finally, we analyze the results with respect to which parts of the model are more reliant on having multiple heads, and provide precursory evidence that training dynamics play a role in the gains provided by multi-head attention.},
	urldate = {2024-01-21},
	publisher = {arXiv},
	author = {Michel, Paul and Levy, Omer and Neubig, Graham},
	month = nov,
	year = {2019},
	note = {arXiv:1905.10650 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{eldan_tinystories_2023,
	title = {{TinyStories}: {How} {Small} {Can} {Language} {Models} {Be} and {Still} {Speak} {Coherent} {English}?},
	shorttitle = {{TinyStories}},
	url = {http://arxiv.org/abs/2305.07759},
	abstract = {Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention). In this work, we introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities. We also introduce a new paradigm for the evaluation of language models: We suggest a framework which uses GPT-4 to grade the content generated by these models as if those were stories written by students and graded by a (human) teacher. This new paradigm overcomes the flaws of standard benchmarks which often requires the model's output to be very structures, and moreover provides a multidimensional score for the model, providing scores for different capabilities such as grammar, creativity and consistency. We hope that TinyStories can facilitate the development, analysis and research of LMs, especially for low-resource or specialized domains, and shed light on the emergence of language capabilities in LMs.},
	urldate = {2023-11-01},
	publisher = {arXiv},
	author = {Eldan, Ronen and Li, Yuanzhi},
	month = may,
	year = {2023},
	note = {arXiv:2305.07759 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762v5},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2024-01-25},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{warstadt_blimp_2023,
	title = {{BLiMP}: {The} {Benchmark} of {Linguistic} {Minimal} {Pairs} for {English}},
	shorttitle = {{BLiMP}},
	url = {http://arxiv.org/abs/1912.00582},
	abstract = {We introduce The Benchmark of Linguistic Minimal Pairs (shortened to BLiMP), a challenge set for evaluating what language models (LMs) know about major grammatical phenomena in English. BLiMP consists of 67 sub-datasets, each containing 1000 minimal pairs isolating specific contrasts in syntax, morphology, or semantics. The data is automatically generated according to expert-crafted grammars, and aggregate human agreement with the labels is 96.4\%. We use it to evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs. We find that state-of-the-art models identify morphological contrasts reliably, but they struggle with semantic restrictions on the distribution of quantifiers and negative polarity items and subtle syntactic phenomena such as extraction islands.},
	urldate = {2024-01-22},
	publisher = {arXiv},
	author = {Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey, Anhad and Peng, Wei and Wang, Sheng-Fu and Bowman, Samuel R.},
	month = feb,
	year = {2023},
	note = {arXiv:1912.00582 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{wang_glue_2019,
	title = {{GLUE}: {A} {Multi}-{Task} {Benchmark} and {Analysis} {Platform} for {Natural} {Language} {Understanding}},
	shorttitle = {{GLUE}},
	url = {http://arxiv.org/abs/1804.07461},
	abstract = {For natural language understanding (NLU) technology to be maximally useful, both practically and as a scientific object of study, it must be general: it must be able to process language in a way that is not exclusively tailored to any one specific task or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation benchmark (GLUE), a tool for evaluating and analyzing the performance of models across a diverse range of existing NLU tasks. GLUE is model-agnostic, but it incentivizes sharing knowledge across tasks because certain tasks have very limited training data. We further provide a hand-crafted diagnostic test suite that enables detailed linguistic analysis of NLU models. We evaluate baselines based on current methods for multi-task and transfer learning and find that they do not immediately give substantial improvements over the aggregate performance of training a separate model per task, indicating room for improvement in developing general and robust NLU systems.},
	urldate = {2024-01-25},
	publisher = {arXiv},
	author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
	month = feb,
	year = {2019},
	note = {arXiv:1804.07461 [cs]},
	keywords = {Computer Science - Computation and Language},
}
