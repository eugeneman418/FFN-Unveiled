
\section{Experimental Setup}
\label{sec:setup}
This section reports the research questions answered by the experiments, the dataset used in pretraining, the evaluation metrics, and the hyperparameters\footnote{Our implementation can be found at: github.com/AISE-TUDelft/tiny-transformers}.


\subsection{Research Questions}
The experiments address the following questions:
\begin{enumerate}
    \item \textit{Can sparse feedforward layers offer better model performance than a dense feedforward layer of the same size?} We compare the performance of vanilla GPT-Neo and RoBERTa against their sparse variants augmented with MoE, CNT, and PKM as feedforward layers while keeping the number of parameters as close as possible.
    \item \textit{Is there a direct relationship between the degree of sparsity in the feedforward layer and the model performance?} For each sparse variant, we evaluate several configurations corresponding to different levels of sparsity.
    \item \textit{How does the type of sparse feedforward layer affect inference speed?} We measure the forward pass time for MoE, CNT, and PKM.
\end{enumerate}

\subsection{TinyStories}
All models investigated in this paper are pretrained on the TinyStories dataset. This is a collection of short stories generated by GPT-3.5 and GPT-4 tailored to match the cognitive level of a 3-4 year-old \cite{eldan_tinystories_2023}. It is a suitable dataset for pretraining small generative language models because it covers a wide range of grammatically correct stories without forcing the model to learn a vast amount of domain knowledge. 
% In addition, it has been demonstrated that small models, having fewer than $35$ million parameters, can outperform variants of GPT-2 (each with more than $125$ million parameters) when pretrained on this dataset, even with less training budget.

\subsection{BabyLM Pipeline}
BablyLM is a challenge on sample-efficient pretraining of small language models \cite{warstadt_call_2023}. All models investigated in this paper are evaluated on performance metrics from BabyLM\footnote{The official evaluation pipeline can be found at: https://github.com/babylm/evaluation-pipeline-2023}, namely BLiMP and (Super)GLUE. We used a modified implementation of the BabyLM evaluation pipeline to accommodate the custom feedforward layers.

BLiMP originally consisted of 67000 pairs of sentences, where the sentences in each pair are minimally different, but one contains one of twelve types of grammar error \cite{warstadt_blimp_2023}. For BabyLM, this dataset set was extended with supplement tasks, covering a wider range of grammar errors. Performance is measured as classification accuracy, where a model classifies a pair correctly if it assigns the grammatically correct sentence a higher likelihood.

GLUE is a collection of causal reasoning, question answering, word sense disambiguation, and coreference resolution tasks \cite{wang_glue_2019}. It is designed to test a model's general linguistic knowledge and offers a difficult human baseline for language models to compete against. Like GLUE, SuperGLUE is also designed to assess a model's language understanding, albeit through harder tasks \cite{wang_superglue_2020}. BabyLM uses a mixture of GLUE and SuperGLUE tasks.


\subsection{Hyperparameters}
The baseline for all experiments are GPT-Neo \cite{black_gpt-neo_2021}, which is an implementation of GPT-2, and RoBERTa \cite{liu_roberta_2019}, which is an implementation of BERT. The number of transformer blocks is fixed to $2$. Token embedding dimensionality is set to $d_\text{in}=256$ and the context length is kept at $512$. For the baseline feedforward network, the intermediate size is $d_m = 4096$. Sparse transformers are constructed by replacing all the feedforward layers with their sparse variants.

For MoE, the number of experts is limited to $N_\text{experts}=4$ and each expert has intermediate size $d_m = 1023$. We experiment with activating top $k=1,2,3$ experts.

For CNT, we experiment with block counts $N_\text{blocks}=64,32,16$, and associated intermediate size $d_m = 4032, 4032, 3968$.

As for PKM, all models have $N_\text{head}=4$ heads, $N_\text{subkeys}=56$ subkeys, and query dimensionality $d_q=1024$. Models with top $k=14,28,42$ (approximately top $25\%,50\%,75\%$) key activation are investigated. \Cref{tab:mode-size} summarizes the model size. 

We pretrain all models for $2$ epochs using the AdamW optimizer with an initial learning rate of $0.0005$ and linear learning rate decay. The batch size and the gradient accumulation steps are set to $16$. All petrainings are accelerated by a NVIDIA A100 GPU.

To measure the average inference time of each type of feedforward layer, we measure the inference time on a batch of $16$ input sequences, each of length $256$ and embedding dimensionality of $256$. This is measured across $1000$ batches. Prior to the experiment, the GPU is warmed up for $100$ batches.


\begin{table}[h]

\centering
\caption{Models involved in our experiments and their number of learnable parameters and the lower bound sparsity ratio (ratio between the number of active and total parameters) in their feedforward layers.}
\label{tab:mode-size}
\tiny
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Parameter Count} & \textbf{Sparsity Ratio} \\
\midrule
GPT-Neo & 7421440 & 1.00 \\
GPT-Neo MoE ($k=1$) & 7422968 & 0.25 \\
GPT-Neo MoE ($k=2$) & 7422968 & 0.50 \\
GPT-Neo MoE ($k=3$) & 7422968 & 0.75\\
GPT-Neo CNT ($N_\text{blocks}=16$) & 7425280 & 0.04\\
GPT-Neo CNT ($N_\text{blocks}=32$) & 7424384 & 0.02\\
GPT-Neo CNT ($N_\text{blocks}=64$) & 7390080 & 0.02 \\
GPT-Neo PKM ($k=14$) & 7396352 & 0.61 \\
GPT-Neo PKM ($k=28$) & 7396352 & 0.62 \\
GPT-Neo PKM ($k=42$) & 7396352 & 0.62 \\
\midrule
RoBERTa & 7500048 & 1.00 \\
RoBERTa MoE ($k=1$) & 7501576 & 0.25 \\
RoBERTa MoE ($k=2$) & 7501576 & 0.50 \\
RoBERTa MoE ($k=3$) & 7501576 & 0.75 \\
RoBERTa CNT ($N_\text{blocks}=16$) & 7503888 & 0.04\\
RoBERTa CNT ($N_\text{blocks}=32$) & 7502992 & 0.02\\
RoBERTa CNT ($N_\text{blocks}=64$) & 7468688 & 0.02\\
RoBERTa PKM ($k=14$) & 7474960 & 0.61 \\
RoBERTa PKM ($k=28$) & 7474960 & 0.62 \\
RoBERTa PKM ($k=42$) & 7474960 & 0.62 \\
\bottomrule
\end{tabular}
\end{table}

% \begin{itemize}
%     \item GPT-Neo and RoBERTa as baselines
%     \item The number of parameters for all models are under 10M, as that is the threshold for small language models as defined by \cite{eldan_tinystories_2023}.
%     \item Studying small language models because they can be trained with limited computational budget.
%     \item All models are pretrained on TinyStories \cite{eldan_tinystories_2023} dataset, so they can learn grammatic knowledge without being exposed to various domain specific information.
%     \item All models' grammatical knowledge are evaluated on BLiMP, MSGS, and SuperGLUE, following the BabyLM pipeline \cite{warstadt_findings_2023}.
%     \item For baseline FFN, $d_\text{in} = 256$, $M=16$ so the baselines have around 7.5M parameters.
%     \item For MoE, $N_\text{expert}=4$ so we can test $25\%, 50\%, 75\%$ experts activation without the model becoming too large. For MoE to have the same number of parameters as baseline, it must be that $d_\text{hidden} = \frac{Md_\text{in}-N_\text{expert}}{N_\text{expert}}$
%     \item For controller FFN, in \cite{jaszczur_sparse_2021}, $d_\text{in} = 1024, N_\text{block}=64$. This is scaled down proportionally for our experiments, meaning $d_\text{in} = 256, N_\text{block}=16$. To consider different levels of sparsity $N_\text{blocks}=32,64$ are also tested. Keeping the total number of parameters the same as the baseline, $d_\text{hidden} = \frac{2MN_\text{block}-1}{2N_\text{block}+1}d_\text{in}$
%     \item For product key memory, $N_\text{head}=4,d_\text{subkey}=2d_\text{in}$. This means $N_\text{subkey}= 56$ for this model to have similar number of parameters as the baseline. The sparsity level varies by selecting top $25\%, 50\%, 75\%$ of subkeys.
    
% \end{itemize}
 