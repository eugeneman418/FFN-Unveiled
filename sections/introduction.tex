\section{Introduction}
\label{sec:intro}
Transformers are state-of-the-art models for natural language tasks such as next word prediction, text summarisation, and sentiment analysis. They are the backbones of tools including ChatGPT\footnote{https://openai.com/chatgpt/} and GitHub Copilot\footnote{https://github.com/features/copilot}. Although performance tends to scale with model size for transformers, bigger transformers are also slower to train and occupy more space. Fortunately, an array of techniques exists to improve a transformer's efficiency \cite{tay_efficient_2022}. Sparsity is one such technique. Sparse transformers are transformer models that only activate a portion of their parameters when processing an input. This allows the model to contain more parameters, and thus greater learning capacities, without extra computational costs. Sparse models such as GLaM \cite{du_glam_2022}, Switch Transformer \cite{fedus_switch_2022}, and Scaling Transformer \cite{jaszczur_sparse_2021} have been shown to outperform archetypal dense transformers including GPT-3 \cite{brown_language_2020}, T5 \cite{raffel_exploring_2023}, and Pegasus \cite{zhang_pegasus_2020}.

The architectural reason for the sparse transformer's success remains unclear. While the number of learnable parameters certainly plays a role, current research is inconclusive on whether sparsity is intrinsically related to performance, in the sense of a more efficient utilization of parameters. This is indeed plausible as earlier works on network pruning demonstrated that it is possible to achieve better \cite{balderas_optimizing_2024} or at least comparable performance \cite{lecun_optimal_1989} to the original model with a sparsified network.

This question calls for controlled and comprehensive experiments. Controlled in terms of the model size, as the sparse model needs to have a comparable size as the dense model to negate the higher capacity gained from the additional parameters. Comprehensive with respect to the implementations of sparsity. There are many ways to sparsify both the attention and the feedforward layer of the transformer. This paper investigates only the feedforward layer to reduce the search space.

We restrict our attention to small transformers, that is, transformers with less than $10\text{M}$ parameters and $256$ embedding dimensions as defined by \citet{eldan_tinystories_2023}. The choice to focus on small transformers is based on two rationales. Firstly, small transformers can be pretrained and fine-tuned within reasonable time using commercial hardware. In practice, it means they can be deployed locally to perform domain-specific tasks rather than being hosted on the cloud, thereby circumventing privacy and security issues. Secondly, current research in large language models is approaching a bottleneck as large language models are predicted to require more training data than what is available in the world by 2032 \cite{villalobos_will_2024}. Thus, small, sample-efficient transformers are becoming a promising research direction. When model size is a limiting factor, architectural decisions are all the more important.

This paper explores a new avenue to improve small transformers via alternate architectures. More specifically, we empirically investigate whether sparsifying the feedforward layers in small transformers, without increasing the number of learnable parameters, improves language understanding and inference speed. Our contribution is as follows:
\begin{itemize}
    \item We found, through empirical study, that inference speed does not increase with sparse feedforward layers because current machine learning frameworks do not have good support for sparse models and sparsity does not receive as many benefits on a single device compared to distributed training in the small language model setting.
    \item We found that sparse feedforward layers under the right configurations are better at language understanding than dense feedforward networks.
    \item We contribute to a unified understanding of sparse feedforward layers by deriving a mixture of experts interpretation of the controller feedforward layer.
    \item A replication package\footnote{https://github.com/AISE-TUDelft/tiny-transformers} for reproducing our findings, and our models\footnote{https://huggingface.co/collections/AISE-TUDelft/brp-tiny-transformers-666c352b3b570f44d7d2a519} published on HuggingFace.
\end{itemize}


% The remainder of this paper is organized as follows: In section \ref{sec:background} we provide an overview of sparse transformers. Section \ref{sec:approach} elaborates on the sparse feedforward paradigms by deriving their sizes. These results are used in section \ref{sec:setup}, which describes the experiment setup. Our findings are presented in section \ref{sec:results}. This is followed by an analysis and interpretability study in section \ref{sec:discussion}. In section \ref{sec:responsible} we reflect on the ethics and reproducibility of our research. Lastly, we state the conclusion and prescribe future directions for our research in section \ref{sec:conclusion}.

