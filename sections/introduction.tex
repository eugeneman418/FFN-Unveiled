\section{Introduction}
\label{sec:intro}
\begin{itemize}
    \item Sparse transformers are transformer models that only activate a portion of their parameters when processing an input.
    \item Sparse transformers such as GLaM, Switch Transformer, and Scaling Transformer have been shown to outperform dense models such as GPT-3 \cite{du_glam_2022}, T5 \cite{fedus_switch_2022}, and BIGBIRD-RoBERTa \cite{jaszczur_sparse_2021}.
    \item But the architectural reason for the sparse transformer's success is uncertain.
    \item Sparsity allows the model more parameters without extra computational costs. More parameters bring greater learning capacities.
    \item But when such models are compared to much smaller baselines, it becomes unclear whether the improved performance is inherently related to sparsity, in the sense of more efficient utilization of model parameters, or simply the result of a bigger model.
    \item Works on network pruning demonstrated that it is indeed possible to achieve better \cite{balderas_optimizing_2024} or at least comparable performance \cite{lecun_optimal_1989} to the original model with a sparsified network.
    \item More theoretical backings of sparse feedforward networks \cite{csordas_approximating_2023,baykal_theoretical_2022}
    \item Our work contributions to the question of sparsity's influence in learning capacity by comparing sparse transformers against dense transformers of similar sizes.
    \item \textbf{TODO: findings and implications}

\end{itemize}

The remainder of this paper is organized as follows: In section \ref{sec:background} we provide an overview of sparse transformers. Section \ref{sec:approach} elaborates on the sparse feedforward paradigms by deriving their sizes. These results are used in section \ref{sec:setup}, which describes the experiment setup. Our findings are presented in section \ref{sec:results}. This is followed by an analysis and interpretability study in section \ref{sec:discussion}. In section \ref{sec:responsible} we reflect on the ethics and reproducibility of our research. Lastly, we state the conclusion and prescribe future directions for our research in section \ref{sec:conclusion}.

