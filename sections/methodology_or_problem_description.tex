\section{Background}
\label{sec:background}
\begin{itemize}
    \item Sparse transformers uses sparsified attention or feedforward
    \item But since the feedforward layers make up the majority of a transformer's parameters, our work is restricted to sparse feedforward layers.
    \item There are 3 sparse feedforward paradims: mixture of experts, weight mask, and memory layer.
    \item Mixture of experts \cite{lepikhin_gshard_2020,du_glam_2022,fedus_switch_2022} replaces the feedforward layer with an ensemble of smaller feedforward networks. There is a routing network which depends the subset of the ensemble that will process the input. In our work we will consider the original MoE layer proposed by \cite{shazeer_outrageously_2017}.
    \item The principle of weight masks is to use a sparse weight matrix in the feedforward network. This can be static, such as \cite{chandy_dyad_2023} or learned, as with \cite{jaszczur_sparse_2021}. For generality, this work considers dynamic masking from \cite{jaszczur_sparse_2021}.
    \item The memory layer \cite{lample_large_2019,wang_--fly_2020,rae_scaling_2016} is an alternative to the feedforward layer. The input is converted to a query vector and only the values corresponding to best matching keys are activated. We will consider \cite{lample_large_2019} as the representative for this paradigm in our experiments.
\end{itemize}
