\begin{abstract}
Although transformers are state-of-the-art models for natural language
tasks, obtaining reasonable performance still often require large transformers which are expensive to train and deploy. Fortunately, there are techniques to increase the size of transformers without extra computing costs. One such technique is sparsity. However, it remains unclear whether sparse architecture is intrinsically more efficient than its dense counterpart. In this paper, we investigate whether replacing the feedforward networks in small transformers with sparse alternatives results in better predictions and faster inference. We found that although inference speed does not increase due to software and hardware limitations, certain sparse alternatives do result in better performance. Our research contributes to smarter architectural decision making when designing small language models.

% The aim of this template is to make it more clear what is expected from you.
% \textbf{It is by no means required to follow this exact same structure.}
% The abstract should be short and give the overall idea:
% what is the background, the research questions, what are your contributions, and what are the main conclusions.
% It should be readable as a stand-alone text (preferably no references to the paper or to outside literature).
\end{abstract}