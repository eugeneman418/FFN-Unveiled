\section{Discussion}
\label{sec:discussion}
\subsection{Performance of Sparse Feedforward Layers}
Whether sparse feedforward layers can achieve comparable performance as the vanilla FFN depends on the baseline architecture and type of sparse feedforward layer as \Cref{fig:spar-rat,fig:spar-rat-super} demonstrate. Regardless, sparse models seem to be fast learners since the loss difference between the sparse models and the baseline is much smaller by the end of pretraining than at the start. This is likely because sparse feedforward layers approximate a larger network than an FFN of the same size. Sparse feedforward layers are more flexible learners. In terms of neural memories, although MoE, CNT, and PKM do not have as many unique value vectors as FFN, their memory coefficients are computed from more complex, non-linear functions.

% all types of sparse feedforward layers except PKM for BERT models, outperformed the RoBERTa baseline on BLiMP tasks. This is not the case for GPT-Neo-based models though. CNT and PKM can achieve comparable performance to the baseline in certain configurations, but overall sparse feedforward layers led to worse performance. In should be noted that in general, all models in this experiment performed poorly on BLiMP tasks. None were significantly better than a random classifier.
% \Cref{fig:eval-loss} presents a similar finding. Although no sparse models achieved lower evaluation loss than their respective baselines, among the GPT-Neo-based models, the MoE variant achieved similar evaluation loss towards the end of pertaining. At any rate, sparse models seem to be fast learners, as the loss difference between the sparse models and the baseline is much smaller by the end of pertaining than at the start. This is corroborated by the RoBERTa-based models. Despite having a higher loss at initialization, almost all sparse models reached a similar evaluation loss as the baseline after the first epoch.

% CNT is consistently the best type of sparse feedforward layer for BLiMP tasks. This is possibly due to CNT approximating a much larger network than the other types of sparse feedforward layers in the experimented configurations. If CNT, MoE, and PKM are all interpreted as mixture of expert models, then, thanks to weight sharing, the CNT models in this experiment contain combinatorially more experts than the MoE and PKM models. It would also imply that for a mixture of expert models, the number of experts is more important to performance than the number of activated experts and the intermediate size of each expert; for CNT, only the top expert is activated and the intermediate size is much smaller than in the input size.

\subsection{Role of Sparsity Ratio}
There is inconclusive evidence on the number of active parameters directly affecting performance. Even when considering a sparse variant such as MoE in isolation, it is not guaranteed for performance to decrease or increase with the sparsity ratio. A plausible explanation is that the feedforward architecture is far more important to language understanding than the proportion of active parameters. In other words, how parameters are activated is far more crucial than the amount of active parameters. This hypothesis is corroborated by the CNT results. CNT outperformed the baselines on both BLiMP and (Super)GLUE tasks despite having less than $5\%$ parameter activation in the feedforward layer. In contrast, PKM cannot compete with the BLiMP baselines even when more than $60\%$ of its parameters are active because it is not as architecturally efficient as CNT.
 
% number learnable parameters is more important to performance than the number of active parameters. The models investigated in this paper are large enough such that they can learn the data distribution with only a fraction of their parameters. This hypothesis is corrobrated by the CNT results. CNT outperformed the baselines on both BLiMP and (Super)GLUE tasks despite having less than $5\%$ parameter activation in the feedforward layer.

% On BLiMP tasks, there is a moderate negative correlation between the sparsity and performance for RoBERTa-based models, as sparser models tends to score higher. The relationship is reversed, albeit with weaker correlation, for GPT-Neo based models, where denser models seems to perform better. The architecture is likely to play a more important role. In other words, how parameters are activated is far more crucial to performance than the amount of active parameters. For example, using CNT, sparse transformers can reach similar BLiMP scores as the baseline, despite having less than $5\%$ parameter activation in the feedforward layer, whereas with PKM, sparse transformers cannot compete against the baseline even when more than $60\%$ of the feedforward parameters are active. This result suggests that CNT is a more efficient architecture than PKM.

\subsection{Inference Cost of Sparse Models}
% The vanilla feedforward network has by far shorter inference time than its sparse variants as illustrated by \Cref{tab:inference}. 
Despite the theoretical speedups sparsity and conditional computation bring, sparse feedforward layers are slowed down by software limitations and the small transformer setting. All models are implemented with PyTorch \cite{paszke_pytorch_2019}, which, at time the of writing, lacks support for various essential features on sparse tensors, such as reshapes and automatic differentiation, resulting in inefficient implementations. Furthermore, the speedups of conditional computation often manifest in a distributed setting, as is the case in \cite{fedus_switch_2022}, where an MoE model is scaled up by distributing the experts across multiple devices. Small transformers do not enjoy this advantage because all training happens on one GPU.

\subsection{Threats to Validity}
Threats to the validity of this work can be categorized into three categories: threats to internal validity, which are the confounders of the experiment, threats to external validity, which are the issues surrounding the generalizability of the experiment, and threats to construct validity, which concerns the soundness of the research questions.
\subsubsection{Internal Validity}
While the sizes of the sparse feedforward layers are set to be as close as possible to their dense counterparts in the experiments, it is not possible for them to be identical. Nonetheless, the relative difference is insignificant so it is unlikely to result in a huge change in learning capacity. Other hyperparameters may have a significant influence on the experiment result. For example, we hypothesize based on \Cref{fig:eval-loss} that all sparse models will converge to the same evaluation loss as that of the baselines given more epochs. However, this would require more computing budget than what is available for this research.
\subsubsection{External Validity}
It must be emphasized that the model configurations investigated in the experiments are by no means exhaustive. As showcased by \Cref{tab:mode-size}, despite considering different configurations for each type of sparse feedforward layer, the sparsity ratio is in some cases almost identical. This is because for some models, such as PKM, a noticeable change in sparsity ratio requires a huge increase in parameter count. 

All experiments are conducted on a single GPU. While this is unlikely to affect pretraining and evaluation, changing the hardware setup will alter the inference speed of sparse transformers, especially if the models are distributed over multiple devices.
\subsubsection{Construct Validity}
There is no universal measure of sparsity. In this research, our measure of sparsity differs per model. For MoE, the degree of sparsity is defined as the number of experts selected. For, CNT, the degree of sparsity is measured in the number of blocks the activation vector is split into. For PKM, the degree of sparsity is defined as the number of keys selected. These definitions do not always correspond to the sparsity ratio well. Although the sparsity ratio appears to be a universal definition of sparsity, it too has shortcomings. As an example, PKM's sparsity ratio is challenging to measure as it depends on the input. Even under the neural memory framework, the number of active memory cells is not guaranteed to be static, so it too is a difficult sparsity measure to implement.

\subsection{Future Work}
\Cref{tab:summary_eval} shows that CNT is an efficient architecture. It outperforms the baselines on both BLiMP and (Super)GLUE while using the lowest percentage of parameters. This paper investigated CNT under a very low sparsity ratio. For future research, we recommend studying the performance of CNT with more active parameters. For other models, we recommend reducing the intermediate size, and instead increasing the number of experts for MoE or keys for PKM. We conjecture that a very small intermediate size does not necessarily lead to worse performance, as shown by CNT under the mixture of experts interpretation. Lastly, we recommend adding more layers and controllers to CNT to mimic hierarchical MoE. Weight sharing the MoE and PKM is also a good direction for further research.